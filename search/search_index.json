{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Distributed Asynchronous Object Storage (DAOS) is an open-source object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next-generation NVM technology, like Storage Class Memory (SCM) and NVM express (NVMe), while presenting a key-value storage interface on top of commodity hardware that provides features, such as, transactional non-blocking I/O, advanced data protection with self-healing, end-to-end data integrity, fine-grained data control, and elastic storage, to optimize performance and cost. The included document versions are associated with DAOS v0.9. Refer to the following documentation for architecture and description: Document Description DAOS Overview Terminology, Storage, Transaction, Fault and the Security models are presented. Administration Guide System administration topics are covered in the Administration Guide. User Guide Documentation for users including the different interface that are supported. Developer Guide Overview of the DAOS internal code structure and major algorithms for DAOS developers. Community Wiki This is the main community repository for DAOS information. Links to discover, use and contribute to DAOS are available from this page. Community Roadmap The DAOS development roadmap is found here. Note that the information contained on the roadmap may change.","title":"Home"},{"location":"coding/","text":"DAOS Coding Rules \u00b6","title":"DAOS Coding Rules"},{"location":"coding/#daos-coding-rules","text":"","title":"DAOS Coding Rules"},{"location":"contributing/","text":"Contributing to DAOS Development \u00b6","title":"Contributing to DAOS Development"},{"location":"contributing/#contributing-to-daos-development","text":"","title":"Contributing to DAOS Development"},{"location":"debugging/","text":"DAOS Debugging \u00b6 DAOS uses the debug system defined in CaRT but more specifically the GURT library. Logging for both client and server are written to \"/tmp/daos.log\" unless otherwise set by D_LOG_FILE . Registered Subsystems/Facilities \u00b6 The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging for. By default all subsystems are enabled (\"DD_SUBSYS=all\"). - DAOS Facilities: [common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, eio, tests] - Common Facilities (GURT): [MISC, MEM] - CaRT Facilities: [RPC, BULK, CORPC, GRP, LM, HG, ST, IV] Priority Logging \u00b6 All macros which output logs have a priority level, shown in decending order below. - D_FATAL(fmt, ...) FATAL - D_CRIT(fmt, ...) CRIT - D_ERROR(fmt, ...) ERR - D_WARN(fmt, ...) WARN - D_NOTE(fmt, ...) NOTE - D_INFO(fmt, ...) INFO - D_DEBUG(mask, fmt, ...) DEBUG The priority level that outputs to stderr can be set with DD_STDERR . By default in DAOS (specific to project), this is set to CRIT (\"DD_STDERR=CRIT\") meaning that all CRIT and more severe log messages will dump to stderr. This however is separate from the priority of logging to \"/tmp/daos.log\". The priority level of logging can be set with D_LOG_MASK , which by default is set to INFO (\"D_LOG_MASK=INFO\"), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well (\"D_LOG_MASK=DEBUG,MEM=ERR\"). Debug Masks/Streams: \u00b6 DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). In order to accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default (\"DD_MASK=all\"). - DAOS Debug Masks: - md = metadata operations - pl = placement operations - mgmt = pool management - epc = epoch system - df = durable format - rebuild = rebuild process - daos_default = (group mask) io, md, pl, and rebuild operations - Common Debug Masks (GURT): - any = generic messages, no classification - trace = function trace, tree/hash/lru operations - mem = memory operations - net = network operations - io = object I/O - test = test programs Common Use Cases \u00b6 Generic setup for all messages (default settings) $ D_LOG_MASK=DEBUG $ DD_SUBSYS=all $ DD_MASK=all Disable all logs for performance tuning $ D_LOG_MASK=ERR -> will only log error messages from all facilities $ D_LOG_MASK=FATAL -> will only log system fatal messages Disable a noisy debug logging subsystem $ D_LOG_MASK=DEBUG,MEM=ERR -> disables MEM facility by restricting all logs from that facility to ERROR or higher priority only Enable a subset of facilities of interest $ DD_SUBSYS=rpc,tests $ D_LOG_MASK=DEBUG -> required to see logs for RPC and TESTS less severe than INFO (majority of log messages) Fine-tune the debug messages by setting a debug mask $ D_LOG_MASK=DEBUG $ DD_MASK=mgmt -> only logs DEBUG messages related to pool management See DAOS Environment Variables documentation for more info about debug system environment.","title":"DAOS Debugging"},{"location":"debugging/#daos-debugging","text":"DAOS uses the debug system defined in CaRT but more specifically the GURT library. Logging for both client and server are written to \"/tmp/daos.log\" unless otherwise set by D_LOG_FILE .","title":"DAOS Debugging"},{"location":"debugging/#registered-subsystemsfacilities","text":"The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging for. By default all subsystems are enabled (\"DD_SUBSYS=all\"). - DAOS Facilities: [common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, eio, tests] - Common Facilities (GURT): [MISC, MEM] - CaRT Facilities: [RPC, BULK, CORPC, GRP, LM, HG, ST, IV]","title":"Registered Subsystems/Facilities"},{"location":"debugging/#priority-logging","text":"All macros which output logs have a priority level, shown in decending order below. - D_FATAL(fmt, ...) FATAL - D_CRIT(fmt, ...) CRIT - D_ERROR(fmt, ...) ERR - D_WARN(fmt, ...) WARN - D_NOTE(fmt, ...) NOTE - D_INFO(fmt, ...) INFO - D_DEBUG(mask, fmt, ...) DEBUG The priority level that outputs to stderr can be set with DD_STDERR . By default in DAOS (specific to project), this is set to CRIT (\"DD_STDERR=CRIT\") meaning that all CRIT and more severe log messages will dump to stderr. This however is separate from the priority of logging to \"/tmp/daos.log\". The priority level of logging can be set with D_LOG_MASK , which by default is set to INFO (\"D_LOG_MASK=INFO\"), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well (\"D_LOG_MASK=DEBUG,MEM=ERR\").","title":"Priority Logging"},{"location":"debugging/#debug-masksstreams","text":"DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). In order to accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default (\"DD_MASK=all\"). - DAOS Debug Masks: - md = metadata operations - pl = placement operations - mgmt = pool management - epc = epoch system - df = durable format - rebuild = rebuild process - daos_default = (group mask) io, md, pl, and rebuild operations - Common Debug Masks (GURT): - any = generic messages, no classification - trace = function trace, tree/hash/lru operations - mem = memory operations - net = network operations - io = object I/O - test = test programs","title":"Debug Masks/Streams:"},{"location":"debugging/#common-use-cases","text":"Generic setup for all messages (default settings) $ D_LOG_MASK=DEBUG $ DD_SUBSYS=all $ DD_MASK=all Disable all logs for performance tuning $ D_LOG_MASK=ERR -> will only log error messages from all facilities $ D_LOG_MASK=FATAL -> will only log system fatal messages Disable a noisy debug logging subsystem $ D_LOG_MASK=DEBUG,MEM=ERR -> disables MEM facility by restricting all logs from that facility to ERROR or higher priority only Enable a subset of facilities of interest $ DD_SUBSYS=rpc,tests $ D_LOG_MASK=DEBUG -> required to see logs for RPC and TESTS less severe than INFO (majority of log messages) Fine-tune the debug messages by setting a debug mask $ D_LOG_MASK=DEBUG $ DD_MASK=mgmt -> only logs DEBUG messages related to pool management See DAOS Environment Variables documentation for more info about debug system environment.","title":"Common Use Cases"},{"location":"development/","text":"DAOS for Development \u00b6 Building DAOS for Development \u00b6 For development, it is recommended to build and install each dependency in a unique subdirectory. The DAOS build system supports this through the TARGET_PREFIX variable. Once the submodules have been initialized and updated, run the following: scons PREFIX=${daos_prefix_path} TARGET_PREFIX=${daos_prefix_path}/opt install --build-deps=yes --config=force Installing the components into seperate directories allow to upgrade the components individually replacing --build-deps=yes with --update-prereq={component_name}. This requires change to the environment configuration from before. For automated environment setup, source scons_local/utils/setup_local.sh. ARGOBOTS=${daos_prefix_path}/opt/argobots CART=${daos_prefix_path}/opt/cart HWLOC=${daos_prefix_path}/opt/hwloc MERCURY=${daos_prefix_path}/opt/mercury PMDK=${daos_prefix_path}/opt/pmdk OPA=${daos_prefix_path}/opt/openpa FIO=${daos_prefix_path}/opt/fio SPDK=${daos_prefix_path}/opt/spdk PATH=$CART/bin/:${daos_prefix_path}/bin/:$PATH With this approach DAOS would get built using the prebuilt dependencies in ${daos_prefix_path}/opt and required options are saved for future compilations. So, after the first time, during development, a mere \"scons --config=force\" and \"scons --config=force install\" would suffice for compiling changes to daos source code. If you wish to compile DAOS with clang rather than gcc, set COMPILER=clang on the scons command line. This option is also saved for future compilations. Go dependencies \u00b6 Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses dep to manage these dependencies. On EL7 and later: yum install yum-plugin-copr yum copr enable hnakamur/golang-dep yum install golang-dep On Fedora 27 and later: dnf install dep On Ubuntu 18.04 and later: apt-get install go-dep For OSes that don't supply a package: * Ensure that you have a personal GOPATH (see \"go env GOPATH\", referred to as \"$GOPATH\" in this document) and a GOBIN ($GOPATH/bin) set up and included in your PATH: mkdir -p $GOPATH/bin export PATH=$GOPATH/bin:$PATH Then follow the installation instructions on Github . To update the vendor directory using dep after changing Gopkg.toml, first make sure DAOS is cloned into $GOPATH/src/github.com/daos-stack/daos Then: cd $GOPATH/src/github.com/daos-stack/daos/src/control dep ensure Protobuf Compiler \u00b6 The DAOS control plane infrastructure uses Protocol Buffers as the data serialization format for its RPC requests. Not all developers will need to compile the *.proto files, but if Protobuf changes are needed, the developer must regenerate the corresponding C and Go source files using a Protobuf compiler compatible with proto3 syntax. Recommended Versions \u00b6 The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work, but are not guaranteed. Protocol Buffers v3.5.1. Installation instructions . Protobuf-C v1.3.1. Installation instructions . gRPC plugin: protoc-gen-go v1.2.0. Must match the proto version in src/control/Gopkg.toml. Install the specific version using GIT_TAG instructions here . Compiling Protobuf Files \u00b6 Generate the Go file using the gRPC plugin. You can designate the directory location: protoc myfile.proto --go_out=plugins=grpc:<go_file_dir> Generate the C files using Protobuf-C. As the header and source files in DAOS are typically kept in separate locations, you will need to move them manually to their destination directories: protoc-c myfile.proto --c_out=. mv myfile.pb-c.h <c_file_include_dir> mv myfile.pb-c.c <c_file_src_dir>","title":"DAOS for Development"},{"location":"development/#daos-for-development","text":"","title":"DAOS for Development"},{"location":"development/#building-daos-for-development","text":"For development, it is recommended to build and install each dependency in a unique subdirectory. The DAOS build system supports this through the TARGET_PREFIX variable. Once the submodules have been initialized and updated, run the following: scons PREFIX=${daos_prefix_path} TARGET_PREFIX=${daos_prefix_path}/opt install --build-deps=yes --config=force Installing the components into seperate directories allow to upgrade the components individually replacing --build-deps=yes with --update-prereq={component_name}. This requires change to the environment configuration from before. For automated environment setup, source scons_local/utils/setup_local.sh. ARGOBOTS=${daos_prefix_path}/opt/argobots CART=${daos_prefix_path}/opt/cart HWLOC=${daos_prefix_path}/opt/hwloc MERCURY=${daos_prefix_path}/opt/mercury PMDK=${daos_prefix_path}/opt/pmdk OPA=${daos_prefix_path}/opt/openpa FIO=${daos_prefix_path}/opt/fio SPDK=${daos_prefix_path}/opt/spdk PATH=$CART/bin/:${daos_prefix_path}/bin/:$PATH With this approach DAOS would get built using the prebuilt dependencies in ${daos_prefix_path}/opt and required options are saved for future compilations. So, after the first time, during development, a mere \"scons --config=force\" and \"scons --config=force install\" would suffice for compiling changes to daos source code. If you wish to compile DAOS with clang rather than gcc, set COMPILER=clang on the scons command line. This option is also saved for future compilations.","title":"Building DAOS for Development"},{"location":"development/#go-dependencies","text":"Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses dep to manage these dependencies. On EL7 and later: yum install yum-plugin-copr yum copr enable hnakamur/golang-dep yum install golang-dep On Fedora 27 and later: dnf install dep On Ubuntu 18.04 and later: apt-get install go-dep For OSes that don't supply a package: * Ensure that you have a personal GOPATH (see \"go env GOPATH\", referred to as \"$GOPATH\" in this document) and a GOBIN ($GOPATH/bin) set up and included in your PATH: mkdir -p $GOPATH/bin export PATH=$GOPATH/bin:$PATH Then follow the installation instructions on Github . To update the vendor directory using dep after changing Gopkg.toml, first make sure DAOS is cloned into $GOPATH/src/github.com/daos-stack/daos Then: cd $GOPATH/src/github.com/daos-stack/daos/src/control dep ensure","title":"Go dependencies"},{"location":"development/#protobuf-compiler","text":"The DAOS control plane infrastructure uses Protocol Buffers as the data serialization format for its RPC requests. Not all developers will need to compile the *.proto files, but if Protobuf changes are needed, the developer must regenerate the corresponding C and Go source files using a Protobuf compiler compatible with proto3 syntax.","title":"Protobuf Compiler"},{"location":"development/#recommended-versions","text":"The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work, but are not guaranteed. Protocol Buffers v3.5.1. Installation instructions . Protobuf-C v1.3.1. Installation instructions . gRPC plugin: protoc-gen-go v1.2.0. Must match the proto version in src/control/Gopkg.toml. Install the specific version using GIT_TAG instructions here .","title":"Recommended Versions"},{"location":"development/#compiling-protobuf-files","text":"Generate the Go file using the gRPC plugin. You can designate the directory location: protoc myfile.proto --go_out=plugins=grpc:<go_file_dir> Generate the C files using Protobuf-C. As the header and source files in DAOS are typically kept in separate locations, you will need to move them manually to their destination directories: protoc-c myfile.proto --c_out=. mv myfile.pb-c.h <c_file_include_dir> mv myfile.pb-c.c <c_file_src_dir>","title":"Compiling Protobuf Files"},{"location":"environ/","text":"DAOS Environment Variables \u00b6 This file lists the environment variables used by DAOS. Many of them are meant for development purposes only and may be removed or changed in the future. The description of each variable follows the following format: A short description. Type . Default to what behavior if not set. A longer description if necessary. Type is defined by this table: Type Values BOOL 0 means false; any other value means true BOOL2 no means false; any other value means true BOOL3 set to empty or any value means true; unset means false INTEGER Non-negative decimal integer STRING String Common \u00b6 Environment variables in this section apply to both the server side and the client side DAOS_IO_MODE \u00b6 Control the DAOS IO mode: server dispatches modification RPCs to replicas or client does that, if it is the former case, whether enable DTX or not. INTEGER . Valid values are as following, default to 0 (server dispatches RPCs and enable DTX). 0: server dispatches RPCs, enable DTX. 1: server dispatches RPCs, disable DTX. 2: client disptaches RPCs, disable DTX. DAOS_IO_BYPASS \u00b6 Server \u00b6 Environment variables in this section only apply to the server side. These checksum algorithms are currently supported: crc64 and crc32 . VOS_MEM_CLASS \u00b6 Memory class used by VOS. STRING . Default to persistent memory. If the value is set to DRAM , all data will be stored in volatile memory; otherwise, all data will be stored to persistent memory. VOS_BDEV_CLASS \u00b6 SPDK bdev class used by VOS. STRING . Default to NVMe bdev. When testing on node without NVMe device available, it can be set to MALLOC or AIO to make VOS using SPDK malloc or AIO device. IO_STAT_PERIOD \u00b6 Print SPDK bdev io statistics periodically. INTEGER . Default to 0 (disabled). If it is set to N (non-zero), SPDK bdev io statistics will be printed on server console in every N seconds. RDB_ELECTION_TIMEOUT \u00b6 Raft election timeout used by RDBs in milliseconds. INTEGER . Default to 7000 ms. RDB_REQUEST_TIMEOUT \u00b6 Raft request timeout used by RDBs in milliseconds. INTEGER . Default to 3000 ms. RDB_COMPACT_THRESHOLD \u00b6 Raft log compaction threshold in applied entries. INTEGER . Default to 256 entries. If set to 0, Raft log entries will never be compacted. DAOS_REBUILD \u00b6 Whether to start rebuilds when excluding targets. BOOL2 . Default to true. DAOS_MD_CAP \u00b6 Size of a metadata pmem pool/file in MBs. INTEGER . Default to 128 MB. DAOS_START_POOL_SVC \u00b6 Whether to start existing pool services when starting a daos_server . BOOL . Default to true. DAOS_IMPLICIT_PURGE \u00b6 Whether to aggregate unreferenced epochs. BOOL . Default to false. DAOS_TARGET_OVERSUBSCRIBE \u00b6 Whether to accept target number oversubscribe for daos server. BOOL . Default to false. Client \u00b6 Environment variables in this section only apply to the client side. DAOS_IO_SRV_DISPATCH \u00b6 Whether to enable the server-side IO dispatch, in that case the replica IO will be sent to a leader shard which will dispatch to other shards. BOOL . Default to true. Debug System (Client & Server) \u00b6 D_LOG_FILE \u00b6 DAOS debug logs (both server and client) are written to /tmp/daos.log by default. This can be modified by setting this environment variable (\"D_LOG_FILE=/tmp/daos_server\"). DD_SUBSYS \u00b6 Used to specify which subsystems to enable. DD_SUBSYS can be set to individual subsystems for finer-grained debugging (\"DD_SUBSYS=vos\"), multiple facilities (\"DD_SUBSYS=eio,mgmt,misc,mem\"), or all facilities (\"DD_SUBSYS=all\") which is also the default setting. If a facility is not enabled, then only ERR messages or more severe messages will print. DD_STDERR \u00b6 Used to specify the priority level to output to stderr. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. By default, all CRIT and more severe DAOS messages will log to stderr (\"DD_STDERR=CRIT\"), and the default for CaRT/GURT is FATAL. D_LOG_MASK \u00b6 Used to specify what type/level of logging will be present for either all of the registered subsystems, or a select few. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. DEBUG option is used to enable all logging (debug messages as well as all higher priority level messages). Note that if D_LOG_MASK is not set, it will default to logging all messages excluding debug (\"D_LOG_MASK=INFO\"). EX: \"D_LOG_MASK=DEBUG\" This will set the logging level for all facilities to DEBUG, meaning that all debug messages, as well as higher priority messages will be logged (INFO, NOTE, WARN, ERR, CRIT, FATAL). EX: \"D_LOG_MASK=DEBUG,MEM=ERR,RPC=ERR\" This will set the logging level to DEBUG for all facilities except MEM & RPC (which will now only log ERR and higher priority level messages, skipping all DEBUG, INFO, NOTE & WARN messages) DD_MASK \u00b6 Used to enable different debug streams for finer-grained debug messages, essentially allowing the user to specify an area of interest to debug (possibly involving many different subsystems) as opposed to parsing through many lines of generic DEBUG messages. All debug streams will be enabled by default (\"DD_MASK=all\"). Single debug masks can be set (\"DD_MASK=trace\") or multiple masks (\"DD_MASK=trace,test,mgmt\"). Note that since these debug streams are strictly related to the debug log messages, D_LOG_MASK must be set to DEBUG.","title":"DAOS Environment Variables"},{"location":"environ/#daos-environment-variables","text":"This file lists the environment variables used by DAOS. Many of them are meant for development purposes only and may be removed or changed in the future. The description of each variable follows the following format: A short description. Type . Default to what behavior if not set. A longer description if necessary. Type is defined by this table: Type Values BOOL 0 means false; any other value means true BOOL2 no means false; any other value means true BOOL3 set to empty or any value means true; unset means false INTEGER Non-negative decimal integer STRING String","title":"DAOS Environment Variables"},{"location":"environ/#common","text":"Environment variables in this section apply to both the server side and the client side","title":"Common"},{"location":"environ/#daos_io_mode","text":"Control the DAOS IO mode: server dispatches modification RPCs to replicas or client does that, if it is the former case, whether enable DTX or not. INTEGER . Valid values are as following, default to 0 (server dispatches RPCs and enable DTX). 0: server dispatches RPCs, enable DTX. 1: server dispatches RPCs, disable DTX. 2: client disptaches RPCs, disable DTX.","title":"DAOS_IO_MODE"},{"location":"environ/#daos_io_bypass","text":"","title":"DAOS_IO_BYPASS"},{"location":"environ/#server","text":"Environment variables in this section only apply to the server side. These checksum algorithms are currently supported: crc64 and crc32 .","title":"Server"},{"location":"environ/#vos_mem_class","text":"Memory class used by VOS. STRING . Default to persistent memory. If the value is set to DRAM , all data will be stored in volatile memory; otherwise, all data will be stored to persistent memory.","title":"VOS_MEM_CLASS"},{"location":"environ/#vos_bdev_class","text":"SPDK bdev class used by VOS. STRING . Default to NVMe bdev. When testing on node without NVMe device available, it can be set to MALLOC or AIO to make VOS using SPDK malloc or AIO device.","title":"VOS_BDEV_CLASS"},{"location":"environ/#io_stat_period","text":"Print SPDK bdev io statistics periodically. INTEGER . Default to 0 (disabled). If it is set to N (non-zero), SPDK bdev io statistics will be printed on server console in every N seconds.","title":"IO_STAT_PERIOD"},{"location":"environ/#rdb_election_timeout","text":"Raft election timeout used by RDBs in milliseconds. INTEGER . Default to 7000 ms.","title":"RDB_ELECTION_TIMEOUT"},{"location":"environ/#rdb_request_timeout","text":"Raft request timeout used by RDBs in milliseconds. INTEGER . Default to 3000 ms.","title":"RDB_REQUEST_TIMEOUT"},{"location":"environ/#rdb_compact_threshold","text":"Raft log compaction threshold in applied entries. INTEGER . Default to 256 entries. If set to 0, Raft log entries will never be compacted.","title":"RDB_COMPACT_THRESHOLD"},{"location":"environ/#daos_rebuild","text":"Whether to start rebuilds when excluding targets. BOOL2 . Default to true.","title":"DAOS_REBUILD"},{"location":"environ/#daos_md_cap","text":"Size of a metadata pmem pool/file in MBs. INTEGER . Default to 128 MB.","title":"DAOS_MD_CAP"},{"location":"environ/#daos_start_pool_svc","text":"Whether to start existing pool services when starting a daos_server . BOOL . Default to true.","title":"DAOS_START_POOL_SVC"},{"location":"environ/#daos_implicit_purge","text":"Whether to aggregate unreferenced epochs. BOOL . Default to false.","title":"DAOS_IMPLICIT_PURGE"},{"location":"environ/#daos_target_oversubscribe","text":"Whether to accept target number oversubscribe for daos server. BOOL . Default to false.","title":"DAOS_TARGET_OVERSUBSCRIBE"},{"location":"environ/#client","text":"Environment variables in this section only apply to the client side.","title":"Client"},{"location":"environ/#daos_io_srv_dispatch","text":"Whether to enable the server-side IO dispatch, in that case the replica IO will be sent to a leader shard which will dispatch to other shards. BOOL . Default to true.","title":"DAOS_IO_SRV_DISPATCH"},{"location":"environ/#debug-system-client-server","text":"","title":"Debug System (Client &amp; Server)"},{"location":"environ/#d_log_file","text":"DAOS debug logs (both server and client) are written to /tmp/daos.log by default. This can be modified by setting this environment variable (\"D_LOG_FILE=/tmp/daos_server\").","title":"D_LOG_FILE"},{"location":"environ/#dd_subsys","text":"Used to specify which subsystems to enable. DD_SUBSYS can be set to individual subsystems for finer-grained debugging (\"DD_SUBSYS=vos\"), multiple facilities (\"DD_SUBSYS=eio,mgmt,misc,mem\"), or all facilities (\"DD_SUBSYS=all\") which is also the default setting. If a facility is not enabled, then only ERR messages or more severe messages will print.","title":"DD_SUBSYS"},{"location":"environ/#dd_stderr","text":"Used to specify the priority level to output to stderr. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. By default, all CRIT and more severe DAOS messages will log to stderr (\"DD_STDERR=CRIT\"), and the default for CaRT/GURT is FATAL.","title":"DD_STDERR"},{"location":"environ/#d_log_mask","text":"Used to specify what type/level of logging will be present for either all of the registered subsystems, or a select few. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. DEBUG option is used to enable all logging (debug messages as well as all higher priority level messages). Note that if D_LOG_MASK is not set, it will default to logging all messages excluding debug (\"D_LOG_MASK=INFO\"). EX: \"D_LOG_MASK=DEBUG\" This will set the logging level for all facilities to DEBUG, meaning that all debug messages, as well as higher priority messages will be logged (INFO, NOTE, WARN, ERR, CRIT, FATAL). EX: \"D_LOG_MASK=DEBUG,MEM=ERR,RPC=ERR\" This will set the logging level to DEBUG for all facilities except MEM & RPC (which will now only log ERR and higher priority level messages, skipping all DEBUG, INFO, NOTE & WARN messages)","title":"D_LOG_MASK"},{"location":"environ/#dd_mask","text":"Used to enable different debug streams for finer-grained debug messages, essentially allowing the user to specify an area of interest to debug (possibly involving many different subsystems) as opposed to parsing through many lines of generic DEBUG messages. All debug streams will be enabled by default (\"DD_MASK=all\"). Single debug masks can be set (\"DD_MASK=trace\") or multiple masks (\"DD_MASK=trace,test,mgmt\"). Note that since these debug streams are strictly related to the debug log messages, D_LOG_MASK must be set to DEBUG.","title":"DD_MASK"},{"location":"admin/","text":"DAOS Administrator Guide \u00b6","title":"DAOS Administrator Guide"},{"location":"admin/#daos-administrator-guide","text":"","title":"DAOS Administrator Guide"},{"location":"admin/administration/","text":"DAOS System Administration \u00b6 System Monitoring \u00b6 System monitoring and telemetry data will be provided as part of the control plane and will be documented in a future revision. System Operations \u00b6 Full Shutdown and Restart \u00b6 Details on how to support proper DAOS server shutdown will be provided in a future revision. Fault Domain Maintenance and Reintegration \u00b6 Details on how to drain an individual storage node or fault domain (e.g. rack) in preparation for maintenance activity and how to reintegrate it will be provided in a future revision. DAOS System Extension \u00b6 Ability to add new DAOS server instances to a pre-existing DAOS system will be documented in a future revision. Fault Management \u00b6 DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. Fault Detection & Isolation \u00b6 DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM 1 that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure makes an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Once detected, the faulty target or servers (effectively a set of targets) must be excluded from each pool membership. This process is triggered either manually by the administrator or automatically (see next section for more information). Upon exclusion from the pool map, each target starts the collective rebuild process automatically to restore data redundancy. The rebuild process is designed to operate online while servers continue to process incoming I/O operations from applications. Tools to monitor and manage rebuild are still under development. Rebuild Throttling \u00b6 The rebuild process may consume many resources on each server and can be throttled to reduce the impact on application performance. This current logic relies on CPU cycles on the storage nodes. By default, the rebuild process is configured to consume up to 30% of the CPU cycles, leaving the other 70% for regular I/O operations. During the rebuild process, the user can set the throttle to guarantee the rebuild will not use more resource than the user setting. The user can only set the CPU cycle for now. For example, if the user set the throttle to 50, then the rebuild will at most use 50% of the CPU cycle to do the rebuild job. The default rebuild throttle for CPU cycle is 30. This parameter can be changed via the daos_mgmt_set_params() API call and will be eventually available through the management tools. Software Upgrade \u00b6 Interoperability in DAOS is handled via protocol and schema versioning for persistent data structures. Further instructions on how to manage DAOS software upgrades will be provided in a future revision. Protocol Interoperability \u00b6 Limited protocol interoperability is provided by the DAOS storage stack. Version compatibility checks will be performed to verify that: All targets in the same pool run the same protocol version. Client libraries linked with the application may be up to one protocol version older than the targets. If a protocol version mismatch is detected among storage targets in the same pool, the entire DAOS system will fail to start up and will report failure to the control API. Similarly, the connection from clients running a protocol version incompatible with the targets will return an error. Persistent Schema Compatibility and Update \u00b6 The schema of persistent data structures may evolve from time to time to fix bugs, add new optimizations, or support new features. To that end, the persistent data structures support schema versioning. Upgrading the schema version will not be performed automatically and must be initiated by the administrator. A dedicated upgrade tool will be provided to upgrade the schema version to the latest one. All targets in the same pool must have the same schema version. Version checks are performed at system initialization time to enforce this constraint. To limit the validation matrix, each new DAOS release will be published with a list of supported schema versions. To run with the new DAOS release, administrators will then need to upgrade the DAOS system to one of the supported schema versions. New pool shards will always be formatted with the latest version. This versioning schema only applies to a data structure stored in persistent memory and not to block storage that only stores user data with no metadata. Storage Scrubbing \u00b6 Support for end-to-end data integrity is planned for DAOS v1.2 and background checksum scrubbing for v2.2. Once available, that functionality will be documented here. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028914 \u21a9","title":"System Administration"},{"location":"admin/administration/#daos-system-administration","text":"","title":"DAOS System Administration"},{"location":"admin/administration/#system-monitoring","text":"System monitoring and telemetry data will be provided as part of the control plane and will be documented in a future revision.","title":"System Monitoring"},{"location":"admin/administration/#system-operations","text":"","title":"System Operations"},{"location":"admin/administration/#full-shutdown-and-restart","text":"Details on how to support proper DAOS server shutdown will be provided in a future revision.","title":"Full Shutdown and Restart"},{"location":"admin/administration/#fault-domain-maintenance-and-reintegration","text":"Details on how to drain an individual storage node or fault domain (e.g. rack) in preparation for maintenance activity and how to reintegrate it will be provided in a future revision.","title":"Fault Domain Maintenance and Reintegration"},{"location":"admin/administration/#daos-system-extension","text":"Ability to add new DAOS server instances to a pre-existing DAOS system will be documented in a future revision.","title":"DAOS System Extension"},{"location":"admin/administration/#fault-management","text":"DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains.","title":"Fault Management"},{"location":"admin/administration/#fault-detection-isolation","text":"DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM 1 that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure makes an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Once detected, the faulty target or servers (effectively a set of targets) must be excluded from each pool membership. This process is triggered either manually by the administrator or automatically (see next section for more information). Upon exclusion from the pool map, each target starts the collective rebuild process automatically to restore data redundancy. The rebuild process is designed to operate online while servers continue to process incoming I/O operations from applications. Tools to monitor and manage rebuild are still under development.","title":"Fault Detection &amp; Isolation"},{"location":"admin/administration/#rebuild-throttling","text":"The rebuild process may consume many resources on each server and can be throttled to reduce the impact on application performance. This current logic relies on CPU cycles on the storage nodes. By default, the rebuild process is configured to consume up to 30% of the CPU cycles, leaving the other 70% for regular I/O operations. During the rebuild process, the user can set the throttle to guarantee the rebuild will not use more resource than the user setting. The user can only set the CPU cycle for now. For example, if the user set the throttle to 50, then the rebuild will at most use 50% of the CPU cycle to do the rebuild job. The default rebuild throttle for CPU cycle is 30. This parameter can be changed via the daos_mgmt_set_params() API call and will be eventually available through the management tools.","title":"Rebuild Throttling"},{"location":"admin/administration/#software-upgrade","text":"Interoperability in DAOS is handled via protocol and schema versioning for persistent data structures. Further instructions on how to manage DAOS software upgrades will be provided in a future revision.","title":"Software Upgrade"},{"location":"admin/administration/#protocol-interoperability","text":"Limited protocol interoperability is provided by the DAOS storage stack. Version compatibility checks will be performed to verify that: All targets in the same pool run the same protocol version. Client libraries linked with the application may be up to one protocol version older than the targets. If a protocol version mismatch is detected among storage targets in the same pool, the entire DAOS system will fail to start up and will report failure to the control API. Similarly, the connection from clients running a protocol version incompatible with the targets will return an error.","title":"Protocol Interoperability"},{"location":"admin/administration/#persistent-schema-compatibility-and-update","text":"The schema of persistent data structures may evolve from time to time to fix bugs, add new optimizations, or support new features. To that end, the persistent data structures support schema versioning. Upgrading the schema version will not be performed automatically and must be initiated by the administrator. A dedicated upgrade tool will be provided to upgrade the schema version to the latest one. All targets in the same pool must have the same schema version. Version checks are performed at system initialization time to enforce this constraint. To limit the validation matrix, each new DAOS release will be published with a list of supported schema versions. To run with the new DAOS release, administrators will then need to upgrade the DAOS system to one of the supported schema versions. New pool shards will always be formatted with the latest version. This versioning schema only applies to a data structure stored in persistent memory and not to block storage that only stores user data with no metadata.","title":"Persistent Schema Compatibility and Update"},{"location":"admin/administration/#storage-scrubbing","text":"Support for end-to-end data integrity is planned for DAOS v1.2 and background checksum scrubbing for v2.2. Once available, that functionality will be documented here. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028914 \u21a9","title":"Storage Scrubbing"},{"location":"admin/app_interface_tiering/","text":"Application Interface and Tiering \u00b6 DAOS Container Management \u00b6 DAOS containers are the unit of data management for users. Container Creation/Destroy \u00b6 Containers can be created and destroyed through the daos_cont_create/destroy() functions exported by the DAOS API. A user tool called daos is also provided to manage containers. To create a container: $ daos container create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 --svc=0 Successfully created container 008123fc-6b6c-4768-a88a-a2a5ef34a1a2 The container type (i.e., POSIX or HDF5) can be passed via the --type option. As shown below, the pool UUID, container UUID, and container attributes can be stored in the extended attributes of a POSIX file or directory for convenience. Then subsequent invocations of the daos tools need to reference the path to the POSIX file or directory. $ daos container create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 --svc=0 --path=/tmp/mycontainer --type=POSIX --oclass=large --chunk_size=4K Successfully created container 419b7562-5bb8-453f-bd52-917c8f5d80d1 type POSIX $ daos container query --svc=0 --path=/tmp/mycontainer Pool UUID: a171434a-05a5-4671-8fe2-615aa0d05094 Container UUID: 419b7562-5bb8-453f-bd52-917c8f5d80d1 Number of snapshots: 0 Latest Persistent Snapshot: 0 DAOS Unified Namespace Attributes on path /tmp/mycontainer: Container Type: POSIX Object Class: large Chunk Size: 4096 Container Properties \u00b6 At creation time, a list of container properties can be specified: DAOS_PROP_CO_LABEL is a string that a user can associate with a container. e.g., \"Cat Pics\" or \"ResNet-50 training data\" DAOS_PROP_CO_LAYOUT_TYPE is the container type (POSIX, MPI-IO, HDF5, ...) DAOS_PROP_CO_LAYOUT_VER is a version of the layout that can be used by I/O middleware and application to handle interoperability. DAOS_PROP_CO_CSUM defines whether checksums are enabled or disabled and the checksum type used. DAOS_PROP_CO_REDUN_FAC is the redundancy factor that drives the minimal data protection required for objects stored in the container. e.g., RF1 means no data protection, RF3 only allows 3-way replication or erasure code N+2. DAOS_PROP_CO_REDUN_LVL is the fault domain level that should be used to place data redundancy information (e.g., storage nodes, racks ...). This information will be eventually consumed to determine object placement. DAOS_PROP_CO_SNAPSHOT_MAX is the maximum number of snapshots to retain. When a new snapshot is taken, and the threshold is reached, the oldest snapshot will be automatically deleted. DAOS_PROP_CO_ACL is the list of ACL for the container. DAOS_PROP_CO_COMPRESS and DAOS_PROP_CO_ENCRYPT are reserved for configuring respectively compression and encryption. These features are currently not on the roadmap. While those properties are currently stored persistently with container metadata, many of them are still under development. The ability to modify some of these properties on an existing container will also be provided in a future release. Container Snapshot \u00b6 Similar to container create/destroy, a container can be snapshotted through the DAOS API by calling daos_cont_create_snap(). Additional functions are provided to destroy and list container snapshots. The API also provides the ability to subscribe to container snapshot events and to rollback the content of a container to a previous snapshot, but those operations are not yet fully implemented. This section will be updated once support for container snapshot is supported by the daos tool. Container User Attributes \u00b6 Similar to POSIX extended attributes, users can attach some metadata to each container through the daos_cont_{list/get/set}_attr() API. Container ACLs \u00b6 Support for per-container ACLs is scheduled for DAOS v1.2. Similar to pool ACLs, container ACLs will implement a subset of the NFSv4 ACL standard. This feature will be documented here once available. Native Programming Interface \u00b6 Building against the DAOS library \u00b6 To build application or I/O middleware against the native DAOS API, include the daos.h header file in your program and link with -Ldaos. Examples are available under src/tests. DAOS API Reference \u00b6 libdaos is written in C and uses Doxygen comments that are added to C header files. [TODO] Generate Doxygen document and add a link here. Bindings to Different Languages \u00b6 API bindings to both Python 1 and Go 2 languages are available. POSIX Filesystem \u00b6 A regular POSIX namespace can be encapsulated into a DAOS container. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed to applications or I/O frameworks either directly (e.g., for frameworks Spark or TensorFlow, or benchmark like IOR or mdtest that support different a storage backend plugin), or transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottleneck by delivering full OS bypass for POSIX read/write operations. libdfs \u00b6 DFS stands for DAOS File System and is a library that allows a DAOS container to be accessed as a hierarchical POSIX namespace. It supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and not implemented on a per-file or per-directory basis. setuid() and setgid() programs, as well as supplementary groups, are currently not supported. While libdfs can be tested from a single instance (i.e. single process or client node if used through dfuse), special care is required when the same POSIX container is mounted concurrently by multiple processes. Concurrent DFS mounts are not recommended. Support for concurrency control is under development and will be documented here once ready. dfuse \u00b6 A fuse daemon called dfuse is provided to mount a POSIX container in the local filesystem tree. dfuse exposes one mountpoint as a single DFS namespace with a single pool and container and can be mounted by regular use (provided that it is granted access to the pool and container). To mount an existing POSIX container with dfuse, run the following command: $ dfuse --pool a171434a-05a5-4671-8fe2-615aa0d05094 -s 0 --container 464e68ca-0a30-4a5f-8829-238e890899d2 -m /tmp/daos The UUID after -p and -c should be replaced with respectively the pool and container UUID. -s should be followed by the pool svc rank list and -m is the local directory where the mount point will be setup. When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos libioil \u00b6 An interception library called libioil is available to work with dfuse. This library works in conjunction with dfuse and allow to interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any appliction changes. This provides kernel-bypass for I/O data leading to improved performance. To use this set the LD_PRELOAD to point to the shared libray in the DOAS install dir LD_PRELOAD=/path/to/daos/install/lib/libioil.so Support for libioil is currently planned for DAOS v1.2. Unified Namespace \u00b6 The DAOS tier can be tightly integrated with the Lustre parallel filesystem in which DAOS containers will be represented through the Lustre namespace. This capability is under development and is scheduled for DAOS v1.2. Current state of work can be summarized as follow : DAOS integration with Lustre uses the Lustre foreign file/dir feature (from LU-11376 and associated patches) each time a DAOS POSIX container is created, using daos utility and its '--path' UNS option, a Lustre foreign file/dir of 'daos' type is being created with a specific LOV/LMV EA content that will allow to store the DAOS pool and containers UUIDs. Lustre Client patch for LU-12682, adds DAOS specific support to the Lustre foreign file/dir feature. It allows for foreign file/dir of daos type to be presented and act as <absolute-prefix>/<pool-uuid>/<container-uuid> a symlink to the Linux Kernel/VFS. the can be specified as the new daos=<absolute-prefix> Lustre Client mount option, or also thru the new llite.*.daos_prefix Lustre dynamic tuneable. And both and are extracted from foreign file/dir LOV/LMV EA. to allow for symlink resolution and transparent access to DAOS concerned container content, it is expected that a DFuse/DFS instance/mount, of DAOS Server root, exists on presenting all served pools/containers as <pool-uuid>/<container-uuid> relative paths. daos foreign support is enabled at mount time with daos= option present, or dynamically thru llite.*.daos_enable setting. HPC I/O Middleware Support \u00b6 Several HPC I/O middleware libraries have been ported to the native API. MPI-IO \u00b6 DAOS has its own MPI-IO ROM ADIO driver located in a MPICH fork on GitHub: https://github.com/daos-stack/mpich This driver has been submitted upstream for integration. To build the MPI-IO driver: export MPI_LIB=\"\" download the mpich repo from above and switch to daos_adio branch ./autogen.sh mkdir build; cd build ../configure --prefix=dir --enable-fortran=all --enable-romio --enable-cxx --enable-g=all --enable-debuginfo --with-file-system=ufs+daos --with-daos=dir --with-cart=dir make -j8; make install Switch the PATH and LD_LIBRARY_PATH to where you want to build your client apps or libs that use MPI to the installed MPICH. Build any client (HDF5, ior, mpi test suites) normally with the mpicc and mpich library installed above (see child pages). To run an example: Launch DAOS server(s) and create a pool as specified in the previous section. This will return a pool uuid \"puuid\" and service rank list \"svcl\" At the client side, the following environment variables need to be set: export PATH=/path/to/mpich/install/bin:$PATH export LD_LIBRARY_PATH=/path/to/mpich/install/lib:$LD_LIBRARY_PATH export MPI_LIB=\"\" export CRT_ATTACH_INFO_PATH=/path/ (whatever was passed to daos_server start -a) export DAOS_SINGLETON_CLI=1 export DAOS_POOL=puuid; export DAOS_SVCL=svcl This is just temporary till we have a better way of passing pool connect info to MPI-IO and other middleware over DAOS. Run the client application or test. Limitations to the current implementation include: Incorrect MPI_File_set_size and MPI_File_get_size - This will be fixed in the future when DAOS correctly supports records enumeration after punch or key query for max/min key and recx. Reading Holes does not return 0, but leaves the buffer untouched No support for MPI file atomicity, preallocate, shared file pointers. HDF5 \u00b6 A prototype version of an HDF5 DAOS connector is available. Please refer to the DAOS VOL connector user guide 3 for instructions on how to build and use it. Spark Support \u00b6 Spark integration with libdfs is under development and is scheduled for DAOS v1.0 or v1.2. Data Migration \u00b6 Migration to/from a POSIX filesystem \u00b6 A dataset mover tool is under consideration to move a snapshot of a POSIX, MPI-IO or HDF5 container to a POSIX filesystem and vice versa. The copy will be performed at the POSIX or HDF5 level. The resulting HDF5 file over the POSIX filesystem will be accessible through the native HDF5 connector with the POSIX VFD. The first version of the mover tool is currently scheduled for DAOS v1.4. Container Parking \u00b6 The mover tool will also eventually support the ability to serialize and deserialize a DAOS container to a set of POSIX files that can be stored or \"parked\" in an external POSIX filesystem. This transformation is agnostic to the data model and container type and will retain all DAOS internal metadata. https://github.com/daos-stack/daos/blob/master/src/client/pydaos/raw/README.md \u21a9 https://godoc.org/github.com/daos-stack/go-daos/pkg/daos \u21a9 https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/README.md \u21a9","title":"Application Interface and Tiering"},{"location":"admin/app_interface_tiering/#application-interface-and-tiering","text":"","title":"Application Interface and Tiering"},{"location":"admin/app_interface_tiering/#daos-container-management","text":"DAOS containers are the unit of data management for users.","title":"DAOS Container Management"},{"location":"admin/app_interface_tiering/#container-creationdestroy","text":"Containers can be created and destroyed through the daos_cont_create/destroy() functions exported by the DAOS API. A user tool called daos is also provided to manage containers. To create a container: $ daos container create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 --svc=0 Successfully created container 008123fc-6b6c-4768-a88a-a2a5ef34a1a2 The container type (i.e., POSIX or HDF5) can be passed via the --type option. As shown below, the pool UUID, container UUID, and container attributes can be stored in the extended attributes of a POSIX file or directory for convenience. Then subsequent invocations of the daos tools need to reference the path to the POSIX file or directory. $ daos container create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 --svc=0 --path=/tmp/mycontainer --type=POSIX --oclass=large --chunk_size=4K Successfully created container 419b7562-5bb8-453f-bd52-917c8f5d80d1 type POSIX $ daos container query --svc=0 --path=/tmp/mycontainer Pool UUID: a171434a-05a5-4671-8fe2-615aa0d05094 Container UUID: 419b7562-5bb8-453f-bd52-917c8f5d80d1 Number of snapshots: 0 Latest Persistent Snapshot: 0 DAOS Unified Namespace Attributes on path /tmp/mycontainer: Container Type: POSIX Object Class: large Chunk Size: 4096","title":"Container Creation/Destroy"},{"location":"admin/app_interface_tiering/#container-properties","text":"At creation time, a list of container properties can be specified: DAOS_PROP_CO_LABEL is a string that a user can associate with a container. e.g., \"Cat Pics\" or \"ResNet-50 training data\" DAOS_PROP_CO_LAYOUT_TYPE is the container type (POSIX, MPI-IO, HDF5, ...) DAOS_PROP_CO_LAYOUT_VER is a version of the layout that can be used by I/O middleware and application to handle interoperability. DAOS_PROP_CO_CSUM defines whether checksums are enabled or disabled and the checksum type used. DAOS_PROP_CO_REDUN_FAC is the redundancy factor that drives the minimal data protection required for objects stored in the container. e.g., RF1 means no data protection, RF3 only allows 3-way replication or erasure code N+2. DAOS_PROP_CO_REDUN_LVL is the fault domain level that should be used to place data redundancy information (e.g., storage nodes, racks ...). This information will be eventually consumed to determine object placement. DAOS_PROP_CO_SNAPSHOT_MAX is the maximum number of snapshots to retain. When a new snapshot is taken, and the threshold is reached, the oldest snapshot will be automatically deleted. DAOS_PROP_CO_ACL is the list of ACL for the container. DAOS_PROP_CO_COMPRESS and DAOS_PROP_CO_ENCRYPT are reserved for configuring respectively compression and encryption. These features are currently not on the roadmap. While those properties are currently stored persistently with container metadata, many of them are still under development. The ability to modify some of these properties on an existing container will also be provided in a future release.","title":"Container Properties"},{"location":"admin/app_interface_tiering/#container-snapshot","text":"Similar to container create/destroy, a container can be snapshotted through the DAOS API by calling daos_cont_create_snap(). Additional functions are provided to destroy and list container snapshots. The API also provides the ability to subscribe to container snapshot events and to rollback the content of a container to a previous snapshot, but those operations are not yet fully implemented. This section will be updated once support for container snapshot is supported by the daos tool.","title":"Container Snapshot"},{"location":"admin/app_interface_tiering/#container-user-attributes","text":"Similar to POSIX extended attributes, users can attach some metadata to each container through the daos_cont_{list/get/set}_attr() API.","title":"Container User Attributes"},{"location":"admin/app_interface_tiering/#container-acls","text":"Support for per-container ACLs is scheduled for DAOS v1.2. Similar to pool ACLs, container ACLs will implement a subset of the NFSv4 ACL standard. This feature will be documented here once available.","title":"Container ACLs"},{"location":"admin/app_interface_tiering/#native-programming-interface","text":"","title":"Native Programming Interface"},{"location":"admin/app_interface_tiering/#building-against-the-daos-library","text":"To build application or I/O middleware against the native DAOS API, include the daos.h header file in your program and link with -Ldaos. Examples are available under src/tests.","title":"Building against the DAOS library"},{"location":"admin/app_interface_tiering/#daos-api-reference","text":"libdaos is written in C and uses Doxygen comments that are added to C header files. [TODO] Generate Doxygen document and add a link here.","title":"DAOS API Reference"},{"location":"admin/app_interface_tiering/#bindings-to-different-languages","text":"API bindings to both Python 1 and Go 2 languages are available.","title":"Bindings to Different Languages"},{"location":"admin/app_interface_tiering/#posix-filesystem","text":"A regular POSIX namespace can be encapsulated into a DAOS container. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed to applications or I/O frameworks either directly (e.g., for frameworks Spark or TensorFlow, or benchmark like IOR or mdtest that support different a storage backend plugin), or transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottleneck by delivering full OS bypass for POSIX read/write operations.","title":"POSIX Filesystem"},{"location":"admin/app_interface_tiering/#libdfs","text":"DFS stands for DAOS File System and is a library that allows a DAOS container to be accessed as a hierarchical POSIX namespace. It supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and not implemented on a per-file or per-directory basis. setuid() and setgid() programs, as well as supplementary groups, are currently not supported. While libdfs can be tested from a single instance (i.e. single process or client node if used through dfuse), special care is required when the same POSIX container is mounted concurrently by multiple processes. Concurrent DFS mounts are not recommended. Support for concurrency control is under development and will be documented here once ready.","title":"libdfs"},{"location":"admin/app_interface_tiering/#dfuse","text":"A fuse daemon called dfuse is provided to mount a POSIX container in the local filesystem tree. dfuse exposes one mountpoint as a single DFS namespace with a single pool and container and can be mounted by regular use (provided that it is granted access to the pool and container). To mount an existing POSIX container with dfuse, run the following command: $ dfuse --pool a171434a-05a5-4671-8fe2-615aa0d05094 -s 0 --container 464e68ca-0a30-4a5f-8829-238e890899d2 -m /tmp/daos The UUID after -p and -c should be replaced with respectively the pool and container UUID. -s should be followed by the pool svc rank list and -m is the local directory where the mount point will be setup. When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos","title":"dfuse"},{"location":"admin/app_interface_tiering/#libioil","text":"An interception library called libioil is available to work with dfuse. This library works in conjunction with dfuse and allow to interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any appliction changes. This provides kernel-bypass for I/O data leading to improved performance. To use this set the LD_PRELOAD to point to the shared libray in the DOAS install dir LD_PRELOAD=/path/to/daos/install/lib/libioil.so Support for libioil is currently planned for DAOS v1.2.","title":"libioil"},{"location":"admin/app_interface_tiering/#unified-namespace","text":"The DAOS tier can be tightly integrated with the Lustre parallel filesystem in which DAOS containers will be represented through the Lustre namespace. This capability is under development and is scheduled for DAOS v1.2. Current state of work can be summarized as follow : DAOS integration with Lustre uses the Lustre foreign file/dir feature (from LU-11376 and associated patches) each time a DAOS POSIX container is created, using daos utility and its '--path' UNS option, a Lustre foreign file/dir of 'daos' type is being created with a specific LOV/LMV EA content that will allow to store the DAOS pool and containers UUIDs. Lustre Client patch for LU-12682, adds DAOS specific support to the Lustre foreign file/dir feature. It allows for foreign file/dir of daos type to be presented and act as <absolute-prefix>/<pool-uuid>/<container-uuid> a symlink to the Linux Kernel/VFS. the can be specified as the new daos=<absolute-prefix> Lustre Client mount option, or also thru the new llite.*.daos_prefix Lustre dynamic tuneable. And both and are extracted from foreign file/dir LOV/LMV EA. to allow for symlink resolution and transparent access to DAOS concerned container content, it is expected that a DFuse/DFS instance/mount, of DAOS Server root, exists on presenting all served pools/containers as <pool-uuid>/<container-uuid> relative paths. daos foreign support is enabled at mount time with daos= option present, or dynamically thru llite.*.daos_enable setting.","title":"Unified Namespace"},{"location":"admin/app_interface_tiering/#hpc-io-middleware-support","text":"Several HPC I/O middleware libraries have been ported to the native API.","title":"HPC I/O Middleware Support"},{"location":"admin/app_interface_tiering/#mpi-io","text":"DAOS has its own MPI-IO ROM ADIO driver located in a MPICH fork on GitHub: https://github.com/daos-stack/mpich This driver has been submitted upstream for integration. To build the MPI-IO driver: export MPI_LIB=\"\" download the mpich repo from above and switch to daos_adio branch ./autogen.sh mkdir build; cd build ../configure --prefix=dir --enable-fortran=all --enable-romio --enable-cxx --enable-g=all --enable-debuginfo --with-file-system=ufs+daos --with-daos=dir --with-cart=dir make -j8; make install Switch the PATH and LD_LIBRARY_PATH to where you want to build your client apps or libs that use MPI to the installed MPICH. Build any client (HDF5, ior, mpi test suites) normally with the mpicc and mpich library installed above (see child pages). To run an example: Launch DAOS server(s) and create a pool as specified in the previous section. This will return a pool uuid \"puuid\" and service rank list \"svcl\" At the client side, the following environment variables need to be set: export PATH=/path/to/mpich/install/bin:$PATH export LD_LIBRARY_PATH=/path/to/mpich/install/lib:$LD_LIBRARY_PATH export MPI_LIB=\"\" export CRT_ATTACH_INFO_PATH=/path/ (whatever was passed to daos_server start -a) export DAOS_SINGLETON_CLI=1 export DAOS_POOL=puuid; export DAOS_SVCL=svcl This is just temporary till we have a better way of passing pool connect info to MPI-IO and other middleware over DAOS. Run the client application or test. Limitations to the current implementation include: Incorrect MPI_File_set_size and MPI_File_get_size - This will be fixed in the future when DAOS correctly supports records enumeration after punch or key query for max/min key and recx. Reading Holes does not return 0, but leaves the buffer untouched No support for MPI file atomicity, preallocate, shared file pointers.","title":"MPI-IO"},{"location":"admin/app_interface_tiering/#hdf5","text":"A prototype version of an HDF5 DAOS connector is available. Please refer to the DAOS VOL connector user guide 3 for instructions on how to build and use it.","title":"HDF5"},{"location":"admin/app_interface_tiering/#spark-support","text":"Spark integration with libdfs is under development and is scheduled for DAOS v1.0 or v1.2.","title":"Spark Support"},{"location":"admin/app_interface_tiering/#data-migration","text":"","title":"Data Migration"},{"location":"admin/app_interface_tiering/#migration-tofrom-a-posix-filesystem","text":"A dataset mover tool is under consideration to move a snapshot of a POSIX, MPI-IO or HDF5 container to a POSIX filesystem and vice versa. The copy will be performed at the POSIX or HDF5 level. The resulting HDF5 file over the POSIX filesystem will be accessible through the native HDF5 connector with the POSIX VFD. The first version of the mover tool is currently scheduled for DAOS v1.4.","title":"Migration to/from a POSIX filesystem"},{"location":"admin/app_interface_tiering/#container-parking","text":"The mover tool will also eventually support the ability to serialize and deserialize a DAOS container to a set of POSIX files that can be stored or \"parked\" in an external POSIX filesystem. This transformation is agnostic to the data model and container type and will retain all DAOS internal metadata. https://github.com/daos-stack/daos/blob/master/src/client/pydaos/raw/README.md \u21a9 https://godoc.org/github.com/daos-stack/go-daos/pkg/daos \u21a9 https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/README.md \u21a9","title":"Container Parking"},{"location":"admin/architecture/","text":"DAOS Architecture \u00b6 DAOS is an open-source software-defined scale-out object store that provides high bandwidth and high IOPS storage containers to applications and enables next-generation data-centric workflows combining simulation, data analytics, and machine learning. Unlike the traditional storage stacks that were primarily designed for rotating media, DAOS is architected from the ground up to exploit new NVM technologies and is extremely lightweight since it operates End-to-End (E2E) in user space with full OS bypass. DAOS offers a shift away from an I/O model designed for block-based and high-latency storage to one that inherently supports fine-grained data access and unlocks the performance of the next-generation storage technologies. Unlike traditional Burst Buffers, DAOS is a high-performant independent and fault-tolerant storage tier that does not rely on a third-party tier to manage metadata and data resilience. DAOS Features \u00b6 DAOS relies on OFI for low-latency communications and stores data on both storage-class memory and NVMe storage. DAOS presents a native key-array-value storage interface that offers a unified storage model over which domain-specific data models are ported, such as HDF5, MPI-IO, and Apache Arrow. A POSIX I/O emulation layer implementing files and directories over the native DAOS API is also available. DAOS I/O operations are logged and then inserted into a persistent index maintained in SCM. Each I/O is tagged with a particular timestamp called epoch and is associated with a particular version of the dataset. No read-modify-write operations are performed internally. Write operations are non-destructive and not sensitive to alignment. Upon read request, the DAOS service walks through the persistent index and creates a complex scatter-gather Remote Direct Memory Access (RDMA) descriptor to reconstruct the data at the requested version directly in the buffer provided by the application. The SCM storage is memory-mapped directly into the address space of the DAOS service that manages the persistent index via direct load/store. Depending on the I/O characteristics, the DAOS service can decide to store the I/O in either SCM or NVMe storage. As represented in Figure 2\u20111, latency-sensitive I/Os, like application metadata and byte-granular data, will typically be stored in the former, whereas checkpoints and bulk data will be stored in the latter. This approach allows DAOS to deliver the raw NVMe bandwidth for bulk data by streaming the data to NVMe storage and maintaining internal metadata index in SCM. The Persistent Memory Development Kit (PMDK) 1 allows managing transactional access to SCM and the Storage Performance Development Kit (SPDK) 2 enables user-space I/O to NVMe devices. Figure 2\u20111. DAOS Storage DAOS aims at delivering: High throughput and IOPS at arbitrary alignment and size Fine-grained I/O operations with true zero-copy I/O to SCM Support for massively distributed NVM storage via scalable collective communications across the storage servers Non-blocking data and metadata operations to allow I/O and computation to overlap Advanced data placement taking into account fault domains Software-managed redundancy supporting both replication and erasure code with an online rebuild End-to-end data integrity Scalable distributed transactions with guaranteed data consistency and automated recovery Dataset snapshot Security framework to manage access control to storage pools Software-defined storage management to provision, configure, modify and monitor storage pools over COTS hardware Native support for Hierarchical Data Format (HDF)5, MPI-IO and POSIX namespace over the DAOS data model Tools for disaster recovery Seamless integration with the Lustre parallel filesystem Mover agent to migrate datasets among DAOS pools and from parallel filesystems to DAOS and vice versa DAOS Components \u00b6 A data center may have hundreds of thousands of compute nodes interconnected via a scalable high-performance fabric, where all, or a subset of the nodes called storage nodes, have direct access to NVM storage. A DAOS installation involves several components that can be either collocated or distributed. DAOS Target, Server and System \u00b6 The DAOS server is a multi-tenant daemon running on a Linux instance (i.e. natively on the physical node or in a VM or container) of each storage node and exporting through the network the locally-attached NVM storage. It listens to a management port, addressed by an IP address and a TCP port number, plus one or more fabric endpoints, addressed by network URIs. The DAOS server is configured through a YAML file and can be integrated with different daemon management or orchestration frameworks (e.g., a systemd script, a Kubernetes service or even via a parallel launcher like pdsh or srun). A DAOS system is identified by a system name and consists of a set of DAOS servers connected to the same fabric. Membership of the DAOS servers is recorded into the system map that assigns a unique integer rank to each server. Two different systems comprise two disjoint sets of servers and do not coordinate with each other. Inside a DAOS server, the storage is statically partitioned across multiple targets to optimize concurrency. To avoid contention, each target has its private storage, own pool of service threads and dedicated network context that can be directly addressed over the fabric independently of the other targets hosted on the same storage node. A target is typically associated with a single-ported SCM module and NVMe SSD attached to a single storage node. Moreover, a target does not implement any internal data protection mechanism against storage media failure. As a result, a target is a single point of failure. A dynamic state is associated with each target and is set to either up and running, or down and not available. A target is the unit of performance. Hardware components associated with the target, such as the backend storage medium, the server, and the network, have limited capability and capacity. The number of targets exported by a DAOS server instance is configurable and depends on the underlying hardware (i.e., the number of SCM modules, CPUs, NVMe SSDs ...). A target is the unit of fault. Storage API, Application Interface and Tools \u00b6 Applications, users, and administrators can interact with a DAOS system through two different client APIs. The management API offers the ability to administrate a DAOS system. It is intended to be integrated with different vendor-specific storage management or open-source orchestration frameworks. A CLI tool is built over the DAOS management API. On the other hand, the DAOS library (i.e., libdaos) implements the DAOS storage model and is primarily targeted at application and I/O middleware developers who want to store datasets in a DAOS system. User utilities are also built over the API to allow users to manage datasets from a CLI. Applications can access datasets stored in DAOS either directly through the native DAOS API or an I/O middleware libraries (e.g. POSIX emulation, MPI-IO, HDF5) or frameworks (e.g., Spark, TensorFlow) already integrated with the native DAOS storage model. Agent \u00b6 The DAOS agent is a daemon residing on the client node that interacts with the DAOS library to authenticate the application process. It is a trusted entity that can sign the DAOS Client credentials using certificates. The agent can support different authentication frameworks and uses a Unix Domain Socket to communicate with the client library. Storage Model \u00b6 A DAOS pool is a storage reservation distributed across a collection of targets. The actual space allocated to the pool on each target is called a pool shard. The total space allocated to a pool is decided at creation time and can be expanded over time by resizing all the pool shards (within the limit of the storage capacity dedicated to each target) or by spanning more targets (i.e., adding more pool shards). A pool offers storage virtualization and is the unit of provisioning and isolation. DAOS pools cannot span across multiple systems. A pool can host multiple transactional object store called DAOS containers. Each container is a private object address space, which can be modified transactional and independently of the other containers stored in the same pool. A container is the unit of snapshot and data management. DAOS objects belonging to a container can be distributed across any target of the pool for both performance and resilience and can be accessed through different APIs to represent structured, semi-structured and unstructured data efficiently. Figure 2\u20112 illustrates the different DAOS abstractions. Figure 2\u20112. Example of four Storage Nodes, eight DAOS Targets, and three DAOS Pools Table 2\u20111 shows the targeted level of scalability for each DAOS abstraction. Table 2\u20111. DAOS Scalability DAOS Concept Component Order of Magnitude Limit System 10 2 Pools (hundreds) Pool 10 2 Containers (hundreds) Container 10 9 Objects (billions) DAOS Pool \u00b6 A Pool is identified by a unique UUID and maintains target memberships in the pool map stored in persistent memory. The pool map not only records the list of active targets, it also contains the storage topology under the form of a tree that is used to identify targets sharing common hardware components. For instance, the first level of the tree can represent targets sharing the same motherboard, and then the second level can represent all motherboards sharing the same rack and finally the third level can represent all racks in the same cage. This framework effectively represents hierarchical fault domains, which are then used to avoid placing redundant data on targets subject to correlated failures. At any point in time, new targets can be added to the pool map, and failed ones can be excluded. Moreover, the pool map is fully versioned, which effectively assigns a unique sequence to each modification of the map, more particularly for failed node removal. A pool shard is a reservation of NVM storage (i.e., SCM optionally combined with a pre-allocated space on NVMe storage) on a specific target. It has a fixed capacity and fails operations when full. Current space usage can be queried at any time and reports the total amount of bytes used by any data type stored in the pool shard. Space consumed on the different type of storage is reported separately. Upon target failure and exclusion from the pool map, data redundancy inside the pool is automatically restored while the pool remains online. Rebuild progress is recorded regularly in special logs in the pool stored in persistent memory to address cascading failures. When new targets are added, data is automatically migrated to the newly added targets to redistribute space usage equally among all the members. This process is known as space rebalancing and uses dedicated persistent logs as well to support interruption and restart. A pool is a set of targets spread across different storage nodes over which data and metadata are distributed to achieve horizontal scalability, and replicated or erasure-coded to ensure durability and availability. When creating a pool, a set of system properties must be defined to configure the different features supported by the pool. In addition, the user can define their own attributes that will be stored persistently. A pool is only accessible to authenticated and authorized applications. Multiple security frameworks could be supported, from NFSv4 access control lists to third party-based authentication (such as Kerberos). Security is enforced when connecting to the pool. Upon successful connection to the pool, a connection context is returned to the application process. A pool stores many different sorts of persistent metadata, such as the pool map, authentication, and authorization information, user attributes, properties, and rebuild logs. Such metadata are critical and require the highest level of resiliency. Therefore, the pool metadata are replicated on a few nodes from distinct high-level fault domains. For very large configurations with hundreds of thousands of storage nodes, only a very small fraction of those nodes (in the order of tens) run the pool metadata service. With a limited number of storage nodes, DAOS can afford to rely on a consensus algorithm to reach agreement and to guarantee consistency in the presence of faults and to avoid split-brain syndrome. DAOS Container \u00b6 A container represents an object address space inside a pool and is identified by a UUID. Applications (i.e., directly or via I/O middleware, domain-specific data format, big data or AI frameworks) store all related datasets into a container which is the unit of storage management for the user. Like pools, containers can store user attributes and a set of properties must be passed at container creation time to configure different features like checksums. Objects in a container are identified by a unique 128-bit object address and may have different schemas for data distribution and redundancy over targets. Dynamic or static striping, replication or erasure code are some parameters required to define the object schema. The object class defines common schema attributes for a set of objects. Each object class is assigned a unique identifier and is associated with a given schema at the pool level. A new object class can be defined at any time with a configurable schema, which is then immutable after creation, or at least until all objects belonging to the class have been destroyed. For convenience, several object classes expected to be the most commonly used will be predefined by default when the pool is created, as shown in Table 2\u20112. Table 2\u20112. Sample of Pre-defined Object Classes Object Class (RW = read/write, RM = read-mostly Redundancy Metadata in OIT, (SC = stripe count, RC = replica count, PC = parity count, TGT = target Small size & RW Replication No (static SCxRC, e.g. 1x4) Small size & RM Erasure code No (static SC+PC, e.g. 4+2) Large size & RW Replication No (static SCxRC over max #targets) Large size & RM Erasure code No (static SCx(SC+PC) w/ max #TGT) Unknown size & RW Replication SCxRC (e.g. 1x4 initially and grows) Unknown size & RM Erasure code SC+PC (e.g. 4+2 initially and grows) A container is the unit of transaction and snapshot. Container metadata (i.e. list of snapshots, container open handles, object class, user attributes, properties, etc.) are stored in persistent memory and maintained by a dedicated container metadata service that either uses the same replicated engine as the parent metadata pool service, or has its own engine. DAOS Object \u00b6 To avoid scaling problems and overhead common to a traditional storage system, DAOS objects are intentionally simple. No default object metadata beyond the type and schema are provided. This means that the system does not maintain time, size, owner, permissions or even track openers. To achieve high availability and horizontal scalability, many object schemas (replication/erasure code, static/dynamic striping, and others) are provided. The schema framework is flexible and easily expandable to allow for new custom schema types in the future. The layout is generated algorithmically on an object open from the object identifier and the pool map. End-to-end integrity is assured by protecting object data with checksums during network transfer and storage. A DAOS object can be accessed through different native interfaces exported by libdaos: multi-level key-array, key-value or array APIs that allows representing efficiently structured, semi-structured or unstructured data. http://pmem.io/pmdk/ \u21a9 http://www.spdk.io/ \u21a9","title":"DAOS Architecture"},{"location":"admin/architecture/#daos-architecture","text":"DAOS is an open-source software-defined scale-out object store that provides high bandwidth and high IOPS storage containers to applications and enables next-generation data-centric workflows combining simulation, data analytics, and machine learning. Unlike the traditional storage stacks that were primarily designed for rotating media, DAOS is architected from the ground up to exploit new NVM technologies and is extremely lightweight since it operates End-to-End (E2E) in user space with full OS bypass. DAOS offers a shift away from an I/O model designed for block-based and high-latency storage to one that inherently supports fine-grained data access and unlocks the performance of the next-generation storage technologies. Unlike traditional Burst Buffers, DAOS is a high-performant independent and fault-tolerant storage tier that does not rely on a third-party tier to manage metadata and data resilience.","title":"DAOS Architecture"},{"location":"admin/architecture/#daos-features","text":"DAOS relies on OFI for low-latency communications and stores data on both storage-class memory and NVMe storage. DAOS presents a native key-array-value storage interface that offers a unified storage model over which domain-specific data models are ported, such as HDF5, MPI-IO, and Apache Arrow. A POSIX I/O emulation layer implementing files and directories over the native DAOS API is also available. DAOS I/O operations are logged and then inserted into a persistent index maintained in SCM. Each I/O is tagged with a particular timestamp called epoch and is associated with a particular version of the dataset. No read-modify-write operations are performed internally. Write operations are non-destructive and not sensitive to alignment. Upon read request, the DAOS service walks through the persistent index and creates a complex scatter-gather Remote Direct Memory Access (RDMA) descriptor to reconstruct the data at the requested version directly in the buffer provided by the application. The SCM storage is memory-mapped directly into the address space of the DAOS service that manages the persistent index via direct load/store. Depending on the I/O characteristics, the DAOS service can decide to store the I/O in either SCM or NVMe storage. As represented in Figure 2\u20111, latency-sensitive I/Os, like application metadata and byte-granular data, will typically be stored in the former, whereas checkpoints and bulk data will be stored in the latter. This approach allows DAOS to deliver the raw NVMe bandwidth for bulk data by streaming the data to NVMe storage and maintaining internal metadata index in SCM. The Persistent Memory Development Kit (PMDK) 1 allows managing transactional access to SCM and the Storage Performance Development Kit (SPDK) 2 enables user-space I/O to NVMe devices. Figure 2\u20111. DAOS Storage DAOS aims at delivering: High throughput and IOPS at arbitrary alignment and size Fine-grained I/O operations with true zero-copy I/O to SCM Support for massively distributed NVM storage via scalable collective communications across the storage servers Non-blocking data and metadata operations to allow I/O and computation to overlap Advanced data placement taking into account fault domains Software-managed redundancy supporting both replication and erasure code with an online rebuild End-to-end data integrity Scalable distributed transactions with guaranteed data consistency and automated recovery Dataset snapshot Security framework to manage access control to storage pools Software-defined storage management to provision, configure, modify and monitor storage pools over COTS hardware Native support for Hierarchical Data Format (HDF)5, MPI-IO and POSIX namespace over the DAOS data model Tools for disaster recovery Seamless integration with the Lustre parallel filesystem Mover agent to migrate datasets among DAOS pools and from parallel filesystems to DAOS and vice versa","title":"DAOS Features"},{"location":"admin/architecture/#daos-components","text":"A data center may have hundreds of thousands of compute nodes interconnected via a scalable high-performance fabric, where all, or a subset of the nodes called storage nodes, have direct access to NVM storage. A DAOS installation involves several components that can be either collocated or distributed.","title":"DAOS Components"},{"location":"admin/architecture/#daos-target-server-and-system","text":"The DAOS server is a multi-tenant daemon running on a Linux instance (i.e. natively on the physical node or in a VM or container) of each storage node and exporting through the network the locally-attached NVM storage. It listens to a management port, addressed by an IP address and a TCP port number, plus one or more fabric endpoints, addressed by network URIs. The DAOS server is configured through a YAML file and can be integrated with different daemon management or orchestration frameworks (e.g., a systemd script, a Kubernetes service or even via a parallel launcher like pdsh or srun). A DAOS system is identified by a system name and consists of a set of DAOS servers connected to the same fabric. Membership of the DAOS servers is recorded into the system map that assigns a unique integer rank to each server. Two different systems comprise two disjoint sets of servers and do not coordinate with each other. Inside a DAOS server, the storage is statically partitioned across multiple targets to optimize concurrency. To avoid contention, each target has its private storage, own pool of service threads and dedicated network context that can be directly addressed over the fabric independently of the other targets hosted on the same storage node. A target is typically associated with a single-ported SCM module and NVMe SSD attached to a single storage node. Moreover, a target does not implement any internal data protection mechanism against storage media failure. As a result, a target is a single point of failure. A dynamic state is associated with each target and is set to either up and running, or down and not available. A target is the unit of performance. Hardware components associated with the target, such as the backend storage medium, the server, and the network, have limited capability and capacity. The number of targets exported by a DAOS server instance is configurable and depends on the underlying hardware (i.e., the number of SCM modules, CPUs, NVMe SSDs ...). A target is the unit of fault.","title":"DAOS Target, Server and System"},{"location":"admin/architecture/#storage-api-application-interface-and-tools","text":"Applications, users, and administrators can interact with a DAOS system through two different client APIs. The management API offers the ability to administrate a DAOS system. It is intended to be integrated with different vendor-specific storage management or open-source orchestration frameworks. A CLI tool is built over the DAOS management API. On the other hand, the DAOS library (i.e., libdaos) implements the DAOS storage model and is primarily targeted at application and I/O middleware developers who want to store datasets in a DAOS system. User utilities are also built over the API to allow users to manage datasets from a CLI. Applications can access datasets stored in DAOS either directly through the native DAOS API or an I/O middleware libraries (e.g. POSIX emulation, MPI-IO, HDF5) or frameworks (e.g., Spark, TensorFlow) already integrated with the native DAOS storage model.","title":"Storage API, Application Interface and Tools"},{"location":"admin/architecture/#agent","text":"The DAOS agent is a daemon residing on the client node that interacts with the DAOS library to authenticate the application process. It is a trusted entity that can sign the DAOS Client credentials using certificates. The agent can support different authentication frameworks and uses a Unix Domain Socket to communicate with the client library.","title":"Agent"},{"location":"admin/architecture/#storage-model","text":"A DAOS pool is a storage reservation distributed across a collection of targets. The actual space allocated to the pool on each target is called a pool shard. The total space allocated to a pool is decided at creation time and can be expanded over time by resizing all the pool shards (within the limit of the storage capacity dedicated to each target) or by spanning more targets (i.e., adding more pool shards). A pool offers storage virtualization and is the unit of provisioning and isolation. DAOS pools cannot span across multiple systems. A pool can host multiple transactional object store called DAOS containers. Each container is a private object address space, which can be modified transactional and independently of the other containers stored in the same pool. A container is the unit of snapshot and data management. DAOS objects belonging to a container can be distributed across any target of the pool for both performance and resilience and can be accessed through different APIs to represent structured, semi-structured and unstructured data efficiently. Figure 2\u20112 illustrates the different DAOS abstractions. Figure 2\u20112. Example of four Storage Nodes, eight DAOS Targets, and three DAOS Pools Table 2\u20111 shows the targeted level of scalability for each DAOS abstraction. Table 2\u20111. DAOS Scalability DAOS Concept Component Order of Magnitude Limit System 10 2 Pools (hundreds) Pool 10 2 Containers (hundreds) Container 10 9 Objects (billions)","title":"Storage Model"},{"location":"admin/architecture/#daos-pool","text":"A Pool is identified by a unique UUID and maintains target memberships in the pool map stored in persistent memory. The pool map not only records the list of active targets, it also contains the storage topology under the form of a tree that is used to identify targets sharing common hardware components. For instance, the first level of the tree can represent targets sharing the same motherboard, and then the second level can represent all motherboards sharing the same rack and finally the third level can represent all racks in the same cage. This framework effectively represents hierarchical fault domains, which are then used to avoid placing redundant data on targets subject to correlated failures. At any point in time, new targets can be added to the pool map, and failed ones can be excluded. Moreover, the pool map is fully versioned, which effectively assigns a unique sequence to each modification of the map, more particularly for failed node removal. A pool shard is a reservation of NVM storage (i.e., SCM optionally combined with a pre-allocated space on NVMe storage) on a specific target. It has a fixed capacity and fails operations when full. Current space usage can be queried at any time and reports the total amount of bytes used by any data type stored in the pool shard. Space consumed on the different type of storage is reported separately. Upon target failure and exclusion from the pool map, data redundancy inside the pool is automatically restored while the pool remains online. Rebuild progress is recorded regularly in special logs in the pool stored in persistent memory to address cascading failures. When new targets are added, data is automatically migrated to the newly added targets to redistribute space usage equally among all the members. This process is known as space rebalancing and uses dedicated persistent logs as well to support interruption and restart. A pool is a set of targets spread across different storage nodes over which data and metadata are distributed to achieve horizontal scalability, and replicated or erasure-coded to ensure durability and availability. When creating a pool, a set of system properties must be defined to configure the different features supported by the pool. In addition, the user can define their own attributes that will be stored persistently. A pool is only accessible to authenticated and authorized applications. Multiple security frameworks could be supported, from NFSv4 access control lists to third party-based authentication (such as Kerberos). Security is enforced when connecting to the pool. Upon successful connection to the pool, a connection context is returned to the application process. A pool stores many different sorts of persistent metadata, such as the pool map, authentication, and authorization information, user attributes, properties, and rebuild logs. Such metadata are critical and require the highest level of resiliency. Therefore, the pool metadata are replicated on a few nodes from distinct high-level fault domains. For very large configurations with hundreds of thousands of storage nodes, only a very small fraction of those nodes (in the order of tens) run the pool metadata service. With a limited number of storage nodes, DAOS can afford to rely on a consensus algorithm to reach agreement and to guarantee consistency in the presence of faults and to avoid split-brain syndrome.","title":"DAOS Pool"},{"location":"admin/architecture/#daos-container","text":"A container represents an object address space inside a pool and is identified by a UUID. Applications (i.e., directly or via I/O middleware, domain-specific data format, big data or AI frameworks) store all related datasets into a container which is the unit of storage management for the user. Like pools, containers can store user attributes and a set of properties must be passed at container creation time to configure different features like checksums. Objects in a container are identified by a unique 128-bit object address and may have different schemas for data distribution and redundancy over targets. Dynamic or static striping, replication or erasure code are some parameters required to define the object schema. The object class defines common schema attributes for a set of objects. Each object class is assigned a unique identifier and is associated with a given schema at the pool level. A new object class can be defined at any time with a configurable schema, which is then immutable after creation, or at least until all objects belonging to the class have been destroyed. For convenience, several object classes expected to be the most commonly used will be predefined by default when the pool is created, as shown in Table 2\u20112. Table 2\u20112. Sample of Pre-defined Object Classes Object Class (RW = read/write, RM = read-mostly Redundancy Metadata in OIT, (SC = stripe count, RC = replica count, PC = parity count, TGT = target Small size & RW Replication No (static SCxRC, e.g. 1x4) Small size & RM Erasure code No (static SC+PC, e.g. 4+2) Large size & RW Replication No (static SCxRC over max #targets) Large size & RM Erasure code No (static SCx(SC+PC) w/ max #TGT) Unknown size & RW Replication SCxRC (e.g. 1x4 initially and grows) Unknown size & RM Erasure code SC+PC (e.g. 4+2 initially and grows) A container is the unit of transaction and snapshot. Container metadata (i.e. list of snapshots, container open handles, object class, user attributes, properties, etc.) are stored in persistent memory and maintained by a dedicated container metadata service that either uses the same replicated engine as the parent metadata pool service, or has its own engine.","title":"DAOS Container"},{"location":"admin/architecture/#daos-object","text":"To avoid scaling problems and overhead common to a traditional storage system, DAOS objects are intentionally simple. No default object metadata beyond the type and schema are provided. This means that the system does not maintain time, size, owner, permissions or even track openers. To achieve high availability and horizontal scalability, many object schemas (replication/erasure code, static/dynamic striping, and others) are provided. The schema framework is flexible and easily expandable to allow for new custom schema types in the future. The layout is generated algorithmically on an object open from the object identifier and the pool map. End-to-end integrity is assured by protecting object data with checksums during network transfer and storage. A DAOS object can be accessed through different native interfaces exported by libdaos: multi-level key-array, key-value or array APIs that allows representing efficiently structured, semi-structured or unstructured data. http://pmem.io/pmdk/ \u21a9 http://www.spdk.io/ \u21a9","title":"DAOS Object"},{"location":"admin/deployment/","text":"System Deployment \u00b6 The DAOS deployment workflow requires to start the DAOS server instances early on to enable administrators to perform remote operations in parallel across multiple storage nodes via the dmg management utility. Security is guaranteed via the use of certificates. The first type of commands run after installation include network and storage hardware provisioning and would typically be run from a login node. After daos_server instances have been started on each storage node for the first time, dmg storage prepare will set DCPM storage into the necessary state for use with DAOS. Then dmg storage format formats persistent storage devices (specified in the server configuration file) on the storage nodes and writes necessary metadata before starting DAOS I/O processes that will operate across the fabric. To sum up, the typical workflow of a DAOS system deployment consists of the following steps: Configure and start the DAOS server . Provision Hardware on all the storage nodes via the dmg utility. Format the DAOS system Set up and start the agent on the client nodes Validate that the DAOS system is operational Note that starting the DAOS server instances can be performed automatically on boot if start-up scripts are registered with systemd. The following subsections will cover each step in more detail. Before getting started, please make sure to review and complete the pre-flight checklist below. Preflight Checklist \u00b6 This section covers the preliminary setup required on the compute and storage nodes before deploying DAOS. Time Synchronization \u00b6 The DAOS transaction model relies on timestamps and requires time to be synchronized across all the storage and client nodes. This can be done using NTP or any other equivalent protocol. Runtime Directory Setup \u00b6 DAOS uses a series of Unix Domain Sockets to communicate between its various components. On modern Linux systems, Unix Domain Sockets are typically stored under /run or /var/run (usually a symlink to /run) and are a mounted tmpfs file system. There are several methods for ensuring the necessary directories are setup. A sign that this step may have been missed is when starting daos_server or daos_agent, you may see the message: $ mkdir /var/run/daos_server: permission denied Unable to create socket directory: /var/run/daos_server Non-default Directory \u00b6 By default, daos_server and daos_agent will use the directories /var/run/daos_server and /var/run/daos_agent respectively. To change the default location that daos_server uses for its runtime directory, either uncomment and set the socket_dir configuration value in install/etc/daos_server.yml, or pass the location to daos_server on the command line using the -d flag. For the daos_agent, an alternate location can be passed on the command line using the --runtime_dir flag. Default Directory (non-persistent) \u00b6 Files and directories created in /run and /var/run only survive until the next reboot. However, if reboots are infrequent, an easy solution while still utilizing the default locations is to create the required directories manually. To do this execute the following commands. daos_server: $ mkdir /var/run/daos_server $ chmod 0755 /var/run/daos_server $ chown user:user /var/run/daos_server (where user is the user you will run daos_server as) daos_agent: $ mkdir /var/run/daos_agent $ chmod 0755 /var/run/daos_agent $ chown user:user /var/run/daos_agent (where user is the user you will run daos_agent as) Default Directory (persistent) \u00b6 If the server hosting daos_server or daos_agent will be rebooted often, systemd provides a persistent mechanism for creating the required directories called tmpfiles.d. This mechanism will be required every time the system is provisioned and requires a reboot to take effect. To tell systemd to create the necessary directories for DAOS: Copy the file utils/systemd/daosfiles.conf to /etc/tmpfiles.d\\ cp utils/systemd/daosfiles.conf /etc/tmpfiles.d Modify the copied file to change the user and group fields (currently daos) to the user daos will be run as Reboot the system, and the directories will be created automatically on all subsequent reboots. Elevated Privileges \u00b6 DAOS employs a privileged helper binary ( daos_admin ) to perform tasks that require elevated privileges on behalf of daos_server . Privileged Helper Configuration \u00b6 When DAOS is installed from RPM, the daos_admin helper is automatically installed to the correct location with the correct permissions. The RPM creates a \"daos_admins\" system group and configures permissions such that daos_admin may only be invoked from daos_server . For non-RPM installations, there are two supported scenarios: daos_server is run as root, which means that daos_admin is also invoked as root, and therefore no additional setup is necessary daos_server is run as a non-root user, which means that daos_admin must be manually installed and configured The steps to enable the second scenario are as follows (steps are assumed to be running out of a DAOS source tree which may be on a NFS share): $ chmod -x $SL_PREFIX/bin/daos_admin # prevent this copy from being executed $ sudo cp $SL_PREFIX/bin/daos_admin /usr/bin/daos_admin $ sudo chmod 4755 /usr/bin/daos_admin # make this copy setuid root $ sudo mkdir -p /usr/share/daos/control # create symlinks to SPDK scripts $ sudo ln -sf $SL_PREFIX/share/daos/control/setup_spdk.sh \\ /usr/share/daos/control $ sudo mkdir -p /usr/share/spdk/scripts $ sudo ln -sf $SL_PREFIX/share/spdk/scripts/setup.sh \\ /usr/share/spdk/scripts $ sudo ln -sf $SL_PREFIX/share/spdk/scripts/common.sh \\ /usr/share/spdk/scripts $ sudo ln -s $SL_PREFIX/include \\ /usr/share/spdk/include NOTES: * The RPM installation is preferred for production scenarios. Manual installation is most appropriate for development and predeployment proof-of-concept scenarios. DAOS Server Setup \u00b6 First of all, the DAOS server should be started to allow remote administration command to be executed via the dmg tool. This section describes the minimal DAOS server configuration and how to start it on all the storage nodes. Server Configuration File \u00b6 The daos_server configuration file is parsed when starting the daos_server process. The configuration file location can be specified on the command line ( daos_server -h for usage) or default location ( /etc/daos/daos_server.yml ). Parameter descriptions are specified in daos_server.yml and example configuration files in the examples directory. Any option supplied to daos_server as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed configuration values are written to a temporary file for reference, and the location will be written to the log. Configuration File Options \u00b6 The example configuration file lists the default empty configuration, listing all the options (living documentation of the config file). Live examples are available at https://github.com/daos-stack/daos/tree/master/utils/config/examples The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_server command line. Otherwise, /etc/daos_server.conf is used. Refer to the example configuration file ( daos_server.yml ) for latest information and examples. At this point of the process, the servers: and provider: section of the yaml file can be left blank and will be populated in the subsequent sections. Certificate Generation And Configuration \u00b6 The DAOS security framework relies on certificates to authenticate components and administrators in addition to encrypting DAOS control plane communications. A set of certificates for a given DAOS system may be generated by running the gen_certificates.sh script provided with the DAOS DAOS software if there is not an existing TLS certificate infrastructure. When DAOS is installed from RPMs, this script is provided in the base daos RPM, and may be invoked in the directory to which the certificates will be written. As part of the generation process, a new local Certificate Authority is created to handle certificate signing, and three role certificates are created: # /usr/lib64/daos/certgen/gen_certificates.sh Generating Private CA Root Certificate Private CA Root Certificate created in ./daosCA ... Generating Server Certificate Required Server Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/server.key ./daosCA/certs/server.crt ... Generating Agent Certificate Required Agent Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/agent.key ./daosCA/certs/agent.crt ... Generating Admin Certificate Required Admin Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/admin.key ./daosCA/certs/admin.crt The files generated under ./daosCA should be protected from unauthorized access and preserved for future use. The generated keys and certificates must then be securely distributed to all nodes participating in the DAOS system (servers, clients, and admin nodes). Permissions for these files should be set to prevent unauthorized access to the keys and certificates. After the certificates have been securely distributed, the DAOS configuration files must be updated in order to enable authentication and secure communications. These examples assume that the configuration files have been installed under /etc/daos : # /etc/daos/daos_server.yml (servers) transport_config: # Location where daos_server will look for Client certificates client_cert_dir: /etc/daos/clients # Custom CA Root certificate for generated certs ca_cert: /etc/daos/daosCA.crt # Server certificate for use in TLS handshakes cert: /etc/daos/server.crt # Key portion of Server Certificate key: /etc/daos/server.key # /etc/daos/daos_agent.yml (clients) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/daosCA.crt # Agent certificate for use in TLS handshakes cert: /etc/daos/agent.crt # Key portion of Agent Certificate key: /etc/daos/agent.key # /etc/daos/daos.yml (dmg/admin) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/daosCA.crt # Admin certificate for use in TLS handshakes cert: /etc/daos/admin.crt # Key portion of Admin Certificate key: /etc/daos/admin.key Server Startup \u00b6 One instance of the daos_server process is to be started per storage node. The server can be started either individually (e.g. independently on each storage node via systemd) or collectively (e.g. pdsh, mpirun or as a Kubernetes Pod). Parallel Launcher \u00b6 Practically any parallel launcher can be used to start the DAOS server collectively on a set of storage nodes. pdsh, clush and orterun are most commonly used. $ clush -w <server_list> -o \"-t -t\" daos_server start -o <config_file>` will launch daos_server on the specified hosts connecting to the port parameter value specified in the server config file. If the number of storage node exceed the default fanout value, then \"-f\" followed by the number of storage nodes should be used. Similarly, pdsh can be used: $ pdsh -w <server_list> daos_server start -o <config_file>` As for orterun, the list of storage nodes can be specified on the command line via the -H option. To start the DAOS server, run: $ orterun --map-by node --mca btl tcp,self --mca oob tcp -np <num_servers> -H <server_list> --enable-recovery daos_server start -o <config_file> The --enable-recovery is required for fault tolerance to guarantee that the fault of one server does not cause the others to be stopped. The --allow-run-as-root option can be added to the command line to allow the daos_server to run with root privileges on each storage nodes (for example when needing to perform privileged tasks relating to storage format). See the orterun(1) man page for additional options. Systemd Integration \u00b6 DAOS Server can be started as a systemd service. The DAOS Server unit file is installed in the correct location when installing from RPMs. If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system. Once the file is copied modify the ExecStart line to point to your in tree daos_server binary. ExecStart=/usr/bin/daos_server start Once the service file is installed you can start daos_server with the following commands: $ systemctl enable daos_server $ systemctl start daos_server To check the component status use: $ systemctl status daos_server If DAOS Server failed to start, check the logs with: $ journalctl --unit daos_server Kubernetes Pod \u00b6 DAOS service integration with Kubernetes is planned and will be supported in a future DAOS version. Hardware Provisioning \u00b6 Once the DAOS server started, the storage and network can be configured on the storage nodes via the dmg utility. SCM Preparation \u00b6 This section addresses how to verify that Optane DC Persistent Memory Module (DCPMM) is correctly installed on the storage nodes, and how to configure it in interleaved mode to be used by DAOS in AppDirect mode. Instructions for other types of SCM may be covered in the future. Provisioning the SCM occurs by configuring DCPM modules in AppDirect memory regions (interleaved mode) in groups of modules local to a specific socket (NUMA), and resultant nvdimm namespaces are defined by a device identifier (e.g., /dev/pmem0). DCPM preparation is required once per DAOS installation and requires the DAOS Control Servers to be running as root. This step requires a reboot to enable DCPM resource allocation changes to be read by BIOS. DCPM preparation can be performed from the management tool dmg storage prepare --scm-only or using the Control Server directly sudo daos_server storage prepare --scm-only . The first time the command is run, the SCM AppDirect regions will be created as resource allocations on any available DCPM modules (one region per NUMA node/socket). The regions are activated after BIOS reads the new resource allocations, and after initial completion the command prints a message to ask for a reboot (the command will not initiate reboot itself). After running the command a reboot will be required, then the Control Servers will then need to be started again and the command run for a second time to expose the namespace device to be used by DAOS. Example usage: dmg -l wolf-[118-121,130-133] -i storage prepare --scm-only after running, the user should be prompted for a reboot. clush -w wolf-[118-121,130-133] reboot clush -w wolf-[118-121,130-133] daos_server start -o utils/config/examples/daos_server_sockets.yml dmg -l wolf-[118-121,130-133] -i storage prepare --scm-only after running, /dev/pmemX devices should be available on each of the hosts. 'sudo daos_server storage prepare --scm-only' should be run for a second time after system reboot to create the pmem kernel devices (/dev/pmemX namespaces created on the new SCM regions). On the second run, one namespace per region is created, and each namespace may take up to a few minutes to create. Details of the pmem devices will be displayed in JSON format on command completion. Example output from the initial call (with the SCM modules set to default MemoryMode): Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. ensure namespaces are unmounted and SCM is otherwise unused. A reboot is required to process new memory allocation goals. Example output from the subsequent call (SCM modules configured to AppDirect mode, and host rebooted): Memory allocation goals for SCM will be changed and namespaces modified. This will be a destructive operation. Ensure namespaces are unmounted and the SCM is otherwise unused. creating SCM namespace, may take a few minutes... creating SCM namespace, may take a few minutes... Persistent memory kernel devices: [{UUID:5d2f2517-9217-4d7d-9c32-70731c9ac11e Blockdev:pmem1 Dev:namespace1.0 NumaNode:1} {UUID:2bfe6c40-f79a-4b8e-bddf-ba81d4427b9b Blockdev:pmem0 Dev:namespace0.0 NumaNode:0}] Upon successful creation of the pmem devices, DCPMM is properly configured and one can move on to the next step. If required, the pmem devices can be destroyed via the --reset option: sudo daos_server [<app_opts>] storage prepare [--scm-only|-s] --reset [<cmd_opts>] All namespaces are disabled and destroyed. The SCM regions are removed by resetting modules into \"MemoryMode\" through resource allocations. Note that undefined behavior may result if the namespaces/pmem kernel devices are mounted before running reset (as per the printed warning). A subsequent reboot is required for BIOS to read the new resource allocations. Example output when resetting the SCM modules: Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. ensure namespaces are unmounted and SCM is otherwise unused. removing SCM namespace, may take a few minutes... removing SCM namespace, may take a few minutes... resetting SCM memory allocations A reboot is required to process new memory allocation goals. Storage Detection & Selection \u00b6 While the DAOS server auto-detects all the usable storage, the administrator will still be provided with the ability through the configuration file (see next section) to whitelist or blacklist the storage devices to be (or not) used. This section covers how to manually detect the storage devices potentially usable by DAOS to populate the configuration file when the administrator wants to have finer control over the storage selection. dmg storage scan can be run to query remote running daos_server processes over the management network. sudo daos_server storage scan can be used to query daos_server directly (scans locally-attached SSDs and Intel Persistent Memory Modules usable by DAOS). [daos@wolf-72 daos_m]$ dmg -l wolf-7[1-2] -i storage scan --verbose wolf-[71-72]:10001: connected ------------ wolf-[71-72] ------------ SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 2.90TB pmem1 1 2.90TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 1 1.56TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB The NVMe PCI field above is what should be used in the server configuration file to identified NVMe SSDs. Devices with the same NUMA node/socket should be used in the same per-server section of the server configuration file for best performance. Note that other storage query commands are also available, dmg storage --help for listings. The next step consists of adjusting in the server configuration the storage devices that should be used by DAOS. The servers section of the yaml is a list specifying details for each DAOS I/O instance to be started on the host (currently a maximum of 2 per host is imposed). Devices with the same NUMA rating/node/socket should be colocated on a single DAOS I/O instance where possible. more details bdev_list should be populated with NVMe PCI addresses scm_list should be populated with DCPM interleaved set namespaces (e.g. /dev/pmem1 ) DAOS Control Servers will need to be restarted on all hosts after updates to the server configuration file. Pick one host in the system and set access_points to list of that host's hostname or IP address (don't need to specify port). This will be the host which bootstraps the DAOS management service (MS). To illustrate, assume a cluster with homogenous hardware configurations that returns the following from scan for each host: [daos@wolf-72 daos_m]$ dmg -l wolf-7[1-2] -i storage scan --verbose wolf-7[1-2]:10001: connected ------- wolf-7[1-2] ------- SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 2.90TB pmem1 1 2.90TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 0 750.00GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 0 1.56TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB In this situation, the configuration file servers section could be populated as follows: <snip> port: 10001 access_points: [\"wolf-71\"] # <----- updated <snip> servers: - targets: 8 # count of storage targets per each server first_core: 0 # offset of the first core for service xstreams nr_xs_helpers: 2 # count of offload/helper xstreams per target fabric_iface: eth0 # map to OFI_INTERFACE=eth0 fabric_iface_port: 31416 # map to OFI_PORT=31416 log_mask: ERR # map to D_LOG_MASK=ERR log_file: /tmp/server.log # map to D_LOG_FILE=/tmp/server.log env_vars: # influence DAOS IO Server behaviour by setting env variables - DAOS_MD_CAP=1024 - CRT_CTX_SHARE_ADDR=0 - CRT_TIMEOUT=30 - FI_SOCKETS_MAX_CONN_RETRY=1 - FI_SOCKETS_CONN_TIMEOUT=2000 scm_mount: /mnt/daos # map to -s /mnt/daos scm_class: dcpm scm_list: [/dev/pmem0] # <----- updated bdev_class: nvme bdev_list: [\"0000:87:00.0\", \"0000:81:00.0\"] # <----- updated - targets: 8 # count of storage targets per each server first_core: 0 # offset of the first core for service xstreams nr_xs_helpers: 2 # count of offload/helper xstreams per target fabric_iface: eth0 # map to OFI_INTERFACE=eth0 fabric_iface_port: 31416 # map to OFI_PORT=31416 log_mask: ERR # map to D_LOG_MASK=ERR log_file: /tmp/server.log # map to D_LOG_FILE=/tmp/server.log env_vars: # influence DAOS IO Server behaviour by setting env variables - DAOS_MD_CAP=1024 - CRT_CTX_SHARE_ADDR=0 - CRT_TIMEOUT=30 - FI_SOCKETS_MAX_CONN_RETRY=1 - FI_SOCKETS_CONN_TIMEOUT=2000 scm_mount: /mnt/daos # map to -s /mnt/daos scm_class: dcpm scm_list: [/dev/pmem1] # <----- updated bdev_class: nvme bdev_list: [\"0000:da:00.0\"] # <----- updated <end> Network Interface Detection and Selection \u00b6 To display the fabric interface, OFI provider and NUMA node combinations detected on the DAOS server, use the following command: $ daos_server network scan --all fabric_iface: ib0 provider: ofi+psm2 pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+psm2 pinned_numa_node: 1 fabric_iface: ib0 provider: ofi+verbs;ofi_rxm pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+verbs;ofi_rxm pinned_numa_node: 1 fabric_iface: ib0 provider: ofi+verbs pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+verbs pinned_numa_node: 1 fabric_iface: ib0 provider: ofi+sockets pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+sockets pinned_numa_node: 1 fabric_iface: eth0 provider: ofi+sockets pinned_numa_node: 0 fabric_iface: lo provider: ofi+sockets pinned_numa_node: 0 The network scan leverages data from libfabric. Results are ordered from highest performance at the top to lowest performance at the bottom of the list. Once the fabric_iface and provider pair has been chosen, those items and the pinned_numa_node may be inserted directly into the corresponding sections within daos_server.yml. Note that the provider is currently the same for all DAOS IO server instances and is configured once in the server configuration. The fabric_iface and pinned_numa_node are configured for each IO server instance. A list of providers that may be querried is found with the command: $ daos_server network list Supported providers: ofi+gni, ofi+psm2, ofi+tcp, ofi+sockets, ofi+verbs, ofi_rxm Performing a network scan that filters on a specific provider is accomplished by issuing the following command: $ daos_server network scan --provider 'ofi+verbs;ofi_rxm' Scanning fabric for cmdline specified provider: ofi+verbs;ofi_rxm Fabric scan found 2 devices matching the provider spec: ofi+verbs;ofi_rxm fabric_iface: ib0 provider: ofi+verbs;ofi_rxm pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+verbs;ofi_rxm pinned_numa_node: 1 To aid in provider configuration and debug, it may be helpful to run the fi_pingpong test (delivered as part of OFI/libfabric). To run that test, determine the name of the provider to test usually by removing the \"ofi+\" prefix from the network scan provider data. Do use the \"ofi+\" prefix in the daos_server.yml. Do not use the \"ofi+\" prefix with fi_pingpong. Then, the fi_pingpong test can be used to verify that the targeted OFI provider works fine: node1$ fi_pingpong -p psm2 node2$ fi_pingpong -p psm2 ${IP_ADDRESS_NODE1} bytes #sent #ack total time MB/sec usec/xfer Mxfers/sec 64 10 =10 1.2k 0.00s 21.69 2.95 0.34 256 10 =10 5k 0.00s 116.36 2.20 0.45 1k 10 =10 20k 0.00s 379.26 2.70 0.37 4k 10 =10 80k 0.00s 1077.89 3.80 0.26 64k 10 =10 1.2m 0.00s 2145.20 30.55 0.03 1m 10 =10 20m 0.00s 8867.45 118.25 0.01 Storage Formatting \u00b6 Once the daos_server has been restarted with the correct storage devices and network interface to use, one can move to the format phase. When daos_server is started for the first time, it enters \"maintenance mode\" and waits for a dmg storage format call to be issued from the management tool. This remote call will trigger the formatting of the locally attached storage on the host for use with DAOS using the parameters defined in the server config file. dmg -i -l <host:port>[,...] storage format will normally be run on a login node specifying a hostlist ( -l <host:port>[,...] ) of storage nodes with SCM/DCPM modules and NVMe SSDs installed and prepared. Upon successful format, DAOS Control Servers will start DAOS IO instances that have been specified in the server config file. Successful start-up is indicated by the following on stdout: DAOS I/O server (v0.8.0) process 433456 started on rank 1 with 8 target, 2 helper XS per target, firstcore 0, host wolf-72.wolf.hpdd.intel.com. SCM Format \u00b6 When the command is run, the pmem kernel devices created on SCM/DCPM regions are formatted and mounted based on the parameters provided in the server config file. scm_mount specifies the location of the mountpoint to create. scm_class can be set to ram to use a tmpfs in the situation that no SCM/DCPM is available (scm_size dictates the size of tmpfs in GB), when set to dcpm the device specified under scm_list will be mounted at scm_mount path. NVMe Format \u00b6 When the command is run, NVMe SSDs are formatted and set up to be used by DAOS based on the parameters provided in the server config file. bdev_class can be set to nvme to use actual NVMe devices with SPDK for DAOS storage. Other bdev_class values can be used for emulation of NVMe storage as specified in the server config file. bdev_list identifies devices to use with a list of PCI addresses (this can be populated after viewing results from storage scan command). After the format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain a file named daos_nvme.conf . The file should describe the devices with PCI addresses as listed in the bdev_list parameter of the server config file. The presence and contents of the file indicate that the specified NVMe SSDs have been configured correctly for use with DAOS. The contents of the NVMe SSDs listed in the server configuration file bdev_list parameter will be reset on format. Server Format \u00b6 Before the format command is run, no DAOS metadata should exist under the path specified by scm_mount parameter in the server configuration file. After the storage format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain the necessary DAOS metadata indicating that the server has been formatted. When starting, daos_server will skip maintenance mode and attempt to start IO services if valid DAOS metadata is found in scm_mount . Stop and Start a Formatted System \u00b6 A DAOS system can be restarted after a controlled shutdown providing no configurations changes have been made after initial format. The DAOS Control Server instance acting as access point records DAOS I/O Server instances that join the system in a \"membership\". When up and running, the entire system (all I/O Server instances) can be shutdown with the command dmg -l <access_point_addr> system stop , after which DAOS Control Servers will continue to operate and listen on the management network. To start the system again (with no configuration changes) after a controlled shutdown, run the command dmg -l <access_point_addr> system start , DAOS I/O Servers managed by DAOS Control Servers will be started. To query the system membership, run the command dmg -l <access_point_addr> system query , this lists details (rank/uuid/control address/state) of DAOS I/O Servers in the system membership. Controlled Start/Stop Limitations (subject to change) \u00b6 \"start\" restarts all configured instances on all harnesses that can be located in the system membership, regardless of member state supplying list of ranks to \"start\" and \"stop\" is not yet supported Fresh Start \u00b6 To reset the DAOS metadata across all hosts the system must be reformatted. First ensure all daos_server processes on all hosts have been stopped, then for each SCM mount specified in the config file ( scm_mount in the servers section) umount and wipe FS signatures. Example illustration with two IO instances specified in the config file: clush -w wolf-[118-121,130-133] umount /mnt/daos1 clush -w wolf-[118-121,130-133] umount /mnt/daos0 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem1 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem0 Then restart DAOS Servers and format. Agent Configuration and Startup \u00b6 This section addresses how to configure the DAOS agents on the storage nodes before starting it. Agent Certificate Generation \u00b6 The DAOS security framework relies on certificates to authenticate administrators. The security infrastructure is currently under development and will be delivered in DAOS v1.0. Initial support for certificates has been added to DAOS and can be disabled either via the command line or in the DAOS Agent configuration file. Currently, the easiest way to disable certificate support is to pass the -i flag to daos_agent. Agent Configuration File \u00b6 The daos_agent configuration file is parsed when starting the daos_agent process. The configuration file location can be specified on the command line ( daos_agent -h for usage) or default location ( install/etc/daos_agent.yml ). If installed from rpms the default location is ( /etc/daos/daos_agent.yml ). Parameter descriptions are specified in daos_agent.yml . Any option supplied to daos_agent as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed config values are written to a temporary file for reference, and the location will be written to the log. The following section lists the format, options, defaults, and descriptions available in the configuration file. The example configuration file lists the default empty configuration listing all the options (living documentation of the config file). Live examples are available at https://github.com/daos-stack/daos/tree/master/utils/config The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_agent command line. Otherwise, /etc/daos_agent.conf is used. Refer to the example configuration file ( daos_server.yml ) for latest information and examples. Agent Startup \u00b6 DAOS Agent is a standalone application to be run on each compute node. It can be configured to use secure communications (default) or can be allowed to communicate with the control plane over unencrypted channels. The following example shows daos_agent being configured to operate in insecure mode due to incomplete integration of certificate support as of the 0.9 release and configured to use a non-default agent configuration file. To start the DAOS Agent from the command line using the default config file, /etc/daos/daos_agent.yaml, run: $ daos_agent & Or to specify a non-default configuration file, use the -o parameter $ daos_agent -o <'path to agent configuration file/daos_agent.yml'> & Alternatively, the DAOS Agent can be started as a systemd service. The DAOS Agent unit file is installed in the correct location when installing from RPMs. If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system. Once the file is copied modify the ExecStart line to point to your in tree daos_agent binary. ExecStart=/usr/bin/daos_agent Once the service file is installed, you can start daos_agent with the following commands: $ sudo systemctl enable daos_agent $ sudo systemctl start daos_agent To check the component status use: $ sudo systemctl status daos_agent If DAOS Agent failed to start check the logs with: $ sudo journalctl --unit daos_agent System Validation \u00b6 To validate that the DAOS system is properly installed, the daos_test suite can be executed. Ensure the DAOS Agent is configured and running before running daos_test and that the DAOS_SINGLETON_CLI and CRT_ATTACH_INFO_PATH environment variables are properly set as described here . orterun -np <num_clients> --hostfile <hostfile> ./daos_test daos_test requires at least 8GB of SCM (or DRAM with tmpfs) storage on each storage node. NVMe SSD Health Monitoring & Stats \u00b6 Useful admin dmg commands to query NVMe SSD health: Query NVMe SSD Health Stats: $dmg storage query nvme-health Queries raw SPDK NVMe device health statistics for all NVMe SSDs on all hosts in list. $dmg storage query nvme-health -l=boro-11:10001 boro-11:10001: connected boro-11:10001 NVMe controllers and namespaces detail with health statistics: PCI:0000:81:00.0 Model:INTEL SSDPEDKE020T7 FW:QDV10130 Socket:1 Capacity:1.95TB Health Stats: Temperature:288K(15C) Controller Busy Time:5h26m0s Power Cycles:4 Power On Duration:16488h0m0s Unsafe Shutdowns:2 Media Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK Query Per-Server Metadata (SMD): $dmg storage query smd Queries persistently stored device and pool metadata tables. The device table maps device UUID to attached VOS target IDs. The pool table maps VOS target IDs to attached SPDK blob IDs. $dmg storage query smd --devices --pools -l=boro-11:10001 boro-11:10001: connected SMD Device List: boro-11:10001: Device: UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 VOS Target IDs: 0 1 2 3 SMD Pool List: boro-11:10001: Pool: UUID: 01b41f76-a783-462f-bbd2-eb27c2f7e326 VOS Target IDs: 0 1 3 2 SPDK Blobs: 4294967404 4294967405 4294967407 4294967406 Query Blobstore Health Data: $dmg storage query blobstore-health Queries in-memory health data for the SPDK blobstore (ie NVMe SSD). This includes a subset of the SPDK device health stats, as well as I/O error and checksum counters. $dmg storage query blobstore-health --devuuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 -l=boro-11:10001 boro-11:10001: connected Blobstore Health Data: boro-11:10001: Device UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 Read errors: 0 Write errors: 0 Unmap errors: 0 Checksum errors: 0 Device Health: Error log entries: 0 Media errors: 0 Temperature: 289 Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK Query Persistent Device State: $dmg storage query device-state Queries the current persistently stored device state of the specified NVMe SSD (either NORMAL or FAULTY). $dmg storage query device-state --devuuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 -l=boro-11:10001 boro-11:10001: connected Device State Info: boro-11:10001: Device UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 State: NORMAL Manually Set Device State to FAULTY: $dmg storage set nvme-faulty Allows the admin to manually set the device state of the given device to FAULTY, which will trigger faulty device reaction (all targets on the SSD will be rebuilt and the SSD will remain in an OUT state until reintegration is supported). $dmg storage set nvme-faulty --devuuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 -l=boro-11:10001 boro-11:10001: connected Device State Info: boro-11:10001: Device UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 State: FAULTY https://github.com/intel/ipmctl \u21a9 https://github.com/daos-stack/daos/tree/master/utils/config \u21a9 https://www.open-mpi.org/faq/?category=running#mpirun-hostfile \u21a9 https://github.com/daos-stack/daos/tree/master/src/control/README.md \u21a9","title":"System Deployment"},{"location":"admin/deployment/#system-deployment","text":"The DAOS deployment workflow requires to start the DAOS server instances early on to enable administrators to perform remote operations in parallel across multiple storage nodes via the dmg management utility. Security is guaranteed via the use of certificates. The first type of commands run after installation include network and storage hardware provisioning and would typically be run from a login node. After daos_server instances have been started on each storage node for the first time, dmg storage prepare will set DCPM storage into the necessary state for use with DAOS. Then dmg storage format formats persistent storage devices (specified in the server configuration file) on the storage nodes and writes necessary metadata before starting DAOS I/O processes that will operate across the fabric. To sum up, the typical workflow of a DAOS system deployment consists of the following steps: Configure and start the DAOS server . Provision Hardware on all the storage nodes via the dmg utility. Format the DAOS system Set up and start the agent on the client nodes Validate that the DAOS system is operational Note that starting the DAOS server instances can be performed automatically on boot if start-up scripts are registered with systemd. The following subsections will cover each step in more detail. Before getting started, please make sure to review and complete the pre-flight checklist below.","title":"System Deployment"},{"location":"admin/deployment/#preflight-checklist","text":"This section covers the preliminary setup required on the compute and storage nodes before deploying DAOS.","title":"Preflight Checklist"},{"location":"admin/deployment/#time-synchronization","text":"The DAOS transaction model relies on timestamps and requires time to be synchronized across all the storage and client nodes. This can be done using NTP or any other equivalent protocol.","title":"Time Synchronization"},{"location":"admin/deployment/#runtime-directory-setup","text":"DAOS uses a series of Unix Domain Sockets to communicate between its various components. On modern Linux systems, Unix Domain Sockets are typically stored under /run or /var/run (usually a symlink to /run) and are a mounted tmpfs file system. There are several methods for ensuring the necessary directories are setup. A sign that this step may have been missed is when starting daos_server or daos_agent, you may see the message: $ mkdir /var/run/daos_server: permission denied Unable to create socket directory: /var/run/daos_server","title":"Runtime Directory Setup"},{"location":"admin/deployment/#non-default-directory","text":"By default, daos_server and daos_agent will use the directories /var/run/daos_server and /var/run/daos_agent respectively. To change the default location that daos_server uses for its runtime directory, either uncomment and set the socket_dir configuration value in install/etc/daos_server.yml, or pass the location to daos_server on the command line using the -d flag. For the daos_agent, an alternate location can be passed on the command line using the --runtime_dir flag.","title":"Non-default Directory"},{"location":"admin/deployment/#default-directory-non-persistent","text":"Files and directories created in /run and /var/run only survive until the next reboot. However, if reboots are infrequent, an easy solution while still utilizing the default locations is to create the required directories manually. To do this execute the following commands. daos_server: $ mkdir /var/run/daos_server $ chmod 0755 /var/run/daos_server $ chown user:user /var/run/daos_server (where user is the user you will run daos_server as) daos_agent: $ mkdir /var/run/daos_agent $ chmod 0755 /var/run/daos_agent $ chown user:user /var/run/daos_agent (where user is the user you will run daos_agent as)","title":"Default Directory (non-persistent)"},{"location":"admin/deployment/#default-directory-persistent","text":"If the server hosting daos_server or daos_agent will be rebooted often, systemd provides a persistent mechanism for creating the required directories called tmpfiles.d. This mechanism will be required every time the system is provisioned and requires a reboot to take effect. To tell systemd to create the necessary directories for DAOS: Copy the file utils/systemd/daosfiles.conf to /etc/tmpfiles.d\\ cp utils/systemd/daosfiles.conf /etc/tmpfiles.d Modify the copied file to change the user and group fields (currently daos) to the user daos will be run as Reboot the system, and the directories will be created automatically on all subsequent reboots.","title":"Default Directory (persistent)"},{"location":"admin/deployment/#elevated-privileges","text":"DAOS employs a privileged helper binary ( daos_admin ) to perform tasks that require elevated privileges on behalf of daos_server .","title":"Elevated Privileges"},{"location":"admin/deployment/#privileged-helper-configuration","text":"When DAOS is installed from RPM, the daos_admin helper is automatically installed to the correct location with the correct permissions. The RPM creates a \"daos_admins\" system group and configures permissions such that daos_admin may only be invoked from daos_server . For non-RPM installations, there are two supported scenarios: daos_server is run as root, which means that daos_admin is also invoked as root, and therefore no additional setup is necessary daos_server is run as a non-root user, which means that daos_admin must be manually installed and configured The steps to enable the second scenario are as follows (steps are assumed to be running out of a DAOS source tree which may be on a NFS share): $ chmod -x $SL_PREFIX/bin/daos_admin # prevent this copy from being executed $ sudo cp $SL_PREFIX/bin/daos_admin /usr/bin/daos_admin $ sudo chmod 4755 /usr/bin/daos_admin # make this copy setuid root $ sudo mkdir -p /usr/share/daos/control # create symlinks to SPDK scripts $ sudo ln -sf $SL_PREFIX/share/daos/control/setup_spdk.sh \\ /usr/share/daos/control $ sudo mkdir -p /usr/share/spdk/scripts $ sudo ln -sf $SL_PREFIX/share/spdk/scripts/setup.sh \\ /usr/share/spdk/scripts $ sudo ln -sf $SL_PREFIX/share/spdk/scripts/common.sh \\ /usr/share/spdk/scripts $ sudo ln -s $SL_PREFIX/include \\ /usr/share/spdk/include NOTES: * The RPM installation is preferred for production scenarios. Manual installation is most appropriate for development and predeployment proof-of-concept scenarios.","title":"Privileged Helper Configuration"},{"location":"admin/deployment/#daos-server-setup","text":"First of all, the DAOS server should be started to allow remote administration command to be executed via the dmg tool. This section describes the minimal DAOS server configuration and how to start it on all the storage nodes.","title":"DAOS Server Setup"},{"location":"admin/deployment/#server-configuration-file","text":"The daos_server configuration file is parsed when starting the daos_server process. The configuration file location can be specified on the command line ( daos_server -h for usage) or default location ( /etc/daos/daos_server.yml ). Parameter descriptions are specified in daos_server.yml and example configuration files in the examples directory. Any option supplied to daos_server as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed configuration values are written to a temporary file for reference, and the location will be written to the log.","title":"Server Configuration File"},{"location":"admin/deployment/#configuration-file-options","text":"The example configuration file lists the default empty configuration, listing all the options (living documentation of the config file). Live examples are available at https://github.com/daos-stack/daos/tree/master/utils/config/examples The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_server command line. Otherwise, /etc/daos_server.conf is used. Refer to the example configuration file ( daos_server.yml ) for latest information and examples. At this point of the process, the servers: and provider: section of the yaml file can be left blank and will be populated in the subsequent sections.","title":"Configuration File Options"},{"location":"admin/deployment/#certificate-generation-and-configuration","text":"The DAOS security framework relies on certificates to authenticate components and administrators in addition to encrypting DAOS control plane communications. A set of certificates for a given DAOS system may be generated by running the gen_certificates.sh script provided with the DAOS DAOS software if there is not an existing TLS certificate infrastructure. When DAOS is installed from RPMs, this script is provided in the base daos RPM, and may be invoked in the directory to which the certificates will be written. As part of the generation process, a new local Certificate Authority is created to handle certificate signing, and three role certificates are created: # /usr/lib64/daos/certgen/gen_certificates.sh Generating Private CA Root Certificate Private CA Root Certificate created in ./daosCA ... Generating Server Certificate Required Server Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/server.key ./daosCA/certs/server.crt ... Generating Agent Certificate Required Agent Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/agent.key ./daosCA/certs/agent.crt ... Generating Admin Certificate Required Admin Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/admin.key ./daosCA/certs/admin.crt The files generated under ./daosCA should be protected from unauthorized access and preserved for future use. The generated keys and certificates must then be securely distributed to all nodes participating in the DAOS system (servers, clients, and admin nodes). Permissions for these files should be set to prevent unauthorized access to the keys and certificates. After the certificates have been securely distributed, the DAOS configuration files must be updated in order to enable authentication and secure communications. These examples assume that the configuration files have been installed under /etc/daos : # /etc/daos/daos_server.yml (servers) transport_config: # Location where daos_server will look for Client certificates client_cert_dir: /etc/daos/clients # Custom CA Root certificate for generated certs ca_cert: /etc/daos/daosCA.crt # Server certificate for use in TLS handshakes cert: /etc/daos/server.crt # Key portion of Server Certificate key: /etc/daos/server.key # /etc/daos/daos_agent.yml (clients) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/daosCA.crt # Agent certificate for use in TLS handshakes cert: /etc/daos/agent.crt # Key portion of Agent Certificate key: /etc/daos/agent.key # /etc/daos/daos.yml (dmg/admin) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/daosCA.crt # Admin certificate for use in TLS handshakes cert: /etc/daos/admin.crt # Key portion of Admin Certificate key: /etc/daos/admin.key","title":"Certificate Generation And Configuration"},{"location":"admin/deployment/#server-startup","text":"One instance of the daos_server process is to be started per storage node. The server can be started either individually (e.g. independently on each storage node via systemd) or collectively (e.g. pdsh, mpirun or as a Kubernetes Pod).","title":"Server Startup"},{"location":"admin/deployment/#parallel-launcher","text":"Practically any parallel launcher can be used to start the DAOS server collectively on a set of storage nodes. pdsh, clush and orterun are most commonly used. $ clush -w <server_list> -o \"-t -t\" daos_server start -o <config_file>` will launch daos_server on the specified hosts connecting to the port parameter value specified in the server config file. If the number of storage node exceed the default fanout value, then \"-f\" followed by the number of storage nodes should be used. Similarly, pdsh can be used: $ pdsh -w <server_list> daos_server start -o <config_file>` As for orterun, the list of storage nodes can be specified on the command line via the -H option. To start the DAOS server, run: $ orterun --map-by node --mca btl tcp,self --mca oob tcp -np <num_servers> -H <server_list> --enable-recovery daos_server start -o <config_file> The --enable-recovery is required for fault tolerance to guarantee that the fault of one server does not cause the others to be stopped. The --allow-run-as-root option can be added to the command line to allow the daos_server to run with root privileges on each storage nodes (for example when needing to perform privileged tasks relating to storage format). See the orterun(1) man page for additional options.","title":"Parallel Launcher"},{"location":"admin/deployment/#systemd-integration","text":"DAOS Server can be started as a systemd service. The DAOS Server unit file is installed in the correct location when installing from RPMs. If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system. Once the file is copied modify the ExecStart line to point to your in tree daos_server binary. ExecStart=/usr/bin/daos_server start Once the service file is installed you can start daos_server with the following commands: $ systemctl enable daos_server $ systemctl start daos_server To check the component status use: $ systemctl status daos_server If DAOS Server failed to start, check the logs with: $ journalctl --unit daos_server","title":"Systemd Integration"},{"location":"admin/deployment/#kubernetes-pod","text":"DAOS service integration with Kubernetes is planned and will be supported in a future DAOS version.","title":"Kubernetes Pod"},{"location":"admin/deployment/#hardware-provisioning","text":"Once the DAOS server started, the storage and network can be configured on the storage nodes via the dmg utility.","title":"Hardware Provisioning"},{"location":"admin/deployment/#scm-preparation","text":"This section addresses how to verify that Optane DC Persistent Memory Module (DCPMM) is correctly installed on the storage nodes, and how to configure it in interleaved mode to be used by DAOS in AppDirect mode. Instructions for other types of SCM may be covered in the future. Provisioning the SCM occurs by configuring DCPM modules in AppDirect memory regions (interleaved mode) in groups of modules local to a specific socket (NUMA), and resultant nvdimm namespaces are defined by a device identifier (e.g., /dev/pmem0). DCPM preparation is required once per DAOS installation and requires the DAOS Control Servers to be running as root. This step requires a reboot to enable DCPM resource allocation changes to be read by BIOS. DCPM preparation can be performed from the management tool dmg storage prepare --scm-only or using the Control Server directly sudo daos_server storage prepare --scm-only . The first time the command is run, the SCM AppDirect regions will be created as resource allocations on any available DCPM modules (one region per NUMA node/socket). The regions are activated after BIOS reads the new resource allocations, and after initial completion the command prints a message to ask for a reboot (the command will not initiate reboot itself). After running the command a reboot will be required, then the Control Servers will then need to be started again and the command run for a second time to expose the namespace device to be used by DAOS. Example usage: dmg -l wolf-[118-121,130-133] -i storage prepare --scm-only after running, the user should be prompted for a reboot. clush -w wolf-[118-121,130-133] reboot clush -w wolf-[118-121,130-133] daos_server start -o utils/config/examples/daos_server_sockets.yml dmg -l wolf-[118-121,130-133] -i storage prepare --scm-only after running, /dev/pmemX devices should be available on each of the hosts. 'sudo daos_server storage prepare --scm-only' should be run for a second time after system reboot to create the pmem kernel devices (/dev/pmemX namespaces created on the new SCM regions). On the second run, one namespace per region is created, and each namespace may take up to a few minutes to create. Details of the pmem devices will be displayed in JSON format on command completion. Example output from the initial call (with the SCM modules set to default MemoryMode): Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. ensure namespaces are unmounted and SCM is otherwise unused. A reboot is required to process new memory allocation goals. Example output from the subsequent call (SCM modules configured to AppDirect mode, and host rebooted): Memory allocation goals for SCM will be changed and namespaces modified. This will be a destructive operation. Ensure namespaces are unmounted and the SCM is otherwise unused. creating SCM namespace, may take a few minutes... creating SCM namespace, may take a few minutes... Persistent memory kernel devices: [{UUID:5d2f2517-9217-4d7d-9c32-70731c9ac11e Blockdev:pmem1 Dev:namespace1.0 NumaNode:1} {UUID:2bfe6c40-f79a-4b8e-bddf-ba81d4427b9b Blockdev:pmem0 Dev:namespace0.0 NumaNode:0}] Upon successful creation of the pmem devices, DCPMM is properly configured and one can move on to the next step. If required, the pmem devices can be destroyed via the --reset option: sudo daos_server [<app_opts>] storage prepare [--scm-only|-s] --reset [<cmd_opts>] All namespaces are disabled and destroyed. The SCM regions are removed by resetting modules into \"MemoryMode\" through resource allocations. Note that undefined behavior may result if the namespaces/pmem kernel devices are mounted before running reset (as per the printed warning). A subsequent reboot is required for BIOS to read the new resource allocations. Example output when resetting the SCM modules: Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. ensure namespaces are unmounted and SCM is otherwise unused. removing SCM namespace, may take a few minutes... removing SCM namespace, may take a few minutes... resetting SCM memory allocations A reboot is required to process new memory allocation goals.","title":"SCM Preparation"},{"location":"admin/deployment/#storage-detection-selection","text":"While the DAOS server auto-detects all the usable storage, the administrator will still be provided with the ability through the configuration file (see next section) to whitelist or blacklist the storage devices to be (or not) used. This section covers how to manually detect the storage devices potentially usable by DAOS to populate the configuration file when the administrator wants to have finer control over the storage selection. dmg storage scan can be run to query remote running daos_server processes over the management network. sudo daos_server storage scan can be used to query daos_server directly (scans locally-attached SSDs and Intel Persistent Memory Modules usable by DAOS). [daos@wolf-72 daos_m]$ dmg -l wolf-7[1-2] -i storage scan --verbose wolf-[71-72]:10001: connected ------------ wolf-[71-72] ------------ SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 2.90TB pmem1 1 2.90TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 1 1.56TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB The NVMe PCI field above is what should be used in the server configuration file to identified NVMe SSDs. Devices with the same NUMA node/socket should be used in the same per-server section of the server configuration file for best performance. Note that other storage query commands are also available, dmg storage --help for listings. The next step consists of adjusting in the server configuration the storage devices that should be used by DAOS. The servers section of the yaml is a list specifying details for each DAOS I/O instance to be started on the host (currently a maximum of 2 per host is imposed). Devices with the same NUMA rating/node/socket should be colocated on a single DAOS I/O instance where possible. more details bdev_list should be populated with NVMe PCI addresses scm_list should be populated with DCPM interleaved set namespaces (e.g. /dev/pmem1 ) DAOS Control Servers will need to be restarted on all hosts after updates to the server configuration file. Pick one host in the system and set access_points to list of that host's hostname or IP address (don't need to specify port). This will be the host which bootstraps the DAOS management service (MS). To illustrate, assume a cluster with homogenous hardware configurations that returns the following from scan for each host: [daos@wolf-72 daos_m]$ dmg -l wolf-7[1-2] -i storage scan --verbose wolf-7[1-2]:10001: connected ------- wolf-7[1-2] ------- SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 2.90TB pmem1 1 2.90TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 0 750.00GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 0 1.56TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB In this situation, the configuration file servers section could be populated as follows: <snip> port: 10001 access_points: [\"wolf-71\"] # <----- updated <snip> servers: - targets: 8 # count of storage targets per each server first_core: 0 # offset of the first core for service xstreams nr_xs_helpers: 2 # count of offload/helper xstreams per target fabric_iface: eth0 # map to OFI_INTERFACE=eth0 fabric_iface_port: 31416 # map to OFI_PORT=31416 log_mask: ERR # map to D_LOG_MASK=ERR log_file: /tmp/server.log # map to D_LOG_FILE=/tmp/server.log env_vars: # influence DAOS IO Server behaviour by setting env variables - DAOS_MD_CAP=1024 - CRT_CTX_SHARE_ADDR=0 - CRT_TIMEOUT=30 - FI_SOCKETS_MAX_CONN_RETRY=1 - FI_SOCKETS_CONN_TIMEOUT=2000 scm_mount: /mnt/daos # map to -s /mnt/daos scm_class: dcpm scm_list: [/dev/pmem0] # <----- updated bdev_class: nvme bdev_list: [\"0000:87:00.0\", \"0000:81:00.0\"] # <----- updated - targets: 8 # count of storage targets per each server first_core: 0 # offset of the first core for service xstreams nr_xs_helpers: 2 # count of offload/helper xstreams per target fabric_iface: eth0 # map to OFI_INTERFACE=eth0 fabric_iface_port: 31416 # map to OFI_PORT=31416 log_mask: ERR # map to D_LOG_MASK=ERR log_file: /tmp/server.log # map to D_LOG_FILE=/tmp/server.log env_vars: # influence DAOS IO Server behaviour by setting env variables - DAOS_MD_CAP=1024 - CRT_CTX_SHARE_ADDR=0 - CRT_TIMEOUT=30 - FI_SOCKETS_MAX_CONN_RETRY=1 - FI_SOCKETS_CONN_TIMEOUT=2000 scm_mount: /mnt/daos # map to -s /mnt/daos scm_class: dcpm scm_list: [/dev/pmem1] # <----- updated bdev_class: nvme bdev_list: [\"0000:da:00.0\"] # <----- updated <end>","title":"Storage Detection &amp; Selection"},{"location":"admin/deployment/#network-interface-detection-and-selection","text":"To display the fabric interface, OFI provider and NUMA node combinations detected on the DAOS server, use the following command: $ daos_server network scan --all fabric_iface: ib0 provider: ofi+psm2 pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+psm2 pinned_numa_node: 1 fabric_iface: ib0 provider: ofi+verbs;ofi_rxm pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+verbs;ofi_rxm pinned_numa_node: 1 fabric_iface: ib0 provider: ofi+verbs pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+verbs pinned_numa_node: 1 fabric_iface: ib0 provider: ofi+sockets pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+sockets pinned_numa_node: 1 fabric_iface: eth0 provider: ofi+sockets pinned_numa_node: 0 fabric_iface: lo provider: ofi+sockets pinned_numa_node: 0 The network scan leverages data from libfabric. Results are ordered from highest performance at the top to lowest performance at the bottom of the list. Once the fabric_iface and provider pair has been chosen, those items and the pinned_numa_node may be inserted directly into the corresponding sections within daos_server.yml. Note that the provider is currently the same for all DAOS IO server instances and is configured once in the server configuration. The fabric_iface and pinned_numa_node are configured for each IO server instance. A list of providers that may be querried is found with the command: $ daos_server network list Supported providers: ofi+gni, ofi+psm2, ofi+tcp, ofi+sockets, ofi+verbs, ofi_rxm Performing a network scan that filters on a specific provider is accomplished by issuing the following command: $ daos_server network scan --provider 'ofi+verbs;ofi_rxm' Scanning fabric for cmdline specified provider: ofi+verbs;ofi_rxm Fabric scan found 2 devices matching the provider spec: ofi+verbs;ofi_rxm fabric_iface: ib0 provider: ofi+verbs;ofi_rxm pinned_numa_node: 0 fabric_iface: ib1 provider: ofi+verbs;ofi_rxm pinned_numa_node: 1 To aid in provider configuration and debug, it may be helpful to run the fi_pingpong test (delivered as part of OFI/libfabric). To run that test, determine the name of the provider to test usually by removing the \"ofi+\" prefix from the network scan provider data. Do use the \"ofi+\" prefix in the daos_server.yml. Do not use the \"ofi+\" prefix with fi_pingpong. Then, the fi_pingpong test can be used to verify that the targeted OFI provider works fine: node1$ fi_pingpong -p psm2 node2$ fi_pingpong -p psm2 ${IP_ADDRESS_NODE1} bytes #sent #ack total time MB/sec usec/xfer Mxfers/sec 64 10 =10 1.2k 0.00s 21.69 2.95 0.34 256 10 =10 5k 0.00s 116.36 2.20 0.45 1k 10 =10 20k 0.00s 379.26 2.70 0.37 4k 10 =10 80k 0.00s 1077.89 3.80 0.26 64k 10 =10 1.2m 0.00s 2145.20 30.55 0.03 1m 10 =10 20m 0.00s 8867.45 118.25 0.01","title":"Network Interface Detection and Selection"},{"location":"admin/deployment/#storage-formatting","text":"Once the daos_server has been restarted with the correct storage devices and network interface to use, one can move to the format phase. When daos_server is started for the first time, it enters \"maintenance mode\" and waits for a dmg storage format call to be issued from the management tool. This remote call will trigger the formatting of the locally attached storage on the host for use with DAOS using the parameters defined in the server config file. dmg -i -l <host:port>[,...] storage format will normally be run on a login node specifying a hostlist ( -l <host:port>[,...] ) of storage nodes with SCM/DCPM modules and NVMe SSDs installed and prepared. Upon successful format, DAOS Control Servers will start DAOS IO instances that have been specified in the server config file. Successful start-up is indicated by the following on stdout: DAOS I/O server (v0.8.0) process 433456 started on rank 1 with 8 target, 2 helper XS per target, firstcore 0, host wolf-72.wolf.hpdd.intel.com.","title":"Storage Formatting"},{"location":"admin/deployment/#scm-format","text":"When the command is run, the pmem kernel devices created on SCM/DCPM regions are formatted and mounted based on the parameters provided in the server config file. scm_mount specifies the location of the mountpoint to create. scm_class can be set to ram to use a tmpfs in the situation that no SCM/DCPM is available (scm_size dictates the size of tmpfs in GB), when set to dcpm the device specified under scm_list will be mounted at scm_mount path.","title":"SCM Format"},{"location":"admin/deployment/#nvme-format","text":"When the command is run, NVMe SSDs are formatted and set up to be used by DAOS based on the parameters provided in the server config file. bdev_class can be set to nvme to use actual NVMe devices with SPDK for DAOS storage. Other bdev_class values can be used for emulation of NVMe storage as specified in the server config file. bdev_list identifies devices to use with a list of PCI addresses (this can be populated after viewing results from storage scan command). After the format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain a file named daos_nvme.conf . The file should describe the devices with PCI addresses as listed in the bdev_list parameter of the server config file. The presence and contents of the file indicate that the specified NVMe SSDs have been configured correctly for use with DAOS. The contents of the NVMe SSDs listed in the server configuration file bdev_list parameter will be reset on format.","title":"NVMe Format"},{"location":"admin/deployment/#server-format","text":"Before the format command is run, no DAOS metadata should exist under the path specified by scm_mount parameter in the server configuration file. After the storage format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain the necessary DAOS metadata indicating that the server has been formatted. When starting, daos_server will skip maintenance mode and attempt to start IO services if valid DAOS metadata is found in scm_mount .","title":"Server Format"},{"location":"admin/deployment/#stop-and-start-a-formatted-system","text":"A DAOS system can be restarted after a controlled shutdown providing no configurations changes have been made after initial format. The DAOS Control Server instance acting as access point records DAOS I/O Server instances that join the system in a \"membership\". When up and running, the entire system (all I/O Server instances) can be shutdown with the command dmg -l <access_point_addr> system stop , after which DAOS Control Servers will continue to operate and listen on the management network. To start the system again (with no configuration changes) after a controlled shutdown, run the command dmg -l <access_point_addr> system start , DAOS I/O Servers managed by DAOS Control Servers will be started. To query the system membership, run the command dmg -l <access_point_addr> system query , this lists details (rank/uuid/control address/state) of DAOS I/O Servers in the system membership.","title":"Stop and Start a Formatted System"},{"location":"admin/deployment/#controlled-startstop-limitations-subject-to-change","text":"\"start\" restarts all configured instances on all harnesses that can be located in the system membership, regardless of member state supplying list of ranks to \"start\" and \"stop\" is not yet supported","title":"Controlled Start/Stop Limitations (subject to change)"},{"location":"admin/deployment/#fresh-start","text":"To reset the DAOS metadata across all hosts the system must be reformatted. First ensure all daos_server processes on all hosts have been stopped, then for each SCM mount specified in the config file ( scm_mount in the servers section) umount and wipe FS signatures. Example illustration with two IO instances specified in the config file: clush -w wolf-[118-121,130-133] umount /mnt/daos1 clush -w wolf-[118-121,130-133] umount /mnt/daos0 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem1 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem0 Then restart DAOS Servers and format.","title":"Fresh Start"},{"location":"admin/deployment/#agent-configuration-and-startup","text":"This section addresses how to configure the DAOS agents on the storage nodes before starting it.","title":"Agent Configuration and Startup"},{"location":"admin/deployment/#agent-certificate-generation","text":"The DAOS security framework relies on certificates to authenticate administrators. The security infrastructure is currently under development and will be delivered in DAOS v1.0. Initial support for certificates has been added to DAOS and can be disabled either via the command line or in the DAOS Agent configuration file. Currently, the easiest way to disable certificate support is to pass the -i flag to daos_agent.","title":"Agent Certificate Generation"},{"location":"admin/deployment/#agent-configuration-file","text":"The daos_agent configuration file is parsed when starting the daos_agent process. The configuration file location can be specified on the command line ( daos_agent -h for usage) or default location ( install/etc/daos_agent.yml ). If installed from rpms the default location is ( /etc/daos/daos_agent.yml ). Parameter descriptions are specified in daos_agent.yml . Any option supplied to daos_agent as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed config values are written to a temporary file for reference, and the location will be written to the log. The following section lists the format, options, defaults, and descriptions available in the configuration file. The example configuration file lists the default empty configuration listing all the options (living documentation of the config file). Live examples are available at https://github.com/daos-stack/daos/tree/master/utils/config The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_agent command line. Otherwise, /etc/daos_agent.conf is used. Refer to the example configuration file ( daos_server.yml ) for latest information and examples.","title":"Agent Configuration File"},{"location":"admin/deployment/#agent-startup","text":"DAOS Agent is a standalone application to be run on each compute node. It can be configured to use secure communications (default) or can be allowed to communicate with the control plane over unencrypted channels. The following example shows daos_agent being configured to operate in insecure mode due to incomplete integration of certificate support as of the 0.9 release and configured to use a non-default agent configuration file. To start the DAOS Agent from the command line using the default config file, /etc/daos/daos_agent.yaml, run: $ daos_agent & Or to specify a non-default configuration file, use the -o parameter $ daos_agent -o <'path to agent configuration file/daos_agent.yml'> & Alternatively, the DAOS Agent can be started as a systemd service. The DAOS Agent unit file is installed in the correct location when installing from RPMs. If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system. Once the file is copied modify the ExecStart line to point to your in tree daos_agent binary. ExecStart=/usr/bin/daos_agent Once the service file is installed, you can start daos_agent with the following commands: $ sudo systemctl enable daos_agent $ sudo systemctl start daos_agent To check the component status use: $ sudo systemctl status daos_agent If DAOS Agent failed to start check the logs with: $ sudo journalctl --unit daos_agent","title":"Agent Startup"},{"location":"admin/deployment/#system-validation","text":"To validate that the DAOS system is properly installed, the daos_test suite can be executed. Ensure the DAOS Agent is configured and running before running daos_test and that the DAOS_SINGLETON_CLI and CRT_ATTACH_INFO_PATH environment variables are properly set as described here . orterun -np <num_clients> --hostfile <hostfile> ./daos_test daos_test requires at least 8GB of SCM (or DRAM with tmpfs) storage on each storage node.","title":"System Validation"},{"location":"admin/deployment/#nvme-ssd-health-monitoring-stats","text":"Useful admin dmg commands to query NVMe SSD health: Query NVMe SSD Health Stats: $dmg storage query nvme-health Queries raw SPDK NVMe device health statistics for all NVMe SSDs on all hosts in list. $dmg storage query nvme-health -l=boro-11:10001 boro-11:10001: connected boro-11:10001 NVMe controllers and namespaces detail with health statistics: PCI:0000:81:00.0 Model:INTEL SSDPEDKE020T7 FW:QDV10130 Socket:1 Capacity:1.95TB Health Stats: Temperature:288K(15C) Controller Busy Time:5h26m0s Power Cycles:4 Power On Duration:16488h0m0s Unsafe Shutdowns:2 Media Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK Query Per-Server Metadata (SMD): $dmg storage query smd Queries persistently stored device and pool metadata tables. The device table maps device UUID to attached VOS target IDs. The pool table maps VOS target IDs to attached SPDK blob IDs. $dmg storage query smd --devices --pools -l=boro-11:10001 boro-11:10001: connected SMD Device List: boro-11:10001: Device: UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 VOS Target IDs: 0 1 2 3 SMD Pool List: boro-11:10001: Pool: UUID: 01b41f76-a783-462f-bbd2-eb27c2f7e326 VOS Target IDs: 0 1 3 2 SPDK Blobs: 4294967404 4294967405 4294967407 4294967406 Query Blobstore Health Data: $dmg storage query blobstore-health Queries in-memory health data for the SPDK blobstore (ie NVMe SSD). This includes a subset of the SPDK device health stats, as well as I/O error and checksum counters. $dmg storage query blobstore-health --devuuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 -l=boro-11:10001 boro-11:10001: connected Blobstore Health Data: boro-11:10001: Device UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 Read errors: 0 Write errors: 0 Unmap errors: 0 Checksum errors: 0 Device Health: Error log entries: 0 Media errors: 0 Temperature: 289 Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK Query Persistent Device State: $dmg storage query device-state Queries the current persistently stored device state of the specified NVMe SSD (either NORMAL or FAULTY). $dmg storage query device-state --devuuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 -l=boro-11:10001 boro-11:10001: connected Device State Info: boro-11:10001: Device UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 State: NORMAL Manually Set Device State to FAULTY: $dmg storage set nvme-faulty Allows the admin to manually set the device state of the given device to FAULTY, which will trigger faulty device reaction (all targets on the SSD will be rebuilt and the SSD will remain in an OUT state until reintegration is supported). $dmg storage set nvme-faulty --devuuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 -l=boro-11:10001 boro-11:10001: connected Device State Info: boro-11:10001: Device UUID: 5bd91603-d3c7-4fb7-9a71-76bc25690c19 State: FAULTY https://github.com/intel/ipmctl \u21a9 https://github.com/daos-stack/daos/tree/master/utils/config \u21a9 https://www.open-mpi.org/faq/?category=running#mpirun-hostfile \u21a9 https://github.com/daos-stack/daos/tree/master/src/control/README.md \u21a9","title":"NVMe SSD Health Monitoring &amp; Stats"},{"location":"admin/env_variables/","text":"Environment Variables \u00b6 This section lists the environment variables used by DAOS. Many of them are used for development purposes only and may be removed or changed in the future. The description of each variable follows the following format: Short description Type The default behavior if not set. A longer description if necessary This table defines a type: Type Values BOOL 0 means false; any other value means true BOOL2 no means false; any other value means true BOOL3 set to empty, or any value means true; unset means false INTEGER Non-negative decimal integer STRING String Common environment variables \u00b6 Environment variables in this section apply to both the server-side and the client-side. DAOS\\_IO\\_BYPASS Server environment variables \u00b6 Environment variables in this section only apply to the server-side. Variable Description VOS_CHECKSUM Checksum algorithm used by VOS. STRING. Default to disabling checksums. The following checksum algorithms are supported: crc64 and crc32. VOS_MEM_CLASS Memory class used by VOS. STRING. Default to persistent memory. If the value is set to DRAM, all data is stored in volatile memory; otherwise, all data is stored in persistent memory. RDB_ELECTION_TIMEOUT Raft election timeout used by RDBs in milliseconds. INTEGER. Default to 7000 ms. RDB_REQUEST_TIMEOUT Raft request timeout used by RDBs in milliseconds. INTEGER. Default to 3000 ms. DAOS_REBUILD Determines whether to start rebuilds when excluding targets. BOOL2. Default to true. DAOS_MD_CAP Size of a metadata pmem pool/file in MBs. INTEGER. Default to 128 MB. DAOS_START_POOL_SVC Determines whether to start existing pool services when starting a daos_server. BOOL. Default to true. DAOS_IMPLICIT_PURGE Whether to aggregate unreferenced epochs. BOOL. Default to false. DAOS_PURGE_CREDITS The number of credits for probing object trees when aggregating unreferenced epochs. INTEGER. Default to 1000. CRT_DISABLE_MEM_PIN Disable memory pinning workaround on a server side. BOOL. Default to 0. Server and Client \u00b6 Environment variables in this section apply to both server-side and client. |Variable|Description| |----|----| |FI_OFI_RXM_USE_SRX|Enable shared receive buffers for RXM-based providers (verbs, tcp). BOOL. Auto-defaults to 1.| |FI_UNIVERSE_SIZE|Sets expected universe size in OFI layer to be more than expected number of clients. INTEGER. Auto-defaults to 2048.| Client \u00b6 Environment variables in this section only apply to the client-side. Variable Description FI_MR_CACHE_MAX_COUNT Enable MR caching in OFI layer. Recommended to be set to 0 (disable) when CRT_DISABLE_MEM_PIN is NOT set to 1. INTEGER. Default to unset. Debug System (Client & Server) \u00b6 Variable Description D_LOG_FILE DAOS debug logs (both server and client) are written to /tmp/daos.log by default. The debug location can be modified by setting this environment variable (\"D_LOG_FILE=/tmp/daos_server\"). DD_SUBSYS Used to specify which subsystems to enable. DD_SUBSYS can be set to individual subsystems for finer-grained debugging (\"DD_SUBSYS=vos\"), multiple facilities (\"DD_SUBSYS=eio,mgmt,misc,mem\"), or all facilities (\"DD_SUBSYS=all\") which is also the default setting. If a facility is not enabled, then only ERR messages or more severe messages will print. DD_STDERR Used to specify the priority level to output to stderr. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. By default, all CRIT and more severe DAOS messages will log to stderr (\"DD_STDERR=CRIT\"), and the default for CaRT/GURT is FATAL. D_LOG_MASK Used to specify what type/level of logging will be present for either all of the registered subsystems or a select few. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. DEBUG option is used to enable all logging (debug messages as well as all higher priority level messages). Note that if D_LOG_MASK is not set, it will default to logging all messages excluding debug (\"D_LOG_MASK=INFO\"). EX: \"D_LOG_MASK=DEBUG\" This will set the logging level for all facilities to DEBUG, meaning that all debug messages, as well as higher priority messages will be logged (INFO, NOTE, WARN, ERR, CRIT, FATAL) EX: \"D_LOG_MASK=DEBUG,MEM=ERR,RPC=ERR\" This will set the logging level to DEBUG for all facilities except MEM & RPC (which will now only log ERR and higher priority level messages, skipping all DEBUG, INFO, NOTE & WARN messages) DD_MASK Used to enable different debug streams for finer-grained debug messages, essentially allowing the user to specify an area of interest to debug (possibly involving many different subsystems) as opposed to parsing through many lines of generic DEBUG messages. All debug streams will be enabled by default (\"DD_MASK=all\"). Single debug masks can be set (\"DD_MASK=trace\") or multiple masks (\"DD_MASK=trace,test,mgmt\"). Note that since these debug streams are strictly related to the debug log messages, DD_LOG_MASK must be set to DEBUG. Priority messages higher than DEBUG will still be logged for all facilities unless otherwise specified by D_LOG_MASK (not affected by enabling debug masks).","title":"Environment Variables"},{"location":"admin/env_variables/#environment-variables","text":"This section lists the environment variables used by DAOS. Many of them are used for development purposes only and may be removed or changed in the future. The description of each variable follows the following format: Short description Type The default behavior if not set. A longer description if necessary This table defines a type: Type Values BOOL 0 means false; any other value means true BOOL2 no means false; any other value means true BOOL3 set to empty, or any value means true; unset means false INTEGER Non-negative decimal integer STRING String","title":"Environment Variables"},{"location":"admin/env_variables/#common-environment-variables","text":"Environment variables in this section apply to both the server-side and the client-side. DAOS\\_IO\\_BYPASS","title":"Common environment variables"},{"location":"admin/env_variables/#server-environment-variables","text":"Environment variables in this section only apply to the server-side. Variable Description VOS_CHECKSUM Checksum algorithm used by VOS. STRING. Default to disabling checksums. The following checksum algorithms are supported: crc64 and crc32. VOS_MEM_CLASS Memory class used by VOS. STRING. Default to persistent memory. If the value is set to DRAM, all data is stored in volatile memory; otherwise, all data is stored in persistent memory. RDB_ELECTION_TIMEOUT Raft election timeout used by RDBs in milliseconds. INTEGER. Default to 7000 ms. RDB_REQUEST_TIMEOUT Raft request timeout used by RDBs in milliseconds. INTEGER. Default to 3000 ms. DAOS_REBUILD Determines whether to start rebuilds when excluding targets. BOOL2. Default to true. DAOS_MD_CAP Size of a metadata pmem pool/file in MBs. INTEGER. Default to 128 MB. DAOS_START_POOL_SVC Determines whether to start existing pool services when starting a daos_server. BOOL. Default to true. DAOS_IMPLICIT_PURGE Whether to aggregate unreferenced epochs. BOOL. Default to false. DAOS_PURGE_CREDITS The number of credits for probing object trees when aggregating unreferenced epochs. INTEGER. Default to 1000. CRT_DISABLE_MEM_PIN Disable memory pinning workaround on a server side. BOOL. Default to 0.","title":"Server environment variables"},{"location":"admin/env_variables/#server-and-client","text":"Environment variables in this section apply to both server-side and client. |Variable|Description| |----|----| |FI_OFI_RXM_USE_SRX|Enable shared receive buffers for RXM-based providers (verbs, tcp). BOOL. Auto-defaults to 1.| |FI_UNIVERSE_SIZE|Sets expected universe size in OFI layer to be more than expected number of clients. INTEGER. Auto-defaults to 2048.|","title":"Server and Client"},{"location":"admin/env_variables/#client","text":"Environment variables in this section only apply to the client-side. Variable Description FI_MR_CACHE_MAX_COUNT Enable MR caching in OFI layer. Recommended to be set to 0 (disable) when CRT_DISABLE_MEM_PIN is NOT set to 1. INTEGER. Default to unset.","title":"Client"},{"location":"admin/env_variables/#debug-system-client-server","text":"Variable Description D_LOG_FILE DAOS debug logs (both server and client) are written to /tmp/daos.log by default. The debug location can be modified by setting this environment variable (\"D_LOG_FILE=/tmp/daos_server\"). DD_SUBSYS Used to specify which subsystems to enable. DD_SUBSYS can be set to individual subsystems for finer-grained debugging (\"DD_SUBSYS=vos\"), multiple facilities (\"DD_SUBSYS=eio,mgmt,misc,mem\"), or all facilities (\"DD_SUBSYS=all\") which is also the default setting. If a facility is not enabled, then only ERR messages or more severe messages will print. DD_STDERR Used to specify the priority level to output to stderr. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. By default, all CRIT and more severe DAOS messages will log to stderr (\"DD_STDERR=CRIT\"), and the default for CaRT/GURT is FATAL. D_LOG_MASK Used to specify what type/level of logging will be present for either all of the registered subsystems or a select few. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. DEBUG option is used to enable all logging (debug messages as well as all higher priority level messages). Note that if D_LOG_MASK is not set, it will default to logging all messages excluding debug (\"D_LOG_MASK=INFO\"). EX: \"D_LOG_MASK=DEBUG\" This will set the logging level for all facilities to DEBUG, meaning that all debug messages, as well as higher priority messages will be logged (INFO, NOTE, WARN, ERR, CRIT, FATAL) EX: \"D_LOG_MASK=DEBUG,MEM=ERR,RPC=ERR\" This will set the logging level to DEBUG for all facilities except MEM & RPC (which will now only log ERR and higher priority level messages, skipping all DEBUG, INFO, NOTE & WARN messages) DD_MASK Used to enable different debug streams for finer-grained debug messages, essentially allowing the user to specify an area of interest to debug (possibly involving many different subsystems) as opposed to parsing through many lines of generic DEBUG messages. All debug streams will be enabled by default (\"DD_MASK=all\"). Single debug masks can be set (\"DD_MASK=trace\") or multiple masks (\"DD_MASK=trace,test,mgmt\"). Note that since these debug streams are strictly related to the debug log messages, DD_LOG_MASK must be set to DEBUG. Priority messages higher than DEBUG will still be logged for all facilities unless otherwise specified by D_LOG_MASK (not affected by enabling debug masks).","title":"Debug System (Client &amp; Server)"},{"location":"admin/hardware/","text":"Hardware Requirements \u00b6 The purpose of this section is to describe processor, storage, and network requirements to deploy a DAOS system. Deployment Options \u00b6 As illustrated in the figure below, a DAOS system can be deployed in two different ways: Pooled Storage Model : The DAOS servers can run on dedicated storage nodes in separate racks. This is a traditional pool model where storage is uniformly accessed by all compute nodes. In order to minimize the number of I/O racks and to optimize floor space, this approach usually requires high-density storage servers. Disaggregated Storage Model : In the disaggregated model, the storage nodes are integrated into compute racks and can be either dedicated or shared (e.g., in a hyper-converged infrastructure) nodes. The DAOS servers are thus massively distributed and storage access is non-uniform and must take locality into account. While DAOS is mostly deployed following the pooled model, active research is conducted to efficiently support the disaggregated model as well. Processor Requirements \u00b6 DAOS requires a 64-bit processor architecture and is primarily developed on Intel 64 architecture. The DAOS software and the libraries it depends on (e.g., ISA-L, SPDK, PMDK, and DPDK) can take advantage of Intel\u00ae SSE and AVX extensions. DAOS is also regularly tested on 64-bit ARM processors configured in Little Endian mode. The same build instructions that are used for x86-64 are applicable for ARM builds as well. DAOS and its dependencies will make the necessary adjustments automatically in their respective build systems for ARM platforms. Network Requirements \u00b6 The DAOS network layer relies on libfabrics and supports OFI providers for Ethernet/sockets, InfiniBand/verbs, RoCE, Cray\u2019s GNI, and the Intel Omni-Path Architecture. An RDMA-capable fabric is preferred for better performance. DAOS can support multiple rails by binding different instances of the DAOS server to individual network cards. An additional out-of-band network connecting the nodes in the DAOS service cluster is required for DAOS administration. Management traffic uses IP over Fabric. Storage Requirements \u00b6 DAOS requires each storage node to have direct access to storage-class memory (SCM). While DAOS is primarily tested and tuned for Optane DC Persistent Memory, the DAOS software stack is built over the Persistent Memory Development Kit (PMDK) and the DAX feature of the Linux and Windows operating systems as described in the SNIA NVM Programming Model 1 . As a result, the open-source DAOS software stack should be able to run transparently over any storage-class memory supported by PMDK. The storage node can be optionally equipped with NVMe (non-volatile memory express) SSDs to provide capacity. HDDs, as well as SATA and SAS SSDs, are not supported by DAOS. Both NVMe 3D-NAND and Optane SSDs are supported. Optane SSDs are preferred for DAOS installation that targets a very high IOPS rate. NVMe-oF devices are also supported by the userspace storage stack, but have never been tested. The minimal recommended ratio between SCM and SSDs capacity is 6% to guarantee that DAOS has enough space in SCM to store internal metadata (e.g., pool metadata, SSD block allocation tracking). For testing purposes, SCM can be emulated with DRAM by mounting a tmpfs filesystem, and NVMe SSDs can be also emulated with DRAM or a loopback file. CPU Affinity \u00b6 On recent Xeon platforms, PCIe slots have a natural affinity to one CPU. Although globally accessible from any of the system cores, NVMe SSDs and network interface cards connected through the PCIe bus may provide different performance characteristics (e.g., higher latency, lower bandwidth) to each CPU. Accessing \u201cremote\u201d PCIe devices may involve traffic over the UPI (Ultra Path Interconnect) link that might become a point of congestion. Similarly, persistent memory is non-uniformly accessible (NUMA), and CPU affinity must be respected for maximal performance. Therefore, when running in a multi-socket and multi-rail environment, the DAOS service must be able to detect the CPU to PCIe device and persistent memory affinity and minimize as much as possible non-local access. This can be achieved by spawning one instance of the I/O server per CPU, then accessing only local persistent memory and PCI devices from that server instance. The control plane is responsible for detecting the storage and network affinity and starting the I/O servers accordingly. Fault Domains \u00b6 DAOS relies on single-ported storage massively distributed across different storage nodes. Each storage node is thus a single point of failure. DAOS achieves fault tolerance by providing data redundancy across storage nodes in different fault domains. DAOS assumes that fault domains are hierarchical and do not overlap. For instance, the first level of a fault domain could be the racks and the second one the storage nodes. For efficient placement and optimal data resilience, more fault domains are better. As a result, it is preferable to distribute storage nodes across as many racks as possible. https://www.snia.org/sites/default/files/technical_work/final/NVMProgrammingModel_v1.2.pdf \u21a9","title":"Hardware Requirements"},{"location":"admin/hardware/#hardware-requirements","text":"The purpose of this section is to describe processor, storage, and network requirements to deploy a DAOS system.","title":"Hardware Requirements"},{"location":"admin/hardware/#deployment-options","text":"As illustrated in the figure below, a DAOS system can be deployed in two different ways: Pooled Storage Model : The DAOS servers can run on dedicated storage nodes in separate racks. This is a traditional pool model where storage is uniformly accessed by all compute nodes. In order to minimize the number of I/O racks and to optimize floor space, this approach usually requires high-density storage servers. Disaggregated Storage Model : In the disaggregated model, the storage nodes are integrated into compute racks and can be either dedicated or shared (e.g., in a hyper-converged infrastructure) nodes. The DAOS servers are thus massively distributed and storage access is non-uniform and must take locality into account. While DAOS is mostly deployed following the pooled model, active research is conducted to efficiently support the disaggregated model as well.","title":"Deployment Options"},{"location":"admin/hardware/#processor-requirements","text":"DAOS requires a 64-bit processor architecture and is primarily developed on Intel 64 architecture. The DAOS software and the libraries it depends on (e.g., ISA-L, SPDK, PMDK, and DPDK) can take advantage of Intel\u00ae SSE and AVX extensions. DAOS is also regularly tested on 64-bit ARM processors configured in Little Endian mode. The same build instructions that are used for x86-64 are applicable for ARM builds as well. DAOS and its dependencies will make the necessary adjustments automatically in their respective build systems for ARM platforms.","title":"Processor Requirements"},{"location":"admin/hardware/#network-requirements","text":"The DAOS network layer relies on libfabrics and supports OFI providers for Ethernet/sockets, InfiniBand/verbs, RoCE, Cray\u2019s GNI, and the Intel Omni-Path Architecture. An RDMA-capable fabric is preferred for better performance. DAOS can support multiple rails by binding different instances of the DAOS server to individual network cards. An additional out-of-band network connecting the nodes in the DAOS service cluster is required for DAOS administration. Management traffic uses IP over Fabric.","title":"Network Requirements"},{"location":"admin/hardware/#storage-requirements","text":"DAOS requires each storage node to have direct access to storage-class memory (SCM). While DAOS is primarily tested and tuned for Optane DC Persistent Memory, the DAOS software stack is built over the Persistent Memory Development Kit (PMDK) and the DAX feature of the Linux and Windows operating systems as described in the SNIA NVM Programming Model 1 . As a result, the open-source DAOS software stack should be able to run transparently over any storage-class memory supported by PMDK. The storage node can be optionally equipped with NVMe (non-volatile memory express) SSDs to provide capacity. HDDs, as well as SATA and SAS SSDs, are not supported by DAOS. Both NVMe 3D-NAND and Optane SSDs are supported. Optane SSDs are preferred for DAOS installation that targets a very high IOPS rate. NVMe-oF devices are also supported by the userspace storage stack, but have never been tested. The minimal recommended ratio between SCM and SSDs capacity is 6% to guarantee that DAOS has enough space in SCM to store internal metadata (e.g., pool metadata, SSD block allocation tracking). For testing purposes, SCM can be emulated with DRAM by mounting a tmpfs filesystem, and NVMe SSDs can be also emulated with DRAM or a loopback file.","title":"Storage Requirements"},{"location":"admin/hardware/#cpu-affinity","text":"On recent Xeon platforms, PCIe slots have a natural affinity to one CPU. Although globally accessible from any of the system cores, NVMe SSDs and network interface cards connected through the PCIe bus may provide different performance characteristics (e.g., higher latency, lower bandwidth) to each CPU. Accessing \u201cremote\u201d PCIe devices may involve traffic over the UPI (Ultra Path Interconnect) link that might become a point of congestion. Similarly, persistent memory is non-uniformly accessible (NUMA), and CPU affinity must be respected for maximal performance. Therefore, when running in a multi-socket and multi-rail environment, the DAOS service must be able to detect the CPU to PCIe device and persistent memory affinity and minimize as much as possible non-local access. This can be achieved by spawning one instance of the I/O server per CPU, then accessing only local persistent memory and PCI devices from that server instance. The control plane is responsible for detecting the storage and network affinity and starting the I/O servers accordingly.","title":"CPU Affinity"},{"location":"admin/hardware/#fault-domains","text":"DAOS relies on single-ported storage massively distributed across different storage nodes. Each storage node is thus a single point of failure. DAOS achieves fault tolerance by providing data redundancy across storage nodes in different fault domains. DAOS assumes that fault domains are hierarchical and do not overlap. For instance, the first level of a fault domain could be the racks and the second one the storage nodes. For efficient placement and optimal data resilience, more fault domains are better. As a result, it is preferable to distribute storage nodes across as many racks as possible. https://www.snia.org/sites/default/files/technical_work/final/NVMProgrammingModel_v1.2.pdf \u21a9","title":"Fault Domains"},{"location":"admin/installation/","text":"Software Installation \u00b6 DAOS runs on both Intel 64 and ARM64 platforms and has been successfully tested on CentOS 7, OpenSUSE Leap 15.1, and Ubuntu 18.04 distributions. Software Dependencies \u00b6 DAOS requires a C99-capable compiler, a Go compiler, and the scons build tool. Moreover, the DAOS stack leverages the following open source projects: CaRT for high-performance communication leveraging advanced network capabilities. gRPC provides a secured out-of-band channel for DAOS administration. PMDK for persistent memory programming. SPDK for userspace NVMe device access and management. FIO for flexible testing of Linux I/O subsystems, specifically enabling validation of userspace NVMe device performance through fio-spdk plugin. ISA-L for checksum and erasure code computation. Argobots for thread management. hwloc for discovering system devices, detecting their NUMA node affinity and for CPU binding libfabric for detecting fabric interfaces, providers and connection management. The DAOS build system can be configured to download and build any missing dependencies automatically. Distribution Packages \u00b6 DAOS RPM and deb packaging is under development and will be available for DAOS v1.0. Integration with the Spack package manager is also under consideration. DAOS from Scratch \u00b6 The following instructions have been verified with CentOS. Installations on other Linux distributions might be similar with some variations. Developers of DAOS may want to review the additional sections below before beginning, for suggestions related specifically to development. Contact us in our forum for further help with any issues. Build Prerequisites \u00b6 To build DAOS and its dependencies, several software packages must be installed on the system. This includes scons, libuuid, cmocka, ipmctl, and several other packages usually available on all the Linux distributions. Moreover, a Go version of at least 1.10 is required. An exhaustive list of packages for each supported Linux distribution is maintained in the Docker files: CentOS OpenSUSE Ubuntu The command lines to install the required packages can be extracted from the Docker files by removing the \"RUN\" command, which is specific to Docker. Check the docker directory for different Linux distribution versions. Some DAOS tests use MPI. The DAOS build process uses the environment modules package to detect the presence of MPI. If none is found, the build will skip building those tests. DAOS Source Code \u00b6 To check out the DAOS source code, run the following command: $ git clone https://github.com/daos-stack/daos.git This command clones the DAOS git repository (path referred as ${daospath} below). Then initialize the submodules with: $ cd ${daospath} $ git submodule init $ git submodule update Building DAOS & Dependencies \u00b6 If all the software dependencies listed previously are already satisfied, then type the following command in the top source directory to build the DAOS stack: $ scons --config=force install If you are a developer of DAOS, we recommend following the instructions in the DAOS for Development section. Otherwise, the missing dependencies can be built automatically by invoking scons with the following parameters: $ scons --config=force --build-deps=yes install By default, DAOS and its dependencies are installed under ${daospath}/install. The installation path can be modified by adding the PREFIX= option to the above command line (e.g., PREFIX=/usr/local). Environment setup \u00b6 Once built, the environment must be modified to search for binaries and header files in the installation path. This step is not required if standard locations (e.g. /bin, /sbin, /usr/lib, ...) are used. CPATH=${daospath}/install/include/:$CPATH PATH=${daospath}/install/bin/:${daospath}/install/sbin:$PATH export CPATH PATH If using bash, PATH can be set up for you after a build by sourcing the script scons_local/utils/setup_local.sh from the daos root. This script utilizes a file generated by the build to determine the location of daos and its dependencies. If required, ${daospath}/install must be replaced with the alternative path specified through PREFIX. DAOS in Docker \u00b6 This section describes how to build and run the DAOS service in a Docker container. A minimum of 5GB of DRAM and 16GB of disk space will be required. On Mac, please make sure that the Docker settings under \"Preferences/{Disk,Memory}\" are configured accordingly. Building from GitHub \u00b6 To build the Docker image directly from GitHub, run the following command: $ docker build -t daos -f Dockerfile.centos.7 github.com/daos-stack/daos#:utils/docker This creates a CentOS 7 image, fetches the latest DAOS version from GitHub, builds it, and installs it in the image. For Ubuntu and other Linux distributions, replace Dockerfile.centos.7 with Dockerfile.ubuntu.18.04 and the appropriate version of interest. Once the image created, one can start a container that will eventually run the DAOS service: $ docker run -it -d --privileged --name server \\ -v /dev/hugepages:/dev/hugepages \\ daos If Docker is being run on a non-Linux system (e.g., OSX), the export of /dev/hugepages should be removed since it is not supported. Building from a Local Tree \u00b6 To build from a local tree stored on the host, a volume must be created to share the source tree with the Docker container. To do so, execute the following command to create a docker image without checking out the DAOS source tree: $ docker build -t daos -f utils/docker/Dockerfile.centos.7 --build-arg NOBUILD=1 . Then create a container that can access the local DAOS source tree: $ docker run -it -d --privileged --name server \\ -v ${daospath}:/home/daos/daos:Z \\ -v /dev/hugepages:/dev/hugepages \\ daos ${daospath} should be replaced with the full path to your DAOS source tree. As mentioned above, the export of /dev/hugepages should be removed if the host is not a Linux system. Then execute the following command to build and install DAOS in the container: $ docker exec server scons --build-deps=yes install PREFIX=/usr Running DAOS Service in Docker \u00b6 Please first make sure that the uio_pci_generic module is loaded: $ ls /sys/bus/pci/drivers/uio_pci_generic ls: cannot access /sys/bus/pci/drivers/uio_pci_generic: No such file or director $ sudo modprobe uio_pci_generic $ ls /sys/bus/pci/drivers/uio_pci_generic 0000:00:04.0 0000:00:04.3 0000:00:04.6 0000:5f:00.0 0000:80:04.2 0000:80:04.5 0000:81:00.0 module uevent 0000:00:04.1 0000:00:04.4 0000:00:04.7 0000:80:04.0 0000:80:04.3 0000:80:04.6 0000:da:00.0 new_id unbind 0000:00:04.2 0000:00:04.5 0000:5e:00.0 0000:80:04.1 0000:80:04.4 0000:80:04.7 bind remove_id SCM and NVMe storage can then be configured by running the follow command: $ docker exec server daos_server storage prepare Note that this command reports that /dev/hugepages is not accessible on OSX. This still allows running the DAOS service despite the error. The DAOS service can then be started as follows: $ docker exec server mkdir /var/run/daos_server $ docker exec server daos_server start \\ -o /home/daos/daos/utils/config/examples/daos_server_local.yml The daos_server_local.yml configuration file sets up a simple local DAOS system with a single server instance running in the container. By default, it uses 4GB of DRAM to emulate persistent memory and 16GB of bulk storage under /tmp. The storage size can be changed in the yaml file if necessary. Once started, the DAOS server waits for the administrator to format the system. This can be triggered in a different shell, using the following command: $ docker exec server dmg -i storage format Upon successful completion of the format, the storage engine is started, and pools can be created using the daos admin tool (see next section). DAOS for Development \u00b6 This section covers specific instructions to create a developer-friendly environment to contribute to the DAOS development. This includes how to regenerate the protobuf files or add new Go package dependencies, which is only required for development purposes. Building DAOS for Development \u00b6 For development, it is recommended to build and install each dependency in a unique subdirectory. The DAOS build system supports this through the TARGET_PREFIX variable. Once the submodules have been initialized and updated, run the following commands: $ scons PREFIX=${daos_prefix_path} TARGET_PREFIX=${daos_prefix_path}/opt install --build-deps=yes --config=force Installing the components into seperate directories allow upgrading the components individually by replacing --build-deps=yes with --update-prereq={component_name}. This requires a change to the environment configuration from before. For automated environment setup, source scons_local/utils/setup_local.sh. ARGOBOTS=${daos_prefix_path}/opt/argobots CART=${daos_prefix_path}/opt/cart FIO=${daos_prefix_path}/opt/fio FUSE=${daos_prefix_path}/opt/fuse ISAL=${daos_prefix_path}/opt/isal MERCURY=${daos_prefix_path}/opt/mercury OFI=${daos_prefix_path}/opt/ofi OPENPA=${daos_prefix_path}/opt/openpa PMDK=${daos_prefix_path}/opt/pmdk PROTOBUFC=${daos_prefix_path}/opt/protobufc SPDK=${daos_prefix_path}/opt/spdk LD_LIBRARY_PATH=${daos_prefix_path}/opt/spdk/lib:${daos_prefix_path}/opt/protobufc/lib:${daos_prefix_path}/opt/pmdk/lib:${daos_prefix_path}/opt/openpa/lib:${daos_prefix_path}/opt/ofi/lib:${daos_prefix_path}/opt/mercury/lib:${daos_prefix_path}/opt/isal/lib:${daos_prefix_path}/opt/fuse/lib64:${daos_prefix_path}/opt/cart/lib:${daos_prefix_path}/opt/argobots/lib PATH=${daos_prefix_path}/opt/spdk/bin:${daos_prefix_path}/opt/pmdk/bin:${daos_prefix_path}/opt/ofi/bin:${daos_prefix_path}/opt/isal/bin:${daos_prefix_path}/opt/fio/bin:${daos_prefix_path}/opt/cart/bin With this approach, DAOS would get built using the prebuilt dependencies in ${daos_prefix_path}/opt, and required options are saved for future compilations. So, after the first time, during development, only \"scons --config=force\" and \"scons --config=force install\" would suffice for compiling changes to DAOS source code. Using other compilers \u00b6 If you wish to compile DAOS with clang or use the Intel compilers rather than gcc, set COMPILER=clang or COMPILER=icc on the scons command line. This option is also saved for future compilations. Go dependencies \u00b6 Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses dep to manage these dependencies. On EL7 and later: $ yum install yum-plugin-copr $ yum copr enable hnakamur/golang-dep $ yum install golang-dep On Fedora 27 and later: $ dnf install dep On Ubuntu 18.04 and later: $ apt-get install go-dep For OSes that don't supply a package: Ensure that you have a personal GOPATH (see \"go env GOPATH\", referred to as \"$GOPATH\" in this document) and a GOBIN ($GOPATH/bin) set up and included in your PATH: $ mkdir -p $GOPATH/bin $ export PATH=$GOPATH/bin:$PATH Then follow the installation instructions on Github . To update the vendor directory using dep after changing Gopkg.toml, make sure DAOS is cloned into \"$GOPATH/src/github.com/daos-stack/daos\" Then: $ cd $GOPATH/src/github.com/daos-stack/daos/src/control $ dep ensure Protobuf Compiler \u00b6 The DAOS control plane infrastructure uses protobuf as the data serialization format for its RPC requests. The DAOS proto files use protobuf 3 syntax, which is not supported by the platform protobuf compiler in all cases. Not all developers will need to build the proto files into the various source files. However, if changes are made to the proto files, the corresponding C and Go source files will need to be regenerated with a protobuf 3.* or higher compiler. The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work but are not guaranteed. Protocol Buffers v3.5.1. Installation instructions . Protobuf-C v1.3.1. Installation instructions . gRPC plugin: protoc-gen-go v1.2.0. Must match the proto version in src/control/Gopkg.toml. Install the specific version using GIT_TAG instructions here . Generate the Go file using the gRPC plugin. You can designate the directory location: $ protoc myfile.proto --go_out=plugins=grpc:<go_file_dir> Generate the C files using Protobuf-C. As the header and source files in DAOS are typically kept in separate locations, you will need to move them manually to their destination directories: $ protoc-c myfile.proto --c_out=. $ mv myfile.pb-c.h <c_file_include_dir> $ mv myfile.pb-c.c <c_file_src_dir>","title":"Software Installation"},{"location":"admin/installation/#software-installation","text":"DAOS runs on both Intel 64 and ARM64 platforms and has been successfully tested on CentOS 7, OpenSUSE Leap 15.1, and Ubuntu 18.04 distributions.","title":"Software Installation"},{"location":"admin/installation/#software-dependencies","text":"DAOS requires a C99-capable compiler, a Go compiler, and the scons build tool. Moreover, the DAOS stack leverages the following open source projects: CaRT for high-performance communication leveraging advanced network capabilities. gRPC provides a secured out-of-band channel for DAOS administration. PMDK for persistent memory programming. SPDK for userspace NVMe device access and management. FIO for flexible testing of Linux I/O subsystems, specifically enabling validation of userspace NVMe device performance through fio-spdk plugin. ISA-L for checksum and erasure code computation. Argobots for thread management. hwloc for discovering system devices, detecting their NUMA node affinity and for CPU binding libfabric for detecting fabric interfaces, providers and connection management. The DAOS build system can be configured to download and build any missing dependencies automatically.","title":"Software Dependencies"},{"location":"admin/installation/#distribution-packages","text":"DAOS RPM and deb packaging is under development and will be available for DAOS v1.0. Integration with the Spack package manager is also under consideration.","title":"Distribution Packages"},{"location":"admin/installation/#daos-from-scratch","text":"The following instructions have been verified with CentOS. Installations on other Linux distributions might be similar with some variations. Developers of DAOS may want to review the additional sections below before beginning, for suggestions related specifically to development. Contact us in our forum for further help with any issues.","title":"DAOS from Scratch"},{"location":"admin/installation/#build-prerequisites","text":"To build DAOS and its dependencies, several software packages must be installed on the system. This includes scons, libuuid, cmocka, ipmctl, and several other packages usually available on all the Linux distributions. Moreover, a Go version of at least 1.10 is required. An exhaustive list of packages for each supported Linux distribution is maintained in the Docker files: CentOS OpenSUSE Ubuntu The command lines to install the required packages can be extracted from the Docker files by removing the \"RUN\" command, which is specific to Docker. Check the docker directory for different Linux distribution versions. Some DAOS tests use MPI. The DAOS build process uses the environment modules package to detect the presence of MPI. If none is found, the build will skip building those tests.","title":"Build Prerequisites"},{"location":"admin/installation/#daos-source-code","text":"To check out the DAOS source code, run the following command: $ git clone https://github.com/daos-stack/daos.git This command clones the DAOS git repository (path referred as ${daospath} below). Then initialize the submodules with: $ cd ${daospath} $ git submodule init $ git submodule update","title":"DAOS Source Code"},{"location":"admin/installation/#building-daos-dependencies","text":"If all the software dependencies listed previously are already satisfied, then type the following command in the top source directory to build the DAOS stack: $ scons --config=force install If you are a developer of DAOS, we recommend following the instructions in the DAOS for Development section. Otherwise, the missing dependencies can be built automatically by invoking scons with the following parameters: $ scons --config=force --build-deps=yes install By default, DAOS and its dependencies are installed under ${daospath}/install. The installation path can be modified by adding the PREFIX= option to the above command line (e.g., PREFIX=/usr/local).","title":"Building DAOS &amp; Dependencies"},{"location":"admin/installation/#environment-setup","text":"Once built, the environment must be modified to search for binaries and header files in the installation path. This step is not required if standard locations (e.g. /bin, /sbin, /usr/lib, ...) are used. CPATH=${daospath}/install/include/:$CPATH PATH=${daospath}/install/bin/:${daospath}/install/sbin:$PATH export CPATH PATH If using bash, PATH can be set up for you after a build by sourcing the script scons_local/utils/setup_local.sh from the daos root. This script utilizes a file generated by the build to determine the location of daos and its dependencies. If required, ${daospath}/install must be replaced with the alternative path specified through PREFIX.","title":"Environment setup"},{"location":"admin/installation/#daos-in-docker","text":"This section describes how to build and run the DAOS service in a Docker container. A minimum of 5GB of DRAM and 16GB of disk space will be required. On Mac, please make sure that the Docker settings under \"Preferences/{Disk,Memory}\" are configured accordingly.","title":"DAOS in Docker"},{"location":"admin/installation/#building-from-github","text":"To build the Docker image directly from GitHub, run the following command: $ docker build -t daos -f Dockerfile.centos.7 github.com/daos-stack/daos#:utils/docker This creates a CentOS 7 image, fetches the latest DAOS version from GitHub, builds it, and installs it in the image. For Ubuntu and other Linux distributions, replace Dockerfile.centos.7 with Dockerfile.ubuntu.18.04 and the appropriate version of interest. Once the image created, one can start a container that will eventually run the DAOS service: $ docker run -it -d --privileged --name server \\ -v /dev/hugepages:/dev/hugepages \\ daos If Docker is being run on a non-Linux system (e.g., OSX), the export of /dev/hugepages should be removed since it is not supported.","title":"Building from GitHub"},{"location":"admin/installation/#building-from-a-local-tree","text":"To build from a local tree stored on the host, a volume must be created to share the source tree with the Docker container. To do so, execute the following command to create a docker image without checking out the DAOS source tree: $ docker build -t daos -f utils/docker/Dockerfile.centos.7 --build-arg NOBUILD=1 . Then create a container that can access the local DAOS source tree: $ docker run -it -d --privileged --name server \\ -v ${daospath}:/home/daos/daos:Z \\ -v /dev/hugepages:/dev/hugepages \\ daos ${daospath} should be replaced with the full path to your DAOS source tree. As mentioned above, the export of /dev/hugepages should be removed if the host is not a Linux system. Then execute the following command to build and install DAOS in the container: $ docker exec server scons --build-deps=yes install PREFIX=/usr","title":"Building from a Local Tree"},{"location":"admin/installation/#running-daos-service-in-docker","text":"Please first make sure that the uio_pci_generic module is loaded: $ ls /sys/bus/pci/drivers/uio_pci_generic ls: cannot access /sys/bus/pci/drivers/uio_pci_generic: No such file or director $ sudo modprobe uio_pci_generic $ ls /sys/bus/pci/drivers/uio_pci_generic 0000:00:04.0 0000:00:04.3 0000:00:04.6 0000:5f:00.0 0000:80:04.2 0000:80:04.5 0000:81:00.0 module uevent 0000:00:04.1 0000:00:04.4 0000:00:04.7 0000:80:04.0 0000:80:04.3 0000:80:04.6 0000:da:00.0 new_id unbind 0000:00:04.2 0000:00:04.5 0000:5e:00.0 0000:80:04.1 0000:80:04.4 0000:80:04.7 bind remove_id SCM and NVMe storage can then be configured by running the follow command: $ docker exec server daos_server storage prepare Note that this command reports that /dev/hugepages is not accessible on OSX. This still allows running the DAOS service despite the error. The DAOS service can then be started as follows: $ docker exec server mkdir /var/run/daos_server $ docker exec server daos_server start \\ -o /home/daos/daos/utils/config/examples/daos_server_local.yml The daos_server_local.yml configuration file sets up a simple local DAOS system with a single server instance running in the container. By default, it uses 4GB of DRAM to emulate persistent memory and 16GB of bulk storage under /tmp. The storage size can be changed in the yaml file if necessary. Once started, the DAOS server waits for the administrator to format the system. This can be triggered in a different shell, using the following command: $ docker exec server dmg -i storage format Upon successful completion of the format, the storage engine is started, and pools can be created using the daos admin tool (see next section).","title":"Running DAOS Service in Docker"},{"location":"admin/installation/#daos-for-development","text":"This section covers specific instructions to create a developer-friendly environment to contribute to the DAOS development. This includes how to regenerate the protobuf files or add new Go package dependencies, which is only required for development purposes.","title":"DAOS for Development"},{"location":"admin/installation/#building-daos-for-development","text":"For development, it is recommended to build and install each dependency in a unique subdirectory. The DAOS build system supports this through the TARGET_PREFIX variable. Once the submodules have been initialized and updated, run the following commands: $ scons PREFIX=${daos_prefix_path} TARGET_PREFIX=${daos_prefix_path}/opt install --build-deps=yes --config=force Installing the components into seperate directories allow upgrading the components individually by replacing --build-deps=yes with --update-prereq={component_name}. This requires a change to the environment configuration from before. For automated environment setup, source scons_local/utils/setup_local.sh. ARGOBOTS=${daos_prefix_path}/opt/argobots CART=${daos_prefix_path}/opt/cart FIO=${daos_prefix_path}/opt/fio FUSE=${daos_prefix_path}/opt/fuse ISAL=${daos_prefix_path}/opt/isal MERCURY=${daos_prefix_path}/opt/mercury OFI=${daos_prefix_path}/opt/ofi OPENPA=${daos_prefix_path}/opt/openpa PMDK=${daos_prefix_path}/opt/pmdk PROTOBUFC=${daos_prefix_path}/opt/protobufc SPDK=${daos_prefix_path}/opt/spdk LD_LIBRARY_PATH=${daos_prefix_path}/opt/spdk/lib:${daos_prefix_path}/opt/protobufc/lib:${daos_prefix_path}/opt/pmdk/lib:${daos_prefix_path}/opt/openpa/lib:${daos_prefix_path}/opt/ofi/lib:${daos_prefix_path}/opt/mercury/lib:${daos_prefix_path}/opt/isal/lib:${daos_prefix_path}/opt/fuse/lib64:${daos_prefix_path}/opt/cart/lib:${daos_prefix_path}/opt/argobots/lib PATH=${daos_prefix_path}/opt/spdk/bin:${daos_prefix_path}/opt/pmdk/bin:${daos_prefix_path}/opt/ofi/bin:${daos_prefix_path}/opt/isal/bin:${daos_prefix_path}/opt/fio/bin:${daos_prefix_path}/opt/cart/bin With this approach, DAOS would get built using the prebuilt dependencies in ${daos_prefix_path}/opt, and required options are saved for future compilations. So, after the first time, during development, only \"scons --config=force\" and \"scons --config=force install\" would suffice for compiling changes to DAOS source code.","title":"Building DAOS for Development"},{"location":"admin/installation/#using-other-compilers","text":"If you wish to compile DAOS with clang or use the Intel compilers rather than gcc, set COMPILER=clang or COMPILER=icc on the scons command line. This option is also saved for future compilations.","title":"Using other compilers"},{"location":"admin/installation/#go-dependencies","text":"Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses dep to manage these dependencies. On EL7 and later: $ yum install yum-plugin-copr $ yum copr enable hnakamur/golang-dep $ yum install golang-dep On Fedora 27 and later: $ dnf install dep On Ubuntu 18.04 and later: $ apt-get install go-dep For OSes that don't supply a package: Ensure that you have a personal GOPATH (see \"go env GOPATH\", referred to as \"$GOPATH\" in this document) and a GOBIN ($GOPATH/bin) set up and included in your PATH: $ mkdir -p $GOPATH/bin $ export PATH=$GOPATH/bin:$PATH Then follow the installation instructions on Github . To update the vendor directory using dep after changing Gopkg.toml, make sure DAOS is cloned into \"$GOPATH/src/github.com/daos-stack/daos\" Then: $ cd $GOPATH/src/github.com/daos-stack/daos/src/control $ dep ensure","title":"Go dependencies"},{"location":"admin/installation/#protobuf-compiler","text":"The DAOS control plane infrastructure uses protobuf as the data serialization format for its RPC requests. The DAOS proto files use protobuf 3 syntax, which is not supported by the platform protobuf compiler in all cases. Not all developers will need to build the proto files into the various source files. However, if changes are made to the proto files, the corresponding C and Go source files will need to be regenerated with a protobuf 3.* or higher compiler. The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work but are not guaranteed. Protocol Buffers v3.5.1. Installation instructions . Protobuf-C v1.3.1. Installation instructions . gRPC plugin: protoc-gen-go v1.2.0. Must match the proto version in src/control/Gopkg.toml. Install the specific version using GIT_TAG instructions here . Generate the Go file using the gRPC plugin. You can designate the directory location: $ protoc myfile.proto --go_out=plugins=grpc:<go_file_dir> Generate the C files using Protobuf-C. As the header and source files in DAOS are typically kept in separate locations, you will need to move them manually to their destination directories: $ protoc-c myfile.proto --c_out=. $ mv myfile.pb-c.h <c_file_include_dir> $ mv myfile.pb-c.c <c_file_src_dir>","title":"Protobuf Compiler"},{"location":"admin/intro/","text":"Introduction \u00b6 The Distributed Asynchronous Object Storage (DAOS) is an open-source object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next-generation NVM technology, like Storage Class Memory (SCM) and NVM express (NVMe), while presenting a key-value storage interface on top of commodity hardware that provides features, such as, transactional non-blocking I/O, advanced data protection with self-healing, end-to-end data integrity, fine-grained data control, and elastic storage, to optimize performance and cost. This administration guide version is associated with DAOS v0.9. Additional Documentation \u00b6 Refer to the following documentation for architecture and description: Document Location DAOS Internals https://github.com/daos-stack/daos/blob/master/src/README.md DAOS Storage Model https://github.com/daos-stack/daos/blob/master/doc/storage_model.md Community Roadmap https://wiki.hpdd.intel.com/display/DC/Roadmap https://grpc.io/ \u21a9","title":"Introduction"},{"location":"admin/intro/#introduction","text":"The Distributed Asynchronous Object Storage (DAOS) is an open-source object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next-generation NVM technology, like Storage Class Memory (SCM) and NVM express (NVMe), while presenting a key-value storage interface on top of commodity hardware that provides features, such as, transactional non-blocking I/O, advanced data protection with self-healing, end-to-end data integrity, fine-grained data control, and elastic storage, to optimize performance and cost. This administration guide version is associated with DAOS v0.9.","title":"Introduction"},{"location":"admin/intro/#additional-documentation","text":"Refer to the following documentation for architecture and description: Document Location DAOS Internals https://github.com/daos-stack/daos/blob/master/src/README.md DAOS Storage Model https://github.com/daos-stack/daos/blob/master/doc/storage_model.md Community Roadmap https://wiki.hpdd.intel.com/display/DC/Roadmap https://grpc.io/ \u21a9","title":"Additional Documentation"},{"location":"admin/performance_tuning/","text":"DAOS Performance Tuning \u00b6 This section will be expanded in a future revision. Network Performance \u00b6 Similar to the Lustre Network stack, the DAOS CART layer can validate and benchmark network communications in the same context as an application and using the same networks/tuning options as regular DAOS. The CART self_test can run against the DAOS servers in a production environment in a non-destructive manner. CART self_test supports different message sizes, bulk transfers, multiple targets, and the following test scenarios: Selftest client to servers where self_test issues RPCs directly to a list of servers Cross-servers where self_test sends instructions to the different servers that will issue cross-server RPCs. This model supports a many to many communication model. Instructions to run CaRT self_test with test_group as the target server are as follows. git clone https://github.com/daos-stack/daos.git cd daos git submodule init git submodule update scons --build-deps=yes install cd install/TESTING Prepare srvhostfile and clihostfile - srvhostfile contains list of nodes from which servers will launch - clihostfile contains node from which self_test will launch The example below uses Ethernet interface and Sockets provider. In the self_test commands: - (client-to-servers) Replace the argument for \"--endpoint\" accordingly. - (cross-servers) Replace the argument for \"--endpoint\" and \"--master-endpoint\" accordingly. - For example, if you have 8 servers, you would specify \"--endpoint 0-7:0\" (and --master-endpoint 0-7:0) The commands below will run self_test benchmark using the following message sizes: b1048576 1Mb bulk transfer Get and Put b1048576 0 1Mb bulk transfer Get only 0 b1048576 1Mb bulk transfer Put only I2048 2Kb iovec Input and Output i2048 0 2Kb iovec Input only 0 i2048 2Kb iovec Output only For full description of self_test usage, run: ../bin/self_test --help To start test_group server: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile srvhostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/test_group_srv.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 -x CRT_CTX_NUM=16 ../bin/crt_launch -e tests/test_group_np_srv --name self_test_srv_grp --cfg_path=. & To run self_test in client-to-servers mode: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile clihostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/self_test.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 -x CRT_CTX_NUM=16 ../bin/self_test --group-name self_test_srv_grp --endpoint 0- :0 --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" --max-inflight-rpcs 16 --repetitions 100 -t -n -p . To run self_test in cross-servers mode: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile clihostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/self_test.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 -x CRT_CTX_NUM=16 ../bin/self_test --group-name self_test_srv_grp --endpoint 0- :0 --master-endpoint 0- :0 --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" --max-inflight-rpcs 16 --repetitions 100 -t -n -p . To shutdown test_group server: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile clihostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/test_group_cli.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 tests/test_group_np_cli --name client-group --attach_to self_test_srv_grp --shut_only --cfg_path=. Benchmarking DAOS \u00b6 DAOS can be benchmarked with both IOR and mdtest through the following backends: native MPI-IO plugin combined with the ROMIO DAOS ADIO driver native HDF5 plugin combined with the HDF5 DAOS connector (under development) native POSIX plugin over dfuse and interception library (under development) a custom DFS plugin integrating mdtest & IOR directly with libfs without requiring FUSE or an interception library a custom DAOS plugin integrating IOR directly with the native DAOS array API.","title":"Performance Tuning"},{"location":"admin/performance_tuning/#daos-performance-tuning","text":"This section will be expanded in a future revision.","title":"DAOS Performance Tuning"},{"location":"admin/performance_tuning/#network-performance","text":"Similar to the Lustre Network stack, the DAOS CART layer can validate and benchmark network communications in the same context as an application and using the same networks/tuning options as regular DAOS. The CART self_test can run against the DAOS servers in a production environment in a non-destructive manner. CART self_test supports different message sizes, bulk transfers, multiple targets, and the following test scenarios: Selftest client to servers where self_test issues RPCs directly to a list of servers Cross-servers where self_test sends instructions to the different servers that will issue cross-server RPCs. This model supports a many to many communication model. Instructions to run CaRT self_test with test_group as the target server are as follows. git clone https://github.com/daos-stack/daos.git cd daos git submodule init git submodule update scons --build-deps=yes install cd install/TESTING Prepare srvhostfile and clihostfile - srvhostfile contains list of nodes from which servers will launch - clihostfile contains node from which self_test will launch The example below uses Ethernet interface and Sockets provider. In the self_test commands: - (client-to-servers) Replace the argument for \"--endpoint\" accordingly. - (cross-servers) Replace the argument for \"--endpoint\" and \"--master-endpoint\" accordingly. - For example, if you have 8 servers, you would specify \"--endpoint 0-7:0\" (and --master-endpoint 0-7:0) The commands below will run self_test benchmark using the following message sizes: b1048576 1Mb bulk transfer Get and Put b1048576 0 1Mb bulk transfer Get only 0 b1048576 1Mb bulk transfer Put only I2048 2Kb iovec Input and Output i2048 0 2Kb iovec Input only 0 i2048 2Kb iovec Output only For full description of self_test usage, run: ../bin/self_test --help To start test_group server: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile srvhostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/test_group_srv.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 -x CRT_CTX_NUM=16 ../bin/crt_launch -e tests/test_group_np_srv --name self_test_srv_grp --cfg_path=. & To run self_test in client-to-servers mode: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile clihostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/self_test.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 -x CRT_CTX_NUM=16 ../bin/self_test --group-name self_test_srv_grp --endpoint 0- :0 --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" --max-inflight-rpcs 16 --repetitions 100 -t -n -p . To run self_test in cross-servers mode: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile clihostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/self_test.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 -x CRT_CTX_NUM=16 ../bin/self_test --group-name self_test_srv_grp --endpoint 0- :0 --master-endpoint 0- :0 --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" --max-inflight-rpcs 16 --repetitions 100 -t -n -p . To shutdown test_group server: /usr/lib64/openmpi3/bin/orterun --mca btl self,tcp -N 1 --hostfile clihostfile --output-filename testLogs/ -x D_LOG_FILE=testLogs/test_group_cli.log -x D_LOG_FILE_APPEND_PID=1 -x D_LOG_MASK=WARN -x CRT_PHY_ADDR_STR=ofi+sockets -x OFI_INTERFACE=eth0 -x CRT_CTX_SHARE_ADDR=0 tests/test_group_np_cli --name client-group --attach_to self_test_srv_grp --shut_only --cfg_path=.","title":"Network Performance"},{"location":"admin/performance_tuning/#benchmarking-daos","text":"DAOS can be benchmarked with both IOR and mdtest through the following backends: native MPI-IO plugin combined with the ROMIO DAOS ADIO driver native HDF5 plugin combined with the HDF5 DAOS connector (under development) native POSIX plugin over dfuse and interception library (under development) a custom DFS plugin integrating mdtest & IOR directly with libfs without requiring FUSE or an interception library a custom DAOS plugin integrating IOR directly with the native DAOS array API.","title":"Benchmarking DAOS"},{"location":"admin/pool_operations/","text":"Pool Operations \u00b6 A DAOS pool is a storage reservation that can span any storage nodes and is managed by the administrator. The amount of space allocated to a pool is decided at creation time and can eventually be expanded through the management interface. Pool Creation/Destroy \u00b6 A DAOS pool can be created and destroyed through the DAOS management API (see daos_mgmt.h). DAOS also provides a utility called dmg to manage storage pools from the command line. To create a pool: $ dmg pool create --scm-size=xxG --nvme-size=yyT This command creates a pool distributed across the DAOS servers with a target size on each server with xxGB of SCM and yyTB of NVMe storage. The UUID allocated to the newly created pool is printed to stdout (referred as ${puuid}) as well as the rank where the pool service is located (referred as ${svcl}). $ dmg pool create --help ... [create command options] -g, --group= DAOS pool to be owned by given group, format name@domain -u, --user= DAOS pool to be owned by given user, format name@domain -a, --acl-file= Access Control List file path for DAOS pool -s, --scm-size= Size of SCM component of DAOS pool -n, --nvme-size= Size of NVMe component of DAOS pool -r, --ranks= Storage server unique identifiers (ranks) for DAOS pool -v, --nsvc= Number of pool service replicas (default: 1) -S, --sys= DAOS system that pool is to be a part of (default: daos_server) The typical output of this command is as follows: $ dmg -i pool create -s 1G -n 10G -g root -u root -S daos Active connections: [localhost:10001] Creating DAOS pool with 1GB SCM and 10GB NvMe storage (0.100 ratio) Pool-create command SUCCEEDED: UUID: 5d6fa7bf-637f-4dba-bcd2-480ad251cdc7, Service replicas: 0,1 This created a pool with UUID 5d6fa7bf-637f-4dba-bcd2-480ad251cdc7, two pool service replica on rank 0 and 1. To destroy a pool: $ dmg pool destroy --pool=${puuid} To see a list of the pools in your DAOS system: $ dmg system list-pools This will return a table of pool UUIDs and the ranks of their pool service replicas. For example: $ dmg system list-pools localhost:10001: connected Pool UUID Svc Replicas --------- ------------ 2a8ec3b2-729b-4617-bf51-77f37f764194 0,1 a106d667-5c5d-4d6f-ac3a-89099196c41a 0 85141a07-e3ba-42a6-81c2-3f18253c5e47 0 Pool Properties \u00b6 At creation time, a list of pool properties can be specified through the API (not supported by the tool yet): DAOS_PROP_CO_LABEL is a string that the administrator can associate with a pool. e.g., project A, project B, IO500 test pool DAOS_PROP_PO_ACL is the access control list (ACL) associated with the pool DAOS_PROP_PO_SPACE_RB is the space to be reserved on each target for rebuild purpose. DAOS_PROP_PO_SELF_HEAL defines whether the pool wants automatically-trigger, or manually-triggered self-healing. DAOS_PROP_PO_RECLAIM is used to tune the space reclaim strategy based on time interval, batched commits or snapshot creation. While those pool properties are currently stored persistently with pool metadata, many of them are still under development. Moreover, the ability to modify some of those properties on an existing pool will also be provided in a future release. Pool Access Control Lists \u00b6 User and group access for pools is controlled by Access Control Lists (ACLs). A DAOS ACL is a list of zero or more Access Control Entries (ACEs). ACEs are the individual rules applied to each access decision. If no ACL is provided when creating the pool, the default ACL grants read and write access to the pool's owner-user and owner-group. Access Control Entries \u00b6 ACEs are designated by a colon-separated string format: TYPE:FLAGS:IDENTITY:PERMISSIONS Available values for these fields: TYPE: Allow (A) FLAGS: Group (G) IDENTITY: See below PERMISSIONS: Read (r), Write (w) Identity \u00b6 The identity (also called the principal) is specified in the name@domain format. The domain should be left off if the name is a user/group on the local domain. Currently, this is the only case supported by DAOS. There are three special identities, OWNER@ , GROUP@ and EVERYONE@ , which align with User, Group, and Other from traditional POSIX permission bits. When providing them in the ACE string format, they must be spelled exactly as written here, in uppercase with no domain appended. Examples \u00b6 A::daos_user@:rw Allow the UNIX user named daos_user to have read-write access A:G:project_users@:r Allow anyone in the UNIX group project_users to have read-only access A::EVERYONE@:r Allow any user not covered by other rules to have read-only access Enforcement \u00b6 Access Control Entries (ACEs) will be enforced in the following order: Owner-User Named users Owner-Group and named groups Everyone In general, enforcement will be based on the first match, ignoring lower-priority entries. For example, if the user has an ACE for their user identity, they will not receive the permissions for any of their groups, even if those group entries have broader permissions than the user entry does. The user is expected to match at most one user entry. If no matching user entry is found, but entries match one or more of the user's groups, enforcement will be based on the union of the permissions of all matching groups. By default, if a user matches no ACEs in the list, access will be denied. Limitations \u00b6 The maximum length of the ACE list in a DAOS ACL structure is 64KiB. To calculate the actual length of an ACL, use the following formula for each ACE: The base size of an ACE is 256B. If the ACE principal is not one of the special principals: Add the length of the identity string + 1. If that value is not 64B aligned, round up to the nearest 64B boundary. Creating a pool with a custom ACL \u00b6 To create a pool with a custom ACL: $ dmg pool create --scm-size <size> --acl-file <path> The ACL file is expected to be a text file with one ACE listed on each line. For example: # Entries: A::OWNER@:rw A:G:GROUP@:rw # Everyone should be allowed to read A::EVERYONE@:r You may add comments to the ACL file by starting the line with # . Displaying a pool's ACL \u00b6 To view a pool's ACL: $ dmg pool get-acl --pool <UUID> The output is in the same string format used in the ACL file during creation, with one ACE per line. Modifying a pool's ACL \u00b6 For all of these commands using an ACL file, the ACL file must be in the format noted above for pool creation. Overwriting the ACL \u00b6 To replace a pool's ACL with a new ACL: $ dmg pool overwrite-acl --pool <UUID> --acl-file <path> Updating entries in an existing ACL \u00b6 To add or update multiple entries in an existing pool ACL: $ dmg pool update-acl --pool <UUID> --acl-file <path> To add or update a single entry in an existing pool ACL: $ dmg pool update-acl --pool <UUID> --entry <ACE> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one. Removing an entry from the ACL \u00b6 To delete an entry for a given principal, or identity, in an existing pool ACL: $ dmg pool delete-acl --pool <UUID> --principal <principal> The principal corresponds to the principal/identity portion of an ACE that was set during pool creation or a previous pool ACL operation. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ GROUP@ EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the pool will be decided based on the remaining ACL rules. Pool Query \u00b6 The pool query operation retrieves information (i.e., the number of targets, space usage, rebuild status, property list, and more) about a created pool. It is integrated into the dmg_old utility. To query a pool: $ dmg pool query --pool <UUID> Below is the output for a pool created with SCM space only. pool=47293abe-aa6f-4147-97f6-42a9f796d64a Pool 47293abe-aa6f-4147-97f6-42a9f796d64a, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 30064771072 Free: 30044570496, min:530139584, max:536869696, mean:536510187 - NVMe: Total size: 0 Free: 0, min:0, max:0, mean:0 Rebuild done, 10 objs, 1026 recs The total and free sizes are the sum across all the targets whereas min/max/mean gives information about individual targets. A min value close to 0 means that one target is running out of space. The example below shows a rebuild in progress and NVMe space allocated. pool=95886b8b-7eb8-454d-845c-fc0ae0ba5671 Pool 95886b8b-7eb8-454d-845c-fc0ae0ba5671, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 30064771072 Free: 29885237632, min:493096384, max:536869696, mean:533664957 - NVMe: Total size: 60129542144 Free: 29885237632, min:493096384, max:536869696, mean:533664957 Rebuild busy, 75 objs, 9722 recs Additional status and telemetry data are planned to be exported through the management API and tool and will be documented here once available. Pool Modifications \u00b6 Target Exclusion and Self-Healing \u00b6 To exclude a target from a pool: $ dmg_old exclude --svc=${svcl} --pool=${puuid} --target=${rank} Pool Extension \u00b6 Target Addition & Space Rebalancing \u00b6 Support for online target addition and automatic space rebalancing is planned for DAOS v1.4 and will be documented here once available. Pool Shard Resize \u00b6 Support for quiescent pool shard resize is currently not supported and is under consideration. Pool Catastrophic Recovery \u00b6 A DAOS pool is instantiated on each target by a set of pmemobj files managed by PMDK and SPDK blobs on SSDs. Tools to verify and repair this persistent data is scheduled for DAOS v2.4 and will be documented here once available. Meanwhile, PMDK provides a recovery tool (i.e., pmempool check) to verify and possibly repair a pmemobj file. As discussed in the previous section, the rebuild status can be consulted via the pool query and will be expanded with more information.","title":"Pool Operations"},{"location":"admin/pool_operations/#pool-operations","text":"A DAOS pool is a storage reservation that can span any storage nodes and is managed by the administrator. The amount of space allocated to a pool is decided at creation time and can eventually be expanded through the management interface.","title":"Pool Operations"},{"location":"admin/pool_operations/#pool-creationdestroy","text":"A DAOS pool can be created and destroyed through the DAOS management API (see daos_mgmt.h). DAOS also provides a utility called dmg to manage storage pools from the command line. To create a pool: $ dmg pool create --scm-size=xxG --nvme-size=yyT This command creates a pool distributed across the DAOS servers with a target size on each server with xxGB of SCM and yyTB of NVMe storage. The UUID allocated to the newly created pool is printed to stdout (referred as ${puuid}) as well as the rank where the pool service is located (referred as ${svcl}). $ dmg pool create --help ... [create command options] -g, --group= DAOS pool to be owned by given group, format name@domain -u, --user= DAOS pool to be owned by given user, format name@domain -a, --acl-file= Access Control List file path for DAOS pool -s, --scm-size= Size of SCM component of DAOS pool -n, --nvme-size= Size of NVMe component of DAOS pool -r, --ranks= Storage server unique identifiers (ranks) for DAOS pool -v, --nsvc= Number of pool service replicas (default: 1) -S, --sys= DAOS system that pool is to be a part of (default: daos_server) The typical output of this command is as follows: $ dmg -i pool create -s 1G -n 10G -g root -u root -S daos Active connections: [localhost:10001] Creating DAOS pool with 1GB SCM and 10GB NvMe storage (0.100 ratio) Pool-create command SUCCEEDED: UUID: 5d6fa7bf-637f-4dba-bcd2-480ad251cdc7, Service replicas: 0,1 This created a pool with UUID 5d6fa7bf-637f-4dba-bcd2-480ad251cdc7, two pool service replica on rank 0 and 1. To destroy a pool: $ dmg pool destroy --pool=${puuid} To see a list of the pools in your DAOS system: $ dmg system list-pools This will return a table of pool UUIDs and the ranks of their pool service replicas. For example: $ dmg system list-pools localhost:10001: connected Pool UUID Svc Replicas --------- ------------ 2a8ec3b2-729b-4617-bf51-77f37f764194 0,1 a106d667-5c5d-4d6f-ac3a-89099196c41a 0 85141a07-e3ba-42a6-81c2-3f18253c5e47 0","title":"Pool Creation/Destroy"},{"location":"admin/pool_operations/#pool-properties","text":"At creation time, a list of pool properties can be specified through the API (not supported by the tool yet): DAOS_PROP_CO_LABEL is a string that the administrator can associate with a pool. e.g., project A, project B, IO500 test pool DAOS_PROP_PO_ACL is the access control list (ACL) associated with the pool DAOS_PROP_PO_SPACE_RB is the space to be reserved on each target for rebuild purpose. DAOS_PROP_PO_SELF_HEAL defines whether the pool wants automatically-trigger, or manually-triggered self-healing. DAOS_PROP_PO_RECLAIM is used to tune the space reclaim strategy based on time interval, batched commits or snapshot creation. While those pool properties are currently stored persistently with pool metadata, many of them are still under development. Moreover, the ability to modify some of those properties on an existing pool will also be provided in a future release.","title":"Pool Properties"},{"location":"admin/pool_operations/#pool-access-control-lists","text":"User and group access for pools is controlled by Access Control Lists (ACLs). A DAOS ACL is a list of zero or more Access Control Entries (ACEs). ACEs are the individual rules applied to each access decision. If no ACL is provided when creating the pool, the default ACL grants read and write access to the pool's owner-user and owner-group.","title":"Pool Access Control Lists"},{"location":"admin/pool_operations/#access-control-entries","text":"ACEs are designated by a colon-separated string format: TYPE:FLAGS:IDENTITY:PERMISSIONS Available values for these fields: TYPE: Allow (A) FLAGS: Group (G) IDENTITY: See below PERMISSIONS: Read (r), Write (w)","title":"Access Control Entries"},{"location":"admin/pool_operations/#identity","text":"The identity (also called the principal) is specified in the name@domain format. The domain should be left off if the name is a user/group on the local domain. Currently, this is the only case supported by DAOS. There are three special identities, OWNER@ , GROUP@ and EVERYONE@ , which align with User, Group, and Other from traditional POSIX permission bits. When providing them in the ACE string format, they must be spelled exactly as written here, in uppercase with no domain appended.","title":"Identity"},{"location":"admin/pool_operations/#examples","text":"A::daos_user@:rw Allow the UNIX user named daos_user to have read-write access A:G:project_users@:r Allow anyone in the UNIX group project_users to have read-only access A::EVERYONE@:r Allow any user not covered by other rules to have read-only access","title":"Examples"},{"location":"admin/pool_operations/#enforcement","text":"Access Control Entries (ACEs) will be enforced in the following order: Owner-User Named users Owner-Group and named groups Everyone In general, enforcement will be based on the first match, ignoring lower-priority entries. For example, if the user has an ACE for their user identity, they will not receive the permissions for any of their groups, even if those group entries have broader permissions than the user entry does. The user is expected to match at most one user entry. If no matching user entry is found, but entries match one or more of the user's groups, enforcement will be based on the union of the permissions of all matching groups. By default, if a user matches no ACEs in the list, access will be denied.","title":"Enforcement"},{"location":"admin/pool_operations/#limitations","text":"The maximum length of the ACE list in a DAOS ACL structure is 64KiB. To calculate the actual length of an ACL, use the following formula for each ACE: The base size of an ACE is 256B. If the ACE principal is not one of the special principals: Add the length of the identity string + 1. If that value is not 64B aligned, round up to the nearest 64B boundary.","title":"Limitations"},{"location":"admin/pool_operations/#creating-a-pool-with-a-custom-acl","text":"To create a pool with a custom ACL: $ dmg pool create --scm-size <size> --acl-file <path> The ACL file is expected to be a text file with one ACE listed on each line. For example: # Entries: A::OWNER@:rw A:G:GROUP@:rw # Everyone should be allowed to read A::EVERYONE@:r You may add comments to the ACL file by starting the line with # .","title":"Creating a pool with a custom ACL"},{"location":"admin/pool_operations/#displaying-a-pools-acl","text":"To view a pool's ACL: $ dmg pool get-acl --pool <UUID> The output is in the same string format used in the ACL file during creation, with one ACE per line.","title":"Displaying a pool's ACL"},{"location":"admin/pool_operations/#modifying-a-pools-acl","text":"For all of these commands using an ACL file, the ACL file must be in the format noted above for pool creation.","title":"Modifying a pool's ACL"},{"location":"admin/pool_operations/#overwriting-the-acl","text":"To replace a pool's ACL with a new ACL: $ dmg pool overwrite-acl --pool <UUID> --acl-file <path>","title":"Overwriting the ACL"},{"location":"admin/pool_operations/#updating-entries-in-an-existing-acl","text":"To add or update multiple entries in an existing pool ACL: $ dmg pool update-acl --pool <UUID> --acl-file <path> To add or update a single entry in an existing pool ACL: $ dmg pool update-acl --pool <UUID> --entry <ACE> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one.","title":"Updating entries in an existing ACL"},{"location":"admin/pool_operations/#removing-an-entry-from-the-acl","text":"To delete an entry for a given principal, or identity, in an existing pool ACL: $ dmg pool delete-acl --pool <UUID> --principal <principal> The principal corresponds to the principal/identity portion of an ACE that was set during pool creation or a previous pool ACL operation. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ GROUP@ EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the pool will be decided based on the remaining ACL rules.","title":"Removing an entry from the ACL"},{"location":"admin/pool_operations/#pool-query","text":"The pool query operation retrieves information (i.e., the number of targets, space usage, rebuild status, property list, and more) about a created pool. It is integrated into the dmg_old utility. To query a pool: $ dmg pool query --pool <UUID> Below is the output for a pool created with SCM space only. pool=47293abe-aa6f-4147-97f6-42a9f796d64a Pool 47293abe-aa6f-4147-97f6-42a9f796d64a, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 30064771072 Free: 30044570496, min:530139584, max:536869696, mean:536510187 - NVMe: Total size: 0 Free: 0, min:0, max:0, mean:0 Rebuild done, 10 objs, 1026 recs The total and free sizes are the sum across all the targets whereas min/max/mean gives information about individual targets. A min value close to 0 means that one target is running out of space. The example below shows a rebuild in progress and NVMe space allocated. pool=95886b8b-7eb8-454d-845c-fc0ae0ba5671 Pool 95886b8b-7eb8-454d-845c-fc0ae0ba5671, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 30064771072 Free: 29885237632, min:493096384, max:536869696, mean:533664957 - NVMe: Total size: 60129542144 Free: 29885237632, min:493096384, max:536869696, mean:533664957 Rebuild busy, 75 objs, 9722 recs Additional status and telemetry data are planned to be exported through the management API and tool and will be documented here once available.","title":"Pool Query"},{"location":"admin/pool_operations/#pool-modifications","text":"","title":"Pool Modifications"},{"location":"admin/pool_operations/#target-exclusion-and-self-healing","text":"To exclude a target from a pool: $ dmg_old exclude --svc=${svcl} --pool=${puuid} --target=${rank}","title":"Target Exclusion and Self-Healing"},{"location":"admin/pool_operations/#pool-extension","text":"","title":"Pool Extension"},{"location":"admin/pool_operations/#target-addition-space-rebalancing","text":"Support for online target addition and automatic space rebalancing is planned for DAOS v1.4 and will be documented here once available.","title":"Target Addition &amp; Space Rebalancing"},{"location":"admin/pool_operations/#pool-shard-resize","text":"Support for quiescent pool shard resize is currently not supported and is under consideration.","title":"Pool Shard Resize"},{"location":"admin/pool_operations/#pool-catastrophic-recovery","text":"A DAOS pool is instantiated on each target by a set of pmemobj files managed by PMDK and SPDK blobs on SSDs. Tools to verify and repair this persistent data is scheduled for DAOS v2.4 and will be documented here once available. Meanwhile, PMDK provides a recovery tool (i.e., pmempool check) to verify and possibly repair a pmemobj file. As discussed in the previous section, the rebuild status can be consulted via the pool query and will be expanded with more information.","title":"Pool Catastrophic Recovery"},{"location":"admin/troubleshooting/","text":"Troubleshooting \u00b6 DAOS Errors \u00b6 DAOS error numbering starts at 1000. The most common errors are documented in the table below. DAOS Error Value Description DER_NO_PERM 1001 No permission DER_NO_HDL 1002 Invalid handle DER_INVAL 1003 Invalid parameters DER_NOSPACE 1007 No space left on storage target DER_NOSYS 1010 Function not implemented DER_IO 2001 Generic I/O error DER_ENOENT 2003 Entry not found DER_KEY2BIG 2012 Key is too large DER_IO_INVAL 2014 IO buffers can't match object extents When an operation fails, DAOS returns a negative DER error. For a full list of errors, please check https://github.com/daos-stack/cart/blob/master/src/include/gurt/errno.h (DER_ERR_GURT_BASE is equal to 1000 and DER_ERR_DAOS_BASE is equal to 2000). The function d_errstr() is provided in the API to convert an error number to an error message. Log Files \u00b6 On the server side, there are three log files created as part of normal server operations: Component Config Parameter Example Config Value Control Plane control_log_file /tmp/daos_control.log Data Plane log_file /tmp/daos_server.log Privileged Helper helper_log_file /tmp/daos_admin.log On the agent side; the following log is created as part of normal operations: Component Config Parameter Example Config Value agent log_file /tmp/daos_agent.log Control Plane Log \u00b6 The default log level for the control plane is INFO. The following levels may be set using the control_log_mask config parameter: DEBUG INFO ERROR Data Plane Log \u00b6 Data Plane ( daos_io_server ) logging is configured on a per-instance basis. In other words, each section under the servers: section must have its own logging configuration. The log_file config parameter is converted to a D_LOG_FILE environment variable value. For more detail, please see the Debugging System section of this document. Privileged Helper Log \u00b6 By default, the privileged helper only emits ERROR-level logging which is captured by the control plane and included in that log. If the helper_log_file parameter is set in the server config, then DEBUG-level logging will be sent to the specified file. Daos Agent Log \u00b6 If the log_file config parameter is set in the agent config, then DEBUG-level logging will be sent to the specified file. Debugging System \u00b6 DAOS uses the debug system defined in CaRT but more specifically the GURT library. Log files for both client and server are written to \"/tmp/daos.log\" unless otherwise set by D_LOG_FILE. Registered Subsystems/Facilities \u00b6 The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging. By default all subsystems are enabled (\"DD_SUBSYS=all\"). DAOS Facilities: common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, bio, tests Common Facilities (GURT): MISC, MEM CaRT Facilities: RPC, BULK, CORPC, GRP, LM, HG, ST, IV Priority Logging \u00b6 All macros that output logs have a priority level, shown in descending order below. D_FATAL(fmt, ...) FATAL D_CRIT(fmt, ...) CRIT D_ERROR(fmt, ...) ERR D_WARN(fmt, ...) WARN D_NOTE(fmt, ...) NOTE D_INFO(fmt, ...) INFO D_DEBUG(mask, fmt, ...) DEBUG The priority level that outputs to stderr is set with DD_STDERR. By default in DAOS (specific to the project), this is set to CRIT (\"DD_STDERR=CRIT\") meaning that all CRIT and more severe log messages will dump to stderr. However, this is separate from the priority of logging to \"/tmp/daos.log\". The priority level of logging can be set with D_LOG_MASK, which by default is set to INFO (\"D_LOG_MASK=INFO\"), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well (\"D_LOG_MASK=DEBUG,MEM=ERR\"). Debug Masks/Streams: \u00b6 DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). To accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can be defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default (\"DD_MASK=all\"). DAOS Debug Masks: md = metadata operations pl = placement operations mgmt = pool management epc = epoch system df = durable format rebuild = rebuild process daos_default = (group mask) io, md, pl, and rebuild operations Common Debug Masks (GURT): any = generic messages, no classification trace = function trace, tree/hash/lru operations mem = memory operations net = network operations io = object I/Otest = test programs Common Use Cases \u00b6 Generic setup for all messages (default settings) $ D_LOG_MASK=DEBUG $ DD_SUBSYS=all $ DD_MASK=all Disable all logs for performance tuning $ D_LOG_MASK=ERR -> will only log error messages from all facilities $ D_LOG_MASK=FATAL -> will only log system fatal messages Disable a noisy debug logging subsystem $ D_LOG_MASK=DEBUG,MEM=ERR -> disables MEM facility by restricting all logs from that facility to ERROR or higher priority only Enable a subset of facilities of interest $ DD_SUBSYS=rpc,tests $ D_LOG_MASK=DEBUG -> required to see logs for RPC and TESTS less severe than INFO (the majority of log messages) Fine-tune the debug messages by setting a debug mask $ D_LOG_MASK=DEBUG $ DD_MASK=mgmt -> only logs DEBUG messages related to pool management Refer to the DAOS Environment Variables document for more information about the debug system environment. Common DAOS Problems \u00b6 This section to be updated in a future revision. Bug Report \u00b6 Bugs should be reported through our issue tracker 1 with a test case to reproduce the issue (when applicable) and debug logs. After creating a ticket, logs should be gathered from the locations described in the Log Files section of this document and attached to the ticket. To avoid problems with attaching large files, please attach the logs in a compressed container format, such as .zip or .tar.bz2. https://jira.hpdd.intel.com \u21a9","title":"Troubleshooting"},{"location":"admin/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"admin/troubleshooting/#daos-errors","text":"DAOS error numbering starts at 1000. The most common errors are documented in the table below. DAOS Error Value Description DER_NO_PERM 1001 No permission DER_NO_HDL 1002 Invalid handle DER_INVAL 1003 Invalid parameters DER_NOSPACE 1007 No space left on storage target DER_NOSYS 1010 Function not implemented DER_IO 2001 Generic I/O error DER_ENOENT 2003 Entry not found DER_KEY2BIG 2012 Key is too large DER_IO_INVAL 2014 IO buffers can't match object extents When an operation fails, DAOS returns a negative DER error. For a full list of errors, please check https://github.com/daos-stack/cart/blob/master/src/include/gurt/errno.h (DER_ERR_GURT_BASE is equal to 1000 and DER_ERR_DAOS_BASE is equal to 2000). The function d_errstr() is provided in the API to convert an error number to an error message.","title":"DAOS Errors"},{"location":"admin/troubleshooting/#log-files","text":"On the server side, there are three log files created as part of normal server operations: Component Config Parameter Example Config Value Control Plane control_log_file /tmp/daos_control.log Data Plane log_file /tmp/daos_server.log Privileged Helper helper_log_file /tmp/daos_admin.log On the agent side; the following log is created as part of normal operations: Component Config Parameter Example Config Value agent log_file /tmp/daos_agent.log","title":"Log Files"},{"location":"admin/troubleshooting/#control-plane-log","text":"The default log level for the control plane is INFO. The following levels may be set using the control_log_mask config parameter: DEBUG INFO ERROR","title":"Control Plane Log"},{"location":"admin/troubleshooting/#data-plane-log","text":"Data Plane ( daos_io_server ) logging is configured on a per-instance basis. In other words, each section under the servers: section must have its own logging configuration. The log_file config parameter is converted to a D_LOG_FILE environment variable value. For more detail, please see the Debugging System section of this document.","title":"Data Plane Log"},{"location":"admin/troubleshooting/#privileged-helper-log","text":"By default, the privileged helper only emits ERROR-level logging which is captured by the control plane and included in that log. If the helper_log_file parameter is set in the server config, then DEBUG-level logging will be sent to the specified file.","title":"Privileged Helper Log"},{"location":"admin/troubleshooting/#daos-agent-log","text":"If the log_file config parameter is set in the agent config, then DEBUG-level logging will be sent to the specified file.","title":"Daos Agent Log"},{"location":"admin/troubleshooting/#debugging-system","text":"DAOS uses the debug system defined in CaRT but more specifically the GURT library. Log files for both client and server are written to \"/tmp/daos.log\" unless otherwise set by D_LOG_FILE.","title":"Debugging System"},{"location":"admin/troubleshooting/#registered-subsystemsfacilities","text":"The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging. By default all subsystems are enabled (\"DD_SUBSYS=all\"). DAOS Facilities: common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, bio, tests Common Facilities (GURT): MISC, MEM CaRT Facilities: RPC, BULK, CORPC, GRP, LM, HG, ST, IV","title":"Registered Subsystems/Facilities"},{"location":"admin/troubleshooting/#priority-logging","text":"All macros that output logs have a priority level, shown in descending order below. D_FATAL(fmt, ...) FATAL D_CRIT(fmt, ...) CRIT D_ERROR(fmt, ...) ERR D_WARN(fmt, ...) WARN D_NOTE(fmt, ...) NOTE D_INFO(fmt, ...) INFO D_DEBUG(mask, fmt, ...) DEBUG The priority level that outputs to stderr is set with DD_STDERR. By default in DAOS (specific to the project), this is set to CRIT (\"DD_STDERR=CRIT\") meaning that all CRIT and more severe log messages will dump to stderr. However, this is separate from the priority of logging to \"/tmp/daos.log\". The priority level of logging can be set with D_LOG_MASK, which by default is set to INFO (\"D_LOG_MASK=INFO\"), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well (\"D_LOG_MASK=DEBUG,MEM=ERR\").","title":"Priority Logging"},{"location":"admin/troubleshooting/#debug-masksstreams","text":"DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). To accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can be defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default (\"DD_MASK=all\"). DAOS Debug Masks: md = metadata operations pl = placement operations mgmt = pool management epc = epoch system df = durable format rebuild = rebuild process daos_default = (group mask) io, md, pl, and rebuild operations Common Debug Masks (GURT): any = generic messages, no classification trace = function trace, tree/hash/lru operations mem = memory operations net = network operations io = object I/Otest = test programs","title":"Debug Masks/Streams:"},{"location":"admin/troubleshooting/#common-use-cases","text":"Generic setup for all messages (default settings) $ D_LOG_MASK=DEBUG $ DD_SUBSYS=all $ DD_MASK=all Disable all logs for performance tuning $ D_LOG_MASK=ERR -> will only log error messages from all facilities $ D_LOG_MASK=FATAL -> will only log system fatal messages Disable a noisy debug logging subsystem $ D_LOG_MASK=DEBUG,MEM=ERR -> disables MEM facility by restricting all logs from that facility to ERROR or higher priority only Enable a subset of facilities of interest $ DD_SUBSYS=rpc,tests $ D_LOG_MASK=DEBUG -> required to see logs for RPC and TESTS less severe than INFO (the majority of log messages) Fine-tune the debug messages by setting a debug mask $ D_LOG_MASK=DEBUG $ DD_MASK=mgmt -> only logs DEBUG messages related to pool management Refer to the DAOS Environment Variables document for more information about the debug system environment.","title":"Common Use Cases"},{"location":"admin/troubleshooting/#common-daos-problems","text":"This section to be updated in a future revision.","title":"Common DAOS Problems"},{"location":"admin/troubleshooting/#bug-report","text":"Bugs should be reported through our issue tracker 1 with a test case to reproduce the issue (when applicable) and debug logs. After creating a ticket, logs should be gathered from the locations described in the Log Files section of this document and attached to the ticket. To avoid problems with attaching large files, please attach the logs in a compressed container format, such as .zip or .tar.bz2. https://jira.hpdd.intel.com \u21a9","title":"Bug Report"},{"location":"admin/utilities_examples/","text":"Utilities & Usage Examples \u00b6 This section to be updated in a future revision.","title":"Utilities and Usage Examples"},{"location":"admin/utilities_examples/#utilities-usage-examples","text":"This section to be updated in a future revision.","title":"Utilities &amp; Usage Examples"},{"location":"dev/coding/","text":"DAOS Coding Rules \u00b6","title":"DAOS Coding Rules"},{"location":"dev/coding/#daos-coding-rules","text":"","title":"DAOS Coding Rules"},{"location":"dev/contributing/","text":"Contributing to DAOS \u00b6 Your contributions are most welcome! There are several good ways to suggest new features, offer to add a feature, or just begin a dialog about DAOS: Open an issue in jira Suggest a feature, ask a question, start a discussion, etc. in our user group Chat with members of the DAOS community real-time on Gitter Coding Rules \u00b6 Please check the coding conventions for code contribution. Commit Comments \u00b6 Commit Message Content \u00b6 Writing good commit comments is critical to ensuring that changes are easily understood, even years after they were originally written. The commit comment should contain enough information about the change to allow the reader to understand the motivation for the change, what parts of the code it is affecting, and any interesting, unusual, or complex parts of the change to draw attention to. The reason for a change may be manyfold: bug, enhancement, feature, code style, etc. so providing information about this sets the stage for understanding the change. If it is a bug, include information about what usage triggers the bug and how it manifests (error messages, assertion failure, etc.). If it is a feature, include information about what improvement is being made, and how it will affect usage. Providing some high-level information about the code path that is being modified is useful for the reader, since the files and patch fragments are not necessarily going to be listed in a sensible order in the patch. Including the important functions being modified provides a starting point for the reader to follow the logic of the change, and makes it easier to search for such changes in the future. If the patch is based on some earlier patch, then including the git commit hash of the original patch, Jira ticket number, etc. is useful for tracking the chain of dependencies. This can be very useful if a patch is landed separately to different maintenance branches, if it is fixing a problem in a previously landed patch, or if it is being imported from an upstream kernel commit. Having long commit comments that describe the change well is a good thing. The commit comments will be tightly associated with the code for a long time into the future, even many of the original commit comments from years earlier are still available through changes of the source code repository. In contrast, bug tracking systems come and go, and cannot be relied upon to track information about a change for extended periods of time. Commit Message Format \u00b6 Unlike the content of the commit message, the format is relatively easy to verify for correctness. Having the same standard format allows Git tools like git shortlog to extract information from the patches more easily. The first line of the commit comment is the commit summary of the change. Changes submitted to the DAOS master branch require a DAOS Jira ticket number at the beginning of the commit summary. A DAOS Jira ticket is one that begins with DAOS and is, therefore, part of the DAOS project within Jira. The commit summary should also have a component: tag immediately following the Jira ticket number that indicates to which DAOS subsystem the commit is related. Example DAOS subsystems relate to modules like client, pool, container, object, vos, rdb; functional components like rebuild; or auxiliary components like build, tests, doc. This subsystem list is not exhaustive but provides a good guideline for consistency. The commit summary line must be 62 characters or less, including the Jira ticket number and component tag, so that git shortlog and git format-patch can fit the summary onto a single line. The summary must be followed by a blank line. The rest of the comments should be wrapped to 70 columns or less. This allows for the first line to be used as a subject in emails, and also for the entire body to be displayed using tools like git log or git shortlog in an 80 column window. DAOS-nnn component: short description of change under 62 columns The \"component:\" should be a lower-case single-word subsystem of the DAOS code that best encompasses the change being made. Examples of components include modules: client, pool, container, object, vos, rdb, cart; functional subsystems: recovery; and auxiliary areas: build, tests, docs. This list is not exhaustive, but is a guideline. The commit comment should contain a detailed explanation of changes being made. This can be as long as you'd like. Please give details of what problem was solved (including error messages or problems that were seen), a good high-level description of how it was solved, and which parts of the code were changed (including important functions that were changed, if this is useful to understand the patch, and for easier searching). Wrap lines at/under 70 columns. Signed-off-by: Your Real Name <your_email@domain.name> The Signed-off-by: line \u00b6 The Signed-off-by: line asserts that you have permission to contribute the code to the project according to the Developer's Certificate of Origin. The -s option to git commit also adds the Signed-off-by: line automatically. Additional commit tags \u00b6 A number of additional commit tags can be used to further explain who has contributed to the patch, and for tracking purposes. These tags are commonly used with Linux kernel patches. These tags should appear before the Signed-off-by: tag. Acked-by: User Name <user@domain.com> Tested-by: User Name <user@domain.com> Reported-by: User Name <user@domain.com> Reviewed-by: User Name <user@domain.com> CC: User Name <user@domain.com> Pull Requests (PR) \u00b6 DAOS uses the common fork & merge workflow used by most GitHub-hosted projects. Please refer to the online GitHub documentation .","title":"Contributing"},{"location":"dev/contributing/#contributing-to-daos","text":"Your contributions are most welcome! There are several good ways to suggest new features, offer to add a feature, or just begin a dialog about DAOS: Open an issue in jira Suggest a feature, ask a question, start a discussion, etc. in our user group Chat with members of the DAOS community real-time on Gitter","title":"Contributing to DAOS"},{"location":"dev/contributing/#coding-rules","text":"Please check the coding conventions for code contribution.","title":"Coding Rules"},{"location":"dev/contributing/#commit-comments","text":"","title":"Commit Comments"},{"location":"dev/contributing/#commit-message-content","text":"Writing good commit comments is critical to ensuring that changes are easily understood, even years after they were originally written. The commit comment should contain enough information about the change to allow the reader to understand the motivation for the change, what parts of the code it is affecting, and any interesting, unusual, or complex parts of the change to draw attention to. The reason for a change may be manyfold: bug, enhancement, feature, code style, etc. so providing information about this sets the stage for understanding the change. If it is a bug, include information about what usage triggers the bug and how it manifests (error messages, assertion failure, etc.). If it is a feature, include information about what improvement is being made, and how it will affect usage. Providing some high-level information about the code path that is being modified is useful for the reader, since the files and patch fragments are not necessarily going to be listed in a sensible order in the patch. Including the important functions being modified provides a starting point for the reader to follow the logic of the change, and makes it easier to search for such changes in the future. If the patch is based on some earlier patch, then including the git commit hash of the original patch, Jira ticket number, etc. is useful for tracking the chain of dependencies. This can be very useful if a patch is landed separately to different maintenance branches, if it is fixing a problem in a previously landed patch, or if it is being imported from an upstream kernel commit. Having long commit comments that describe the change well is a good thing. The commit comments will be tightly associated with the code for a long time into the future, even many of the original commit comments from years earlier are still available through changes of the source code repository. In contrast, bug tracking systems come and go, and cannot be relied upon to track information about a change for extended periods of time.","title":"Commit Message Content"},{"location":"dev/contributing/#commit-message-format","text":"Unlike the content of the commit message, the format is relatively easy to verify for correctness. Having the same standard format allows Git tools like git shortlog to extract information from the patches more easily. The first line of the commit comment is the commit summary of the change. Changes submitted to the DAOS master branch require a DAOS Jira ticket number at the beginning of the commit summary. A DAOS Jira ticket is one that begins with DAOS and is, therefore, part of the DAOS project within Jira. The commit summary should also have a component: tag immediately following the Jira ticket number that indicates to which DAOS subsystem the commit is related. Example DAOS subsystems relate to modules like client, pool, container, object, vos, rdb; functional components like rebuild; or auxiliary components like build, tests, doc. This subsystem list is not exhaustive but provides a good guideline for consistency. The commit summary line must be 62 characters or less, including the Jira ticket number and component tag, so that git shortlog and git format-patch can fit the summary onto a single line. The summary must be followed by a blank line. The rest of the comments should be wrapped to 70 columns or less. This allows for the first line to be used as a subject in emails, and also for the entire body to be displayed using tools like git log or git shortlog in an 80 column window. DAOS-nnn component: short description of change under 62 columns The \"component:\" should be a lower-case single-word subsystem of the DAOS code that best encompasses the change being made. Examples of components include modules: client, pool, container, object, vos, rdb, cart; functional subsystems: recovery; and auxiliary areas: build, tests, docs. This list is not exhaustive, but is a guideline. The commit comment should contain a detailed explanation of changes being made. This can be as long as you'd like. Please give details of what problem was solved (including error messages or problems that were seen), a good high-level description of how it was solved, and which parts of the code were changed (including important functions that were changed, if this is useful to understand the patch, and for easier searching). Wrap lines at/under 70 columns. Signed-off-by: Your Real Name <your_email@domain.name>","title":"Commit Message Format"},{"location":"dev/contributing/#the-signed-off-by-line","text":"The Signed-off-by: line asserts that you have permission to contribute the code to the project according to the Developer's Certificate of Origin. The -s option to git commit also adds the Signed-off-by: line automatically.","title":"The Signed-off-by: line"},{"location":"dev/contributing/#additional-commit-tags","text":"A number of additional commit tags can be used to further explain who has contributed to the patch, and for tracking purposes. These tags are commonly used with Linux kernel patches. These tags should appear before the Signed-off-by: tag. Acked-by: User Name <user@domain.com> Tested-by: User Name <user@domain.com> Reported-by: User Name <user@domain.com> Reviewed-by: User Name <user@domain.com> CC: User Name <user@domain.com>","title":"Additional commit tags"},{"location":"dev/contributing/#pull-requests-pr","text":"DAOS uses the common fork & merge workflow used by most GitHub-hosted projects. Please refer to the online GitHub documentation .","title":"Pull Requests (PR)"},{"location":"dev/development/","text":"Development Environment \u00b6 This section covers specific instructions to create a developer-friendly environment to contribute to the DAOS development. This includes how to regenerate the protobuf files or add new Go package dependencies, which is only required for development purposes. Building DAOS for Development \u00b6 For development, it is recommended to build and install each dependency in a unique subdirectory. The DAOS build system supports this through the TARGET_PREFIX variable. Once the submodules have been initialized and updated, run the following commands: $ scons PREFIX=${daos_prefix_path} TARGET_PREFIX=${daos_prefix_path}/opt install --build-deps=yes --config=force Installing the components into seperate directories allow upgrading the components individually by replacing --build-deps=yes with --update-prereq={component_name}. This requires a change to the environment configuration from before. For automated environment setup, source utils/sl/utils/setup_local.sh. ARGOBOTS=${daos_prefix_path}/opt/argobots CART=${daos_prefix_path}/opt/cart FIO=${daos_prefix_path}/opt/fio FUSE=${daos_prefix_path}/opt/fuse ISAL=${daos_prefix_path}/opt/isal MERCURY=${daos_prefix_path}/opt/mercury OFI=${daos_prefix_path}/opt/ofi OPENPA=${daos_prefix_path}/opt/openpa PMDK=${daos_prefix_path}/opt/pmdk PROTOBUFC=${daos_prefix_path}/opt/protobufc SPDK=${daos_prefix_path}/opt/spdk PATH=$CART/bin/:${daos_prefix_path}/bin/:$PATH With this approach, DAOS would get built using the prebuilt dependencies in ${daos_prefix_path}/opt, and required options are saved for future compilations. So, after the first time, during development, only \"scons --config=force\" and \"scons --config=force install\" would suffice for compiling changes to DAOS source code. If you wish to compile DAOS with clang rather than gcc, set COMPILER=clang on the scons command line. This option is also saved for future com pilations. Go dependencies \u00b6 Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses Go Modules to manage these dependencies. As this feature is built in to Go distributions starting with version 1.11, no additional tools are needed to manage dependencies. Among other benefits, one of the major advantages of using Go Modules is that it removes the requirement for builds to be done within the $GOPATH, which simplifies our build system and other internal tooling. While it is possible to use Go Modules without checking a vendor directory into SCM, the DAOS project continues to use vendored dependencies in order to insulate our build system from transient network issues and other problems associated with nonvendored builds. The following is a short list of example workflows. For more details, please refer to one of the many resources available online. # add a new dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # update an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get -u github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # replace/remove an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/other/thing # make sure that github.com/other/thing is imported somewhere in the codebase, # and that github.com/awesome/thing is no longer imported $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod tidy $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. In all cases, after updating the vendor directory, it is a good idea to verify that your changes were applied as expected. In order to do this, a simple workflow is to clear the caches to force a clean build and then run the test script, which is vendor-aware and will not try to download missing modules: $ cd ~/daos/src/control # or wherever your daos clone lives $ go clean -modcache -cache $ ./run_go_tests.sh $ ls ~/go/pkg/mod # ~/go/pkg/mod should either not exist or be empty Protobuf Compiler \u00b6 The DAOS control plane infrastructure uses Protocol Buffers as the data serialization format for its RPC requests. Not all developers will need to compile the *.proto files, but if Protobuf changes are needed, the developer must regenerate the corresponding C and Go source files using a Protobuf compiler compatible with proto3 syntax. Recommended Versions \u00b6 The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work, but are not guaranteed. You may encounter installation errors when building from source relating to insufficient permissions. If that occurs, you may try relocating the repo to /var/tmp/ in order to build and install from there. Protocol Buffers v3.11.4. Installation instructions . Protobuf-C v1.3.3. Installation instructions . gRPC plugin: protoc-gen-go v1.3.4. Must match the proto version in src/control/go.mod. Install the specific version using GIT_TAG instructions here . Compiling Protobuf Files \u00b6 The source (.proto) files live under $DAOSREPO/src/proto. The preferred mechanism for generating compiled C/Go protobuf definitions is to use the Makefile in this directory. Care should be taken to keep the Makefile updated when source files are added or removed, or generated file destinations are updated. Note that the generated files are checked into SCM and are not generated as part of the normal DAOS build process. This allows developers to ensure that the generated files are correct after any changes to the source files are made. $ cd ~/daos/src/proto # or wherever your daos clone lives $ make protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ acl.proto protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ mgmt.proto ... $ git status ... # modified: ../control/common/proto/mgmt/acl.pb.go # modified: ../control/common/proto/mgmt/mgmt.pb.go ... After verifying that the generated C/Go files are correct, add and commit them as you would any other file.","title":"Dev Environment"},{"location":"dev/development/#development-environment","text":"This section covers specific instructions to create a developer-friendly environment to contribute to the DAOS development. This includes how to regenerate the protobuf files or add new Go package dependencies, which is only required for development purposes.","title":"Development Environment"},{"location":"dev/development/#building-daos-for-development","text":"For development, it is recommended to build and install each dependency in a unique subdirectory. The DAOS build system supports this through the TARGET_PREFIX variable. Once the submodules have been initialized and updated, run the following commands: $ scons PREFIX=${daos_prefix_path} TARGET_PREFIX=${daos_prefix_path}/opt install --build-deps=yes --config=force Installing the components into seperate directories allow upgrading the components individually by replacing --build-deps=yes with --update-prereq={component_name}. This requires a change to the environment configuration from before. For automated environment setup, source utils/sl/utils/setup_local.sh. ARGOBOTS=${daos_prefix_path}/opt/argobots CART=${daos_prefix_path}/opt/cart FIO=${daos_prefix_path}/opt/fio FUSE=${daos_prefix_path}/opt/fuse ISAL=${daos_prefix_path}/opt/isal MERCURY=${daos_prefix_path}/opt/mercury OFI=${daos_prefix_path}/opt/ofi OPENPA=${daos_prefix_path}/opt/openpa PMDK=${daos_prefix_path}/opt/pmdk PROTOBUFC=${daos_prefix_path}/opt/protobufc SPDK=${daos_prefix_path}/opt/spdk PATH=$CART/bin/:${daos_prefix_path}/bin/:$PATH With this approach, DAOS would get built using the prebuilt dependencies in ${daos_prefix_path}/opt, and required options are saved for future compilations. So, after the first time, during development, only \"scons --config=force\" and \"scons --config=force install\" would suffice for compiling changes to DAOS source code. If you wish to compile DAOS with clang rather than gcc, set COMPILER=clang on the scons command line. This option is also saved for future com pilations.","title":"Building DAOS for Development"},{"location":"dev/development/#go-dependencies","text":"Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses Go Modules to manage these dependencies. As this feature is built in to Go distributions starting with version 1.11, no additional tools are needed to manage dependencies. Among other benefits, one of the major advantages of using Go Modules is that it removes the requirement for builds to be done within the $GOPATH, which simplifies our build system and other internal tooling. While it is possible to use Go Modules without checking a vendor directory into SCM, the DAOS project continues to use vendored dependencies in order to insulate our build system from transient network issues and other problems associated with nonvendored builds. The following is a short list of example workflows. For more details, please refer to one of the many resources available online. # add a new dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # update an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get -u github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # replace/remove an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/other/thing # make sure that github.com/other/thing is imported somewhere in the codebase, # and that github.com/awesome/thing is no longer imported $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod tidy $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. In all cases, after updating the vendor directory, it is a good idea to verify that your changes were applied as expected. In order to do this, a simple workflow is to clear the caches to force a clean build and then run the test script, which is vendor-aware and will not try to download missing modules: $ cd ~/daos/src/control # or wherever your daos clone lives $ go clean -modcache -cache $ ./run_go_tests.sh $ ls ~/go/pkg/mod # ~/go/pkg/mod should either not exist or be empty","title":"Go dependencies"},{"location":"dev/development/#protobuf-compiler","text":"The DAOS control plane infrastructure uses Protocol Buffers as the data serialization format for its RPC requests. Not all developers will need to compile the *.proto files, but if Protobuf changes are needed, the developer must regenerate the corresponding C and Go source files using a Protobuf compiler compatible with proto3 syntax.","title":"Protobuf Compiler"},{"location":"dev/development/#recommended-versions","text":"The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work, but are not guaranteed. You may encounter installation errors when building from source relating to insufficient permissions. If that occurs, you may try relocating the repo to /var/tmp/ in order to build and install from there. Protocol Buffers v3.11.4. Installation instructions . Protobuf-C v1.3.3. Installation instructions . gRPC plugin: protoc-gen-go v1.3.4. Must match the proto version in src/control/go.mod. Install the specific version using GIT_TAG instructions here .","title":"Recommended Versions"},{"location":"dev/development/#compiling-protobuf-files","text":"The source (.proto) files live under $DAOSREPO/src/proto. The preferred mechanism for generating compiled C/Go protobuf definitions is to use the Makefile in this directory. Care should be taken to keep the Makefile updated when source files are added or removed, or generated file destinations are updated. Note that the generated files are checked into SCM and are not generated as part of the normal DAOS build process. This allows developers to ensure that the generated files are correct after any changes to the source files are made. $ cd ~/daos/src/proto # or wherever your daos clone lives $ make protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ acl.proto protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ mgmt.proto ... $ git status ... # modified: ../control/common/proto/mgmt/acl.pb.go # modified: ../control/common/proto/mgmt/mgmt.pb.go ... After verifying that the generated C/Go files are correct, add and commit them as you would any other file.","title":"Compiling Protobuf Files"},{"location":"licenses/ApacheLicense2/","text":"Apache License \u00b6 Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: You must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"ApacheLicense2"},{"location":"licenses/ApacheLicense2/#apache-license","text":"Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: You must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Apache License"},{"location":"licenses/Argobots_License/","text":"Argobots License \u00b6 Copyright \u00a9 2016, UChicago Argonne, LLC All Rights Reserved Argobots: A Lightweight Low-level Threading/Tasking Framework, SF-16-039 OPEN SOURCE LICENSE Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Software changes, modifications, or derivative works, should be noted with comments and the author and organization's name. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the names of UChicago Argonne, LLC or the Department of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. The software and the end-user documentation included with the redistribution, if any, must include the following acknowledgment: \"This product includes software produced by UChicago Argonne, LLC under Contract No. DE-AC02-06CH11357 with the Department of Energy.\" DISCLAIMER THE SOFTWARE IS SUPPLIED \"AS IS\" WITHOUT WARRANTY OF ANY KIND. NEITHER THE UNITED STATES GOVERNMENT, NOR THE UNITED STATES DEPARTMENT OF ENERGY, NOR UCHICAGO ARGONNE, LLC, NOR ANY OF THEIR EMPLOYEES, MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LEGAL LIABILITY OR RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR USEFULNESS OF ANY INFORMATION, DATA, APPARATUS, PRODUCT, OR PROCESS DISCLOSED, OR REPRESENTS THAT ITS USE WOULD NOT INFRINGE PRIVATELY OWNED RIGHTS.","title":"Argobots License"},{"location":"licenses/Argobots_License/#argobots-license","text":"Copyright \u00a9 2016, UChicago Argonne, LLC All Rights Reserved Argobots: A Lightweight Low-level Threading/Tasking Framework, SF-16-039 OPEN SOURCE LICENSE Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Software changes, modifications, or derivative works, should be noted with comments and the author and organization's name. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the names of UChicago Argonne, LLC or the Department of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. The software and the end-user documentation included with the redistribution, if any, must include the following acknowledgment: \"This product includes software produced by UChicago Argonne, LLC under Contract No. DE-AC02-06CH11357 with the Department of Energy.\" DISCLAIMER THE SOFTWARE IS SUPPLIED \"AS IS\" WITHOUT WARRANTY OF ANY KIND. NEITHER THE UNITED STATES GOVERNMENT, NOR THE UNITED STATES DEPARTMENT OF ENERGY, NOR UCHICAGO ARGONNE, LLC, NOR ANY OF THEIR EMPLOYEES, MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LEGAL LIABILITY OR RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR USEFULNESS OF ANY INFORMATION, DATA, APPARATUS, PRODUCT, OR PROCESS DISCLOSED, OR REPRESENTS THAT ITS USE WOULD NOT INFRINGE PRIVATELY OWNED RIGHTS.","title":"Argobots License"},{"location":"licenses/Argonne_DOE_License/","text":"Argonne National Laboratory, Department of Energy License \u00b6 Copyright (C) 2013-2019, Argonne National Laboratory, Department of Energy, UChicago Argonne, LLC and The HDF Group. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted for any purpose (including commercial purposes) provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or materials provided with the distribution. In addition, redistributions of modified forms of the source or binary code must carry prominent notices stating that the original code was changed and the date of the change. All publications or advertising materials mentioning features or use of this software are asked, but not required, to acknowledge that it was developed by ANL / the university of Chicago / The HDF Group and credit the contributors. Neither the name of ANL / the university of Chicago / The HDF Group, nor the name of any Contributor may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Argonne DOE License"},{"location":"licenses/Argonne_DOE_License/#argonne-national-laboratory-department-of-energy-license","text":"Copyright (C) 2013-2019, Argonne National Laboratory, Department of Energy, UChicago Argonne, LLC and The HDF Group. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted for any purpose (including commercial purposes) provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or materials provided with the distribution. In addition, redistributions of modified forms of the source or binary code must carry prominent notices stating that the original code was changed and the date of the change. All publications or advertising materials mentioning features or use of this software are asked, but not required, to acknowledge that it was developed by ANL / the university of Chicago / The HDF Group and credit the contributors. Neither the name of ANL / the university of Chicago / The HDF Group, nor the name of any Contributor may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Argonne National Laboratory, Department of Energy License"},{"location":"licenses/Argonne_OPA_License/","text":"Argon OPA License \u00b6 COPYRIGHT The following is a notice of limited availability of the code, and disclaimer which must be included in the prologue of the code and in all source listings of the code. Copyright Notice 2008 University of Chicago Permission is hereby granted to use, reproduce, prepare derivative works, and to redistribute to others. This software was authored by: Argonne National Laboratory Group Mathematics and Computer Science Division Argonne National Laboratory, Argonne IL 60439 Contact: devel@mpich.org GOVERNMENT LICENSE Portions of this material resulted from work developed under a U.S. Government Contract and are subject to the following license: the Government is granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable worldwide license in this computer software to reproduce, prepare derivative works, and perform publicly and display publicly. DISCLAIMER This computer code material was prepared, in part, as an account of work sponsored by an agency of the United States Government. Neither the United States, nor the University of Chicago, nor any of their employees, makes any warranty express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.","title":"Argonne OPA License"},{"location":"licenses/Argonne_OPA_License/#argon-opa-license","text":"COPYRIGHT The following is a notice of limited availability of the code, and disclaimer which must be included in the prologue of the code and in all source listings of the code. Copyright Notice 2008 University of Chicago Permission is hereby granted to use, reproduce, prepare derivative works, and to redistribute to others. This software was authored by: Argonne National Laboratory Group Mathematics and Computer Science Division Argonne National Laboratory, Argonne IL 60439 Contact: devel@mpich.org GOVERNMENT LICENSE Portions of this material resulted from work developed under a U.S. Government Contract and are subject to the following license: the Government is granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable worldwide license in this computer software to reproduce, prepare derivative works, and perform publicly and display publicly. DISCLAIMER This computer code material was prepared, in part, as an account of work sponsored by an agency of the United States Government. Neither the United States, nor the University of Chicago, nor any of their employees, makes any warranty express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.","title":"Argon OPA License"},{"location":"licenses/BSD_2_ClauseLicense/","text":"BSD Two Clause License \u00b6 Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 2 ClauseLicense"},{"location":"licenses/BSD_2_ClauseLicense/#bsd-two-clause-license","text":"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD Two Clause License"},{"location":"licenses/BSD_3_ClauseLicense/","text":"BSD 3-clause \"New\" or \"Revised\" License \u00b6 Copyright (c) 2020, Intel Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the ORGANIZATION nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3 ClauseLicense"},{"location":"licenses/BSD_3_ClauseLicense/#bsd-3-clause-new-or-revised-license","text":"Copyright (c) 2020, Intel Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the ORGANIZATION nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3-clause \"New\" or \"Revised\" License"},{"location":"licenses/Boost_License/","text":"Boost Software License - Version 1.0 \u00b6 August 17th, 2003 Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Boost License"},{"location":"licenses/Boost_License/#boost-software-license-version-10","text":"August 17th, 2003 Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Boost Software License - Version 1.0"},{"location":"licenses/CMake_License/","text":"C Make License \u00b6 Copyright (c) 2000-2003 Kitware, Inc., Insight Consortium. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Kitware nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. Modified source versions must be plainly marked as such, and must not be misrepresented as being the original software. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHORS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"CMake License"},{"location":"licenses/CMake_License/#c-make-license","text":"Copyright (c) 2000-2003 Kitware, Inc., Insight Consortium. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Kitware nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. Modified source versions must be plainly marked as such, and must not be misrepresented as being the original software. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHORS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"C Make License"},{"location":"licenses/CommonPublic_License/","text":"Common Public License Version 1.0 \u00b6 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS COMMON PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. DEFINITIONS \"Contribution\" means: a)in the case of the initial Contributor, the initial code and documentation distributed under this Agreement, and b)in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program;</ol> where such changes and/or additions to the Program originate from and are distributed by that particular Contributor. A Contribution 'originates' from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include additions to the Program which: (i) are separate modules of software distributed in conjunction with the Program under their own license agreement, and (ii) are not derivative works of the Program. \"Contributor\" means any person or entity that distributes the Program. \"Licensed Patents \" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement, including all Contributors. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, distribute and sublicense the Contribution of such Contributor, if any, and such derivative works, in source code and object code form. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in source code and object code form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. REQUIREMENTS A Contributor may choose to distribute the Program in object code form under its own license agreement, provided that: a) it complies with the terms and conditions of this Agreement; and b) its license agreement: i) effectively disclaims on behalf of all Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) states that any provisions which differ from this Agreement are offered by that Contributor alone and not by any other party; and iv) states that source code for the Program is available from such Contributor, and informs licensees how to obtain it in a reasonable manner on or through a medium customarily used for software exchange. When the Program is made available in source code form: a) it must be made available under this Agreement; and b) a copy of this Agreement must be included with each copy of the Program. Contributors may not remove or alter any copyright notices contained within the Program. Each Contributor must identify itself as the originator of its Contribution, if any, in a manner that reasonably allows subsequent Recipients to identify the originator of the Contribution. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against a Contributor with respect to a patent applicable to software (including a cross-claim or counterclaim in a lawsuit), then any patent licenses granted by that Contributor to such Recipient under this Agreement shall terminate as of the date such litigation is filed. In addition, if Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. IBM is the initial Agreement Steward. IBM may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. This Agreement is governed by the laws of the State of New York and the intellectual property laws of the United States of America. No party to this Agreement will bring a legal action under this Agreement more than one year after the cause of action arose. Each party waives its rights to a jury trial in any resulting litigation.","title":"CommonPublic License"},{"location":"licenses/CommonPublic_License/#common-public-license-version-10","text":"THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS COMMON PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. DEFINITIONS \"Contribution\" means: a)in the case of the initial Contributor, the initial code and documentation distributed under this Agreement, and b)in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program;</ol> where such changes and/or additions to the Program originate from and are distributed by that particular Contributor. A Contribution 'originates' from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include additions to the Program which: (i) are separate modules of software distributed in conjunction with the Program under their own license agreement, and (ii) are not derivative works of the Program. \"Contributor\" means any person or entity that distributes the Program. \"Licensed Patents \" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement, including all Contributors. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, distribute and sublicense the Contribution of such Contributor, if any, and such derivative works, in source code and object code form. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in source code and object code form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. REQUIREMENTS A Contributor may choose to distribute the Program in object code form under its own license agreement, provided that: a) it complies with the terms and conditions of this Agreement; and b) its license agreement: i) effectively disclaims on behalf of all Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) states that any provisions which differ from this Agreement are offered by that Contributor alone and not by any other party; and iv) states that source code for the Program is available from such Contributor, and informs licensees how to obtain it in a reasonable manner on or through a medium customarily used for software exchange. When the Program is made available in source code form: a) it must be made available under this Agreement; and b) a copy of this Agreement must be included with each copy of the Program. Contributors may not remove or alter any copyright notices contained within the Program. Each Contributor must identify itself as the originator of its Contribution, if any, in a manner that reasonably allows subsequent Recipients to identify the originator of the Contribution. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against a Contributor with respect to a patent applicable to software (including a cross-claim or counterclaim in a lawsuit), then any patent licenses granted by that Contributor to such Recipient under this Agreement shall terminate as of the date such litigation is filed. In addition, if Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. IBM is the initial Agreement Steward. IBM may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. This Agreement is governed by the laws of the State of New York and the intellectual property laws of the United States of America. No party to this Agreement will bring a legal action under this Agreement more than one year after the cause of action arose. Each party waives its rights to a jury trial in any resulting litigation.","title":"Common Public License Version 1.0"},{"location":"licenses/Eclipse_License/","text":"Eclipse Public License - v 1.0 \u00b6 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial code and documentation distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are distributed by that particular Contributor. A Contribution 'originates' from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include additions to the Program which: (i) are separate modules of software distributed in conjunction with the Program under their own license agreement, and (ii) are not derivative works of the Program. \"Contributor\" means any person or entity that distributes the Program. \"Licensed Patents \" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement, including all Contributors. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, distribute and sublicense the Contribution of such Contributor, if any, and such derivative works, in source code and object code form. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in source code and object code form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. REQUIREMENTS A Contributor may choose to distribute the Program in object code form under its own license agreement, provided that: a) it complies with the terms and conditions of this Agreement; and b) its license agreement: i) effectively disclaims on behalf of all Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) states that any provisions which differ from this Agreement are offered by that Contributor alone and not by any other party; and iv) states that source code for the Program is available from such Contributor, and informs licensees how to obtain it in a reasonable manner on or through a medium customarily used for software exchange. When the Program is made available in source code form: a) it must be made available under this Agreement; and b) a copy of this Agreement must be included with each copy of the Program. Contributors may not remove or alter any copyright notices contained within the Program. Each Contributor must identify itself as the originator of its Contribution, if any, in a manner that reasonably allows subsequent Recipients to identify the originator of the Contribution. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement , including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. This Agreement is governed by the laws of the State of New York and the intellectual property laws of the United States of America. No party to this Agreement will bring a legal action under this Agreement more than one year after the cause of action arose. Each party waives its rights to a jury trial in any resulting litigation.","title":"Eclipse License"},{"location":"licenses/Eclipse_License/#eclipse-public-license-v-10","text":"THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial code and documentation distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are distributed by that particular Contributor. A Contribution 'originates' from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include additions to the Program which: (i) are separate modules of software distributed in conjunction with the Program under their own license agreement, and (ii) are not derivative works of the Program. \"Contributor\" means any person or entity that distributes the Program. \"Licensed Patents \" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement, including all Contributors. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, distribute and sublicense the Contribution of such Contributor, if any, and such derivative works, in source code and object code form. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in source code and object code form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. REQUIREMENTS A Contributor may choose to distribute the Program in object code form under its own license agreement, provided that: a) it complies with the terms and conditions of this Agreement; and b) its license agreement: i) effectively disclaims on behalf of all Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) states that any provisions which differ from this Agreement are offered by that Contributor alone and not by any other party; and iv) states that source code for the Program is available from such Contributor, and informs licensees how to obtain it in a reasonable manner on or through a medium customarily used for software exchange. When the Program is made available in source code form: a) it must be made available under this Agreement; and b) a copy of this Agreement must be included with each copy of the Program. Contributors may not remove or alter any copyright notices contained within the Program. Each Contributor must identify itself as the originator of its Contribution, if any, in a manner that reasonably allows subsequent Recipients to identify the originator of the Contribution. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement , including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. This Agreement is governed by the laws of the State of New York and the intellectual property laws of the United States of America. No party to this Agreement will bring a legal action under this Agreement more than one year after the cause of action arose. Each party waives its rights to a jury trial in any resulting litigation.","title":"Eclipse Public License - v 1.0"},{"location":"licenses/GNUPublic_License/","text":"The GNU General Public License (GPL) \u00b6 Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS","title":"GNUPublic License"},{"location":"licenses/GNUPublic_License/#the-gnu-general-public-license-gpl","text":"Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS","title":"The GNU General Public License (GPL)"},{"location":"licenses/GNU_General_Public_License/","text":"The GNU General Public License (GPL) \u00b6 Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. one line to give the program's name and a brief idea of what it does. Copyright (C) This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Also add information on how to contact you by electronic and paper mail. If the program is interactive, make it output a short notice like this when it starts in an interactive mode: Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type show w'. This is free software, and you are welcome to redistribute it under certain conditions; type show c' for details. The hypothetical commands show w' and show c' should show the appropriate parts of the General Public License. Of course, the commands you use may be called something other than show w' and show c'; they could even be mouse-clicks or menu items--whatever suits your program. You should also get your employer (if you work as a programmer) or your school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. Here is a sample; alter the names: Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker. signature of Ty Coon, 1 April 1989 Ty Coon, President of Vice This General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Library General Public License instead of this License.","title":"GNU General Public License"},{"location":"licenses/GNU_General_Public_License/#the-gnu-general-public-license-gpl","text":"Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. one line to give the program's name and a brief idea of what it does. Copyright (C) This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Also add information on how to contact you by electronic and paper mail. If the program is interactive, make it output a short notice like this when it starts in an interactive mode: Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type show w'. This is free software, and you are welcome to redistribute it under certain conditions; type show c' for details. The hypothetical commands show w' and show c' should show the appropriate parts of the General Public License. Of course, the commands you use may be called something other than show w' and show c'; they could even be mouse-clicks or menu items--whatever suits your program. You should also get your employer (if you work as a programmer) or your school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. Here is a sample; alter the names: Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker. signature of Ty Coon, 1 April 1989 Ty Coon, President of Vice This General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Library General Public License instead of this License.","title":"The GNU General Public License (GPL)"},{"location":"licenses/GNU_Lesser_Public_License/","text":"GNU Lesser General Public License \u00b6 Version 2.1, February 1999 Copyright (C) 1991, 1999 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. [This is the first released version of the Lesser GPL. It also counts as the successor of the GNU Library Public License, version 2, hence the version number 2.1.] Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public Licenses are intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This license, the Lesser General Public License, applies to some specially designated software packages--typically libraries--of the Free Software Foundation and other authors who decide to use it. You can use it too, but we suggest you first think carefully about whether this license or the ordinary General Public License is the better strategy to use in any particular case, based on the explanations below. When we speak of free software, we are referring to freedom of use, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish); that you receive source code or can get it if you want it; that you can change the software and use pieces of it in new free programs; and that you are informed that you can do these things. To protect your rights, we need to make restrictions that forbid distributors to deny you these rights or to ask you to surrender these rights. These restrictions translate to certain responsibilities for you if you distribute copies of the library or if you modify it. For example, if you distribute copies of the library, whether gratis or for a fee, you must give the recipients all the rights that we gave you. You must make sure that they, too, receive or can get the source code. If you link other code with the library, you must provide complete object files to the recipients, so that they can relink them with the library after making changes to the library and recompiling it. And you must show them these terms so they know their rights. We protect your rights with a two-step method: (1) we copyright the library, and (2) we offer you this license, which gives you legal permission to copy, distribute and/or modify the library. To protect each distributor, we want to make it very clear that there is no warranty for the free library. Also, if the library is modified by someone else and passed on, the recipients should know that what they have is not the original version, so that the original author's reputation will not be affected by problems that might be introduced by others. Finally, software patents pose a constant threat to the existence of any free program. We wish to make sure that a company cannot effectively restrict the users of a free program by obtaining a restrictive license from a patent holder. Therefore, we insist that any patent license obtained for a version of the library must be consistent with the full freedom of use specified in this license. Most GNU software, including some libraries, is covered by the ordinary GNU General Public License. This license, the GNU Lesser General Public License, applies to certain designated libraries, and is quite different from the ordinary General Public License. We use this license for certain libraries in order to permit linking those libraries into non-free programs. When a program is linked with a library, whether statically or using a shared library, the combination of the two is legally speaking a combined work, a derivative of the original library. The ordinary General Public License therefore permits such linking only if the entire combination fits its criteria of freedom. The Lesser General Public License permits more lax criteria for linking other code with the library. We call this license the \"Lesser\" General Public License because it does Less to protect the user's freedom than the ordinary General Public License. It also provides other free software developers Less of an advantage over competing non-free programs. These disadvantages are the reason we use the ordinary General Public License for many libraries. However, the Lesser license provides advantages in certain special circumstances. For example, on rare occasions, there may be a special need to encourage the widest possible use of a certain library, so that it becomes a de-facto standard. To achieve this, non-free programs must be allowed to use the library. A more frequent case is that a free library does the same job as widely used non-free libraries. In this case, there is little to gain by limiting the free library to free software only, so we use the Lesser General Public License. In other cases, permission to use a particular library in non-free programs enables a greater number of people to use a large body of free software. For example, permission to use the GNU C Library in non-free programs enables many more people to use the whole GNU operating system, as well as its variant, the GNU/Linux operating system. Although the Lesser General Public License is Less protective of the users' freedom, it does ensure that the user of a program that is linked with the Library has the freedom and the wherewithal to run that program using a modified version of the Library. The precise terms and conditions for copying, distribution and modification follow. Pay close attention to the difference between a \"work based on the library\" and a \"work that uses the library\". The former contains code derived from the library, whereas the latter must be combined with the library in order to run. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION 0. This License Agreement applies to any software library or other program which contains a notice placed by the copyright holder or other authorized party saying it may be distributed under the terms of this Lesser General Public License (also called \"this License\"). Each licensee is addressed as \"you\". A \"library\" means a collection of software functions and/or data prepared so as to be conveniently linked with application programs (which use some of those functions and data) to form executables. The \"Library\", below, refers to any such software library or work which has been distributed under these terms. A \"work based on the Library\" means either the Library or any derivative work under copyright law: that is to say, a work containing the Library or a portion of it, either verbatim or with modifications and/or translated straightforwardly into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) \"Source code\" for a work means the preferred form of the work for making modifications to it. For a library, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the library. Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running a program using the Library is not restricted, and output from such a program is covered only if its contents constitute a work based on the Library (independent of the use of the Library in a tool for writing it). Whether that is true depends on what the Library does and what the program that uses the Library does. You may copy and distribute verbatim copies of the Library's complete source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and distribute a copy of this License along with the Library. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. You may modify your copy or copies of the Library or any portion of it, thus forming a work based on the Library, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) The modified work must itself be a software library. b) You must cause the files modified to carry prominent notices stating that you changed the files and the date of any change. c) You must cause the whole of the work to be licensed at no charge to all third parties under the terms of this License. d) If a facility in the modified Library refers to a function or a table of data to be supplied by an application program that uses the facility, other than as an argument passed when the facility is invoked, then you must make a good faith effort to ensure that, in the event an application does not supply such function or table, the facility still operates, and performs whatever part of its purpose remains meaningful. (For example, a function in a library to compute square roots has a purpose that is entirely well-defined independent of the application. Therefore, Subsection 2d requires that any application-supplied function or table used by this function must be optional: if the application does not supply it, the square root function must still compute square roots.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Library, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Library, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Library. In addition, mere aggregation of another work not based on the Library with the Library (or with a work based on the Library) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. You may opt to apply the terms of the ordinary GNU General Public License instead of this License to a given copy of the Library. To do this, you must alter all the notices that refer to this License, so that they refer to the ordinary GNU General Public License, version 2, instead of to this License. (If a newer version than version 2 of the ordinary GNU General Public License has appeared, then you can specify that version instead if you wish.) Do not make any other change in these notices. Once this change is made in a given copy, it is irreversible for that copy, so the ordinary GNU General Public License applies to all subsequent copies and derivative works made from that copy. This option is useful when you wish to copy part of the code of the Library into a program that is not a library. You may copy and distribute the Library (or a portion or derivative of it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange. If distribution of object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place satisfies the requirement to distribute the source code, even though third parties are not compelled to copy the source along with the object code. A program that contains no derivative of any portion of the Library, but is designed to work with the Library by being compiled or linked with it, is called a \"work that uses the Library\". Such a work, in isolation, is not a derivative work of the Library, and therefore falls outside the scope of this License. However, linking a \"work that uses the Library\" with the Library creates an executable that is a derivative of the Library (because it contains portions of the Library), rather than a \"work that uses the library\". The executable is therefore covered by this License. Section 6 states terms for distribution of such executables. When a \"work that uses the Library\" uses material from a header file that is part of the Library, the object code for the work may be a derivative work of the Library even though the source code is not. Whether this is true is especially significant if the work can be linked without the Library, or if the work is itself a library. The threshold for this to be true is not precisely defined by law. If such an object file uses only numerical parameters, data structure layouts and accessors, and small macros and small inline functions (ten lines or less in length), then the use of the object file is unrestricted, regardless of whether it is legally a derivative work. (Executables containing this object code plus portions of the Library will still fall under Section 6.) Otherwise, if the work is a derivative of the Library, you may distribute the object code for the work under the terms of Section 6. Any executables containing that work also fall under Section 6, whether or not they are linked directly with the Library itself. As an exception to the Sections above, you may also combine or link a \"work that uses the Library\" with the Library to produce a work containing portions of the Library, and distribute that work under terms of your choice, provided that the terms permit modification of the work for the customer's own use and reverse engineering for debugging such modifications. You must give prominent notice with each copy of the work that the Library is used in it and that the Library and its use are covered by this License. You must supply a copy of this License. If the work during execution displays copyright notices, you must include the copyright notice for the Library among them, as well as a reference directing the user to the copy of this License. Also, you must do one of these things: a) Accompany the work with the complete corresponding machine-readable source code for the Library including whatever changes were used in the work (which must be distributed under Sections 1 and 2 above); and, if the work is an executable linked with the Library, with the complete machine-readable \"work that uses the Library\", as object code and/or source code, so that the user can modify the Library and then relink to produce a modified executable containing the modified Library. (It is understood that the user who changes the contents of definitions files in the Library will not necessarily be able to recompile the application to use the modified definitions.) b) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (1) uses at run time a copy of the library already present on the user's computer system, rather than copying library functions into the executable, and (2) will operate properly with a modified version of the library, if the user installs one, as long as the modified version is interface-compatible with the version that the work was made with. c) Accompany the work with a written offer, valid for at least three years, to give the same user the materials specified in Subsection 6a, above, for a charge no more than the cost of performing this distribution. d) If distribution of the work is made by offering access to copy from a designated place, offer equivalent access to copy the above specified materials from the same place. e) Verify that the user has already received a copy of these materials or that you have already sent this user a copy. For an executable, the required form of the \"work that uses the Library\" must include any data and utility programs needed for reproducing the executable from it. However, as a special exception, the materials to be distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. It may happen that this requirement contradicts the license restrictions of other proprietary libraries that do not normally accompany the operating system. Such a contradiction means you cannot use both them and the Library together in an executable that you distribute. You may place library facilities that are a work based on the Library side-by-side in a single library together with other library facilities not covered by this License, and distribute such a combined library, provided that the separate distribution of the work based on the Library and of the other library facilities is otherwise permitted, and provided that you do these two things: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities. This must be distributed under the terms of the Sections above. b) Give prominent notice with the combined library of the fact that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work. You may not copy, modify, sublicense, link with, or distribute the Library except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, link with, or distribute the Library is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Library or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Library (or any work based on the Library), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Library or works based on it. Each time you redistribute the Library (or any work based on the Library), the recipient automatically receives a license from the original licensor to copy, distribute, link with or modify the Library subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties with this License. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Library at all. For example, if a patent license would not permit royalty-free redistribution of the Library by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Library. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply, and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. If the distribution and/or use of the Library is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Library under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. The Free Software Foundation may publish revised and/or new versions of the Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Library does not specify a license version number, you may choose any version ever published by the Free Software Foundation. If you wish to incorporate parts of the Library into other free programs whose distribution conditions are incompatible with these, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE LIBRARY IS WITH YOU. SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE LIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Libraries If you develop a new library, and you want it to be of the greatest possible use to the public, we recommend making it free software that everyone can redistribute and change. You can do so by permitting redistribution under these terms (or, alternatively, under the terms of the ordinary General Public License). To apply these terms, attach the following notices to the library. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. one line to give the library's name and an idea of what it does. Copyright (C) year name of author This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version. This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Also add information on how to contact you by electronic and paper mail. You should also get your employer (if you work as a programmer) or your school, if any, to sign a \"copyright disclaimer\" for the library, if necessary. Here is a sample; alter the names: Yoyodyne, Inc., hereby disclaims all copyright interest in the library `Frob' (a library for tweaking knobs) written by James Random Hacker. signature of Ty Coon, 1 April 1990 Ty Coon, President of Vice That's all there is to it!","title":"GNU Lesser Public License"},{"location":"licenses/GNU_Lesser_Public_License/#gnu-lesser-general-public-license","text":"Version 2.1, February 1999 Copyright (C) 1991, 1999 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. [This is the first released version of the Lesser GPL. It also counts as the successor of the GNU Library Public License, version 2, hence the version number 2.1.] Preamble The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public Licenses are intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This license, the Lesser General Public License, applies to some specially designated software packages--typically libraries--of the Free Software Foundation and other authors who decide to use it. You can use it too, but we suggest you first think carefully about whether this license or the ordinary General Public License is the better strategy to use in any particular case, based on the explanations below. When we speak of free software, we are referring to freedom of use, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish); that you receive source code or can get it if you want it; that you can change the software and use pieces of it in new free programs; and that you are informed that you can do these things. To protect your rights, we need to make restrictions that forbid distributors to deny you these rights or to ask you to surrender these rights. These restrictions translate to certain responsibilities for you if you distribute copies of the library or if you modify it. For example, if you distribute copies of the library, whether gratis or for a fee, you must give the recipients all the rights that we gave you. You must make sure that they, too, receive or can get the source code. If you link other code with the library, you must provide complete object files to the recipients, so that they can relink them with the library after making changes to the library and recompiling it. And you must show them these terms so they know their rights. We protect your rights with a two-step method: (1) we copyright the library, and (2) we offer you this license, which gives you legal permission to copy, distribute and/or modify the library. To protect each distributor, we want to make it very clear that there is no warranty for the free library. Also, if the library is modified by someone else and passed on, the recipients should know that what they have is not the original version, so that the original author's reputation will not be affected by problems that might be introduced by others. Finally, software patents pose a constant threat to the existence of any free program. We wish to make sure that a company cannot effectively restrict the users of a free program by obtaining a restrictive license from a patent holder. Therefore, we insist that any patent license obtained for a version of the library must be consistent with the full freedom of use specified in this license. Most GNU software, including some libraries, is covered by the ordinary GNU General Public License. This license, the GNU Lesser General Public License, applies to certain designated libraries, and is quite different from the ordinary General Public License. We use this license for certain libraries in order to permit linking those libraries into non-free programs. When a program is linked with a library, whether statically or using a shared library, the combination of the two is legally speaking a combined work, a derivative of the original library. The ordinary General Public License therefore permits such linking only if the entire combination fits its criteria of freedom. The Lesser General Public License permits more lax criteria for linking other code with the library. We call this license the \"Lesser\" General Public License because it does Less to protect the user's freedom than the ordinary General Public License. It also provides other free software developers Less of an advantage over competing non-free programs. These disadvantages are the reason we use the ordinary General Public License for many libraries. However, the Lesser license provides advantages in certain special circumstances. For example, on rare occasions, there may be a special need to encourage the widest possible use of a certain library, so that it becomes a de-facto standard. To achieve this, non-free programs must be allowed to use the library. A more frequent case is that a free library does the same job as widely used non-free libraries. In this case, there is little to gain by limiting the free library to free software only, so we use the Lesser General Public License. In other cases, permission to use a particular library in non-free programs enables a greater number of people to use a large body of free software. For example, permission to use the GNU C Library in non-free programs enables many more people to use the whole GNU operating system, as well as its variant, the GNU/Linux operating system. Although the Lesser General Public License is Less protective of the users' freedom, it does ensure that the user of a program that is linked with the Library has the freedom and the wherewithal to run that program using a modified version of the Library. The precise terms and conditions for copying, distribution and modification follow. Pay close attention to the difference between a \"work based on the library\" and a \"work that uses the library\". The former contains code derived from the library, whereas the latter must be combined with the library in order to run. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION 0. This License Agreement applies to any software library or other program which contains a notice placed by the copyright holder or other authorized party saying it may be distributed under the terms of this Lesser General Public License (also called \"this License\"). Each licensee is addressed as \"you\". A \"library\" means a collection of software functions and/or data prepared so as to be conveniently linked with application programs (which use some of those functions and data) to form executables. The \"Library\", below, refers to any such software library or work which has been distributed under these terms. A \"work based on the Library\" means either the Library or any derivative work under copyright law: that is to say, a work containing the Library or a portion of it, either verbatim or with modifications and/or translated straightforwardly into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) \"Source code\" for a work means the preferred form of the work for making modifications to it. For a library, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the library. Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running a program using the Library is not restricted, and output from such a program is covered only if its contents constitute a work based on the Library (independent of the use of the Library in a tool for writing it). Whether that is true depends on what the Library does and what the program that uses the Library does. You may copy and distribute verbatim copies of the Library's complete source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and distribute a copy of this License along with the Library. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. You may modify your copy or copies of the Library or any portion of it, thus forming a work based on the Library, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) The modified work must itself be a software library. b) You must cause the files modified to carry prominent notices stating that you changed the files and the date of any change. c) You must cause the whole of the work to be licensed at no charge to all third parties under the terms of this License. d) If a facility in the modified Library refers to a function or a table of data to be supplied by an application program that uses the facility, other than as an argument passed when the facility is invoked, then you must make a good faith effort to ensure that, in the event an application does not supply such function or table, the facility still operates, and performs whatever part of its purpose remains meaningful. (For example, a function in a library to compute square roots has a purpose that is entirely well-defined independent of the application. Therefore, Subsection 2d requires that any application-supplied function or table used by this function must be optional: if the application does not supply it, the square root function must still compute square roots.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Library, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Library, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Library. In addition, mere aggregation of another work not based on the Library with the Library (or with a work based on the Library) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. You may opt to apply the terms of the ordinary GNU General Public License instead of this License to a given copy of the Library. To do this, you must alter all the notices that refer to this License, so that they refer to the ordinary GNU General Public License, version 2, instead of to this License. (If a newer version than version 2 of the ordinary GNU General Public License has appeared, then you can specify that version instead if you wish.) Do not make any other change in these notices. Once this change is made in a given copy, it is irreversible for that copy, so the ordinary GNU General Public License applies to all subsequent copies and derivative works made from that copy. This option is useful when you wish to copy part of the code of the Library into a program that is not a library. You may copy and distribute the Library (or a portion or derivative of it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange. If distribution of object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place satisfies the requirement to distribute the source code, even though third parties are not compelled to copy the source along with the object code. A program that contains no derivative of any portion of the Library, but is designed to work with the Library by being compiled or linked with it, is called a \"work that uses the Library\". Such a work, in isolation, is not a derivative work of the Library, and therefore falls outside the scope of this License. However, linking a \"work that uses the Library\" with the Library creates an executable that is a derivative of the Library (because it contains portions of the Library), rather than a \"work that uses the library\". The executable is therefore covered by this License. Section 6 states terms for distribution of such executables. When a \"work that uses the Library\" uses material from a header file that is part of the Library, the object code for the work may be a derivative work of the Library even though the source code is not. Whether this is true is especially significant if the work can be linked without the Library, or if the work is itself a library. The threshold for this to be true is not precisely defined by law. If such an object file uses only numerical parameters, data structure layouts and accessors, and small macros and small inline functions (ten lines or less in length), then the use of the object file is unrestricted, regardless of whether it is legally a derivative work. (Executables containing this object code plus portions of the Library will still fall under Section 6.) Otherwise, if the work is a derivative of the Library, you may distribute the object code for the work under the terms of Section 6. Any executables containing that work also fall under Section 6, whether or not they are linked directly with the Library itself. As an exception to the Sections above, you may also combine or link a \"work that uses the Library\" with the Library to produce a work containing portions of the Library, and distribute that work under terms of your choice, provided that the terms permit modification of the work for the customer's own use and reverse engineering for debugging such modifications. You must give prominent notice with each copy of the work that the Library is used in it and that the Library and its use are covered by this License. You must supply a copy of this License. If the work during execution displays copyright notices, you must include the copyright notice for the Library among them, as well as a reference directing the user to the copy of this License. Also, you must do one of these things: a) Accompany the work with the complete corresponding machine-readable source code for the Library including whatever changes were used in the work (which must be distributed under Sections 1 and 2 above); and, if the work is an executable linked with the Library, with the complete machine-readable \"work that uses the Library\", as object code and/or source code, so that the user can modify the Library and then relink to produce a modified executable containing the modified Library. (It is understood that the user who changes the contents of definitions files in the Library will not necessarily be able to recompile the application to use the modified definitions.) b) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (1) uses at run time a copy of the library already present on the user's computer system, rather than copying library functions into the executable, and (2) will operate properly with a modified version of the library, if the user installs one, as long as the modified version is interface-compatible with the version that the work was made with. c) Accompany the work with a written offer, valid for at least three years, to give the same user the materials specified in Subsection 6a, above, for a charge no more than the cost of performing this distribution. d) If distribution of the work is made by offering access to copy from a designated place, offer equivalent access to copy the above specified materials from the same place. e) Verify that the user has already received a copy of these materials or that you have already sent this user a copy. For an executable, the required form of the \"work that uses the Library\" must include any data and utility programs needed for reproducing the executable from it. However, as a special exception, the materials to be distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. It may happen that this requirement contradicts the license restrictions of other proprietary libraries that do not normally accompany the operating system. Such a contradiction means you cannot use both them and the Library together in an executable that you distribute. You may place library facilities that are a work based on the Library side-by-side in a single library together with other library facilities not covered by this License, and distribute such a combined library, provided that the separate distribution of the work based on the Library and of the other library facilities is otherwise permitted, and provided that you do these two things: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities. This must be distributed under the terms of the Sections above. b) Give prominent notice with the combined library of the fact that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work. You may not copy, modify, sublicense, link with, or distribute the Library except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, link with, or distribute the Library is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Library or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Library (or any work based on the Library), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Library or works based on it. Each time you redistribute the Library (or any work based on the Library), the recipient automatically receives a license from the original licensor to copy, distribute, link with or modify the Library subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties with this License. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Library at all. For example, if a patent license would not permit royalty-free redistribution of the Library by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Library. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply, and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. If the distribution and/or use of the Library is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Library under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. The Free Software Foundation may publish revised and/or new versions of the Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Library does not specify a license version number, you may choose any version ever published by the Free Software Foundation. If you wish to incorporate parts of the Library into other free programs whose distribution conditions are incompatible with these, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE LIBRARY IS WITH YOU. SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE LIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Libraries If you develop a new library, and you want it to be of the greatest possible use to the public, we recommend making it free software that everyone can redistribute and change. You can do so by permitting redistribution under these terms (or, alternatively, under the terms of the ordinary General Public License). To apply these terms, attach the following notices to the library. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. one line to give the library's name and an idea of what it does. Copyright (C) year name of author This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version. This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA Also add information on how to contact you by electronic and paper mail. You should also get your employer (if you work as a programmer) or your school, if any, to sign a \"copyright disclaimer\" for the library, if necessary. Here is a sample; alter the names: Yoyodyne, Inc., hereby disclaims all copyright interest in the library `Frob' (a library for tweaking knobs) written by James Random Hacker. signature of Ty Coon, 1 April 1990 Ty Coon, President of Vice That's all there is to it!","title":"GNU Lesser General Public License"},{"location":"licenses/ISC_License/","text":"ISC License (ISCL) \u00b6 Copyright (c) 4-digit year, Company or Person's Name Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies. THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.","title":"ISC License"},{"location":"licenses/ISC_License/#isc-license-iscl","text":"Copyright (c) 4-digit year, Company or Person's Name Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies. THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.","title":"ISC License (ISCL)"},{"location":"licenses/Licensing/","text":"DAOS Version 1.0 Licensing \u00b6 DAOS version 1.0 uses the Apache 2.0 License for distribution. Other components used in the development, build and shipping may also have the necessary licensing requirements. The table below summarizes the applicable component licenses and a link to the appropriate license. License Component(s) Apache License 2.0 DAOS, build system, Apache License 2.0, yaml for Go, Google go-genproto, Apache Hadoop, grpc-go, Hadoop-and-Swift-integration Argonne National Laboratory, Department of Energy License Argonne National Laboratory Mercury-hpc Boost Software License 1.0 Boost C++ Libraries BSD 2-clause \"Simplified\" License OPA-PSM2, Go pkg/errors, protobuf-c, Open Fabric Interfaces (OFI) libfabric BSD 2-clause NetBSD License NetBSD BSD 3-clause \"New\" or \"Revised\" License net (github.com/golang/net), Open MPI, libuuid - Portable uuid C library, Go Text, willemt Raft Consensus Protocol Impl, google/uuid, 01Org isa-l, sg3_utils, intel ipmctl, libevent - an asynchronous event library, io.grpc:grpc-all, Portable Hardware Locality (hwloc), pmdk, BSD 3-clause \"New\" or \"Revised\" License, spdk spdk, readline, PMIx, CaRT, golang sys, Intel Data Plane Development Kit (DPDK), Google go-cmp, go-flags Common Public License 1.0 GraphViz Eclipse Public License 1.0 libtool GNU General Public License v2.0 only Linux-rdma rdma-core GNU General Public License v2.0 or later numactl, librdmacm, The PCI Utilities GNU Lesser General Public License v2.1 or later libaio, Persistent Memory Programming ndctl, Linux Libfuse ISC License python2, python3 Los Alamos National Laboratory BSD+ License PLFS MIT License LibYAML, safeclib, Mockito, scons-local, Dustin Sallings go-humanize Mpich License MPICH OpenSSL Combined License openssl UChicago Argonne -- OpenPA License Argonne National Laboratory OpenPA UChicago Argonne, LLC -- Argobots License Argonne National Laboratory Argobots","title":"Licensing"},{"location":"licenses/Licensing/#daos-version-10-licensing","text":"DAOS version 1.0 uses the Apache 2.0 License for distribution. Other components used in the development, build and shipping may also have the necessary licensing requirements. The table below summarizes the applicable component licenses and a link to the appropriate license. License Component(s) Apache License 2.0 DAOS, build system, Apache License 2.0, yaml for Go, Google go-genproto, Apache Hadoop, grpc-go, Hadoop-and-Swift-integration Argonne National Laboratory, Department of Energy License Argonne National Laboratory Mercury-hpc Boost Software License 1.0 Boost C++ Libraries BSD 2-clause \"Simplified\" License OPA-PSM2, Go pkg/errors, protobuf-c, Open Fabric Interfaces (OFI) libfabric BSD 2-clause NetBSD License NetBSD BSD 3-clause \"New\" or \"Revised\" License net (github.com/golang/net), Open MPI, libuuid - Portable uuid C library, Go Text, willemt Raft Consensus Protocol Impl, google/uuid, 01Org isa-l, sg3_utils, intel ipmctl, libevent - an asynchronous event library, io.grpc:grpc-all, Portable Hardware Locality (hwloc), pmdk, BSD 3-clause \"New\" or \"Revised\" License, spdk spdk, readline, PMIx, CaRT, golang sys, Intel Data Plane Development Kit (DPDK), Google go-cmp, go-flags Common Public License 1.0 GraphViz Eclipse Public License 1.0 libtool GNU General Public License v2.0 only Linux-rdma rdma-core GNU General Public License v2.0 or later numactl, librdmacm, The PCI Utilities GNU Lesser General Public License v2.1 or later libaio, Persistent Memory Programming ndctl, Linux Libfuse ISC License python2, python3 Los Alamos National Laboratory BSD+ License PLFS MIT License LibYAML, safeclib, Mockito, scons-local, Dustin Sallings go-humanize Mpich License MPICH OpenSSL Combined License openssl UChicago Argonne -- OpenPA License Argonne National Laboratory OpenPA UChicago Argonne, LLC -- Argobots License Argonne National Laboratory Argobots","title":"DAOS Version 1.0 Licensing"},{"location":"licenses/LosAlamos_License/","text":"Los Alamos National Laboratory BSD+ License \u00b6 Copyright (C) 2004, The Regents of the University of California All rights reserved. Copyright (2004). The Regents of the University of California. This software was produced under U.S. Government contract W-7405-ENG-36 for Los Alamos National Laboratory (LANL), which is operated by the University of California for the U.S. Department of Energy. The U.S. Government has rights to use, reproduce, and distribute this software. NEITHER THE GOVERNMENT NOR THE UNIVERSITY MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE. If software is modified to produce derivative works, such modified software should be clearly marked, so as not to confuse it with the version available from LANL. Additionally, redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the University of California, LANL, the U.S. Government, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE UNIVERSITY AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"LosAlamos License"},{"location":"licenses/LosAlamos_License/#los-alamos-national-laboratory-bsd-license","text":"Copyright (C) 2004, The Regents of the University of California All rights reserved. Copyright (2004). The Regents of the University of California. This software was produced under U.S. Government contract W-7405-ENG-36 for Los Alamos National Laboratory (LANL), which is operated by the University of California for the U.S. Department of Energy. The U.S. Government has rights to use, reproduce, and distribute this software. NEITHER THE GOVERNMENT NOR THE UNIVERSITY MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE. If software is modified to produce derivative works, such modified software should be clearly marked, so as not to confuse it with the version available from LANL. Additionally, redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the University of California, LANL, the U.S. Government, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE UNIVERSITY AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Los Alamos National Laboratory BSD+ License"},{"location":"licenses/MIT_License/","text":"The MIT License \u00b6 Copyright (c) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"licenses/MIT_License/#the-mit-license","text":"Copyright (c) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"The MIT License"},{"location":"licenses/MPICH_License/","text":"MPICH License \u00b6 COPYRIGHT The following is a notice of limited availability of the code, and disclaimer which must be included in the prologue of the code and in all source listings of the code. Copyright Notice + 1993 University of Chicago + 1993 Mississippi State University Permission is hereby granted to use, reproduce, prepare derivative works, and to redistribute to others. This software was authored by: Argonne National Laboratory Group W. Gropp: (630) 252-4318; FAX: (630) 252-5986; e-mail: gropp@mcs.anl.gov E. Lusk: (630) 252-7852; FAX: (630) 252-5986; e-mail: lusk@mcs.anl.gov Mathematics and Computer Science Division Argonne National Laboratory, Argonne IL 60439 Mississippi State Group N. Doss: (601) 325-2565; FAX: (601) 325-7692; e-mail: doss@erc.msstate.edu A. Skjellum:(601) 325-8435; FAX: (601) 325-8997; e-mail: tony@erc.msstate.edu Mississippi State University, Computer Science Department & NSF Engineering Research Center for Computational Field Simulation P.O. Box 6176, Mississippi State MS 39762 GOVERNMENT LICENSE Portions of this material resulted from work developed under a U.S. Government Contract and are subject to the following license: the Government is granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable worldwide license in this computer software to reproduce, prepare derivative works, and perform publicly and display publicly. DISCLAIMER This computer code material was prepared, in part, as an account of work sponsored by an agency of the United States Government. Neither the United States, nor the University of Chicago, nor Mississippi State University, nor any of their employees, makes any warranty express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.","title":"MPICH License"},{"location":"licenses/MPICH_License/#mpich-license","text":"COPYRIGHT The following is a notice of limited availability of the code, and disclaimer which must be included in the prologue of the code and in all source listings of the code. Copyright Notice + 1993 University of Chicago + 1993 Mississippi State University Permission is hereby granted to use, reproduce, prepare derivative works, and to redistribute to others. This software was authored by: Argonne National Laboratory Group W. Gropp: (630) 252-4318; FAX: (630) 252-5986; e-mail: gropp@mcs.anl.gov E. Lusk: (630) 252-7852; FAX: (630) 252-5986; e-mail: lusk@mcs.anl.gov Mathematics and Computer Science Division Argonne National Laboratory, Argonne IL 60439 Mississippi State Group N. Doss: (601) 325-2565; FAX: (601) 325-7692; e-mail: doss@erc.msstate.edu A. Skjellum:(601) 325-8435; FAX: (601) 325-8997; e-mail: tony@erc.msstate.edu Mississippi State University, Computer Science Department & NSF Engineering Research Center for Computational Field Simulation P.O. Box 6176, Mississippi State MS 39762 GOVERNMENT LICENSE Portions of this material resulted from work developed under a U.S. Government Contract and are subject to the following license: the Government is granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable worldwide license in this computer software to reproduce, prepare derivative works, and perform publicly and display publicly. DISCLAIMER This computer code material was prepared, in part, as an account of work sponsored by an agency of the United States Government. Neither the United States, nor the University of Chicago, nor Mississippi State University, nor any of their employees, makes any warranty express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.","title":"MPICH License"},{"location":"licenses/NetBSD_License/","text":"NetBSD License \u00b6 Copyright (c) 2008 The NetBSD Foundation, Inc. All rights reserved. This code is derived from software contributed to The NetBSD Foundation by Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"NetBSD License"},{"location":"licenses/NetBSD_License/#netbsd-license","text":"Copyright (c) 2008 The NetBSD Foundation, Inc. All rights reserved. This code is derived from software contributed to The NetBSD Foundation by Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"NetBSD License"},{"location":"licenses/OpenSSL_License/","text":"OpenSSL Combined License \u00b6 LICENSE ISSUES \u00b6 The OpenSSL toolkit stays under a dual license, i.e. both the conditions of the OpenSSL License and the original SSLeay license apply to the toolkit. See below for the actual license texts. Actually both licenses are BSD-style Open Source licenses. In case of any license issues related to OpenSSL please contact openssl-core@openssl.org. OpenSSL License \u00b6 ==================================================================== Copyright (c) 1998-2008 The OpenSSL Project. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. All advertising materials mentioning features or use of this software must display the following acknowledgment: \"This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit. (http://www.openssl.org/)\" The names \"OpenSSL Toolkit\" and \"OpenSSL Project\" must not be used to endorse or promote products derived from this software without prior written permission. For written permission, please contact openssl-core@openssl.org. Products derived from this software may not be called \"OpenSSL\" nor may \"OpenSSL\" appear in their names without prior written permission of the OpenSSL Project. Redistributions of any form whatsoever must retain the following acknowledgment: \"This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit (http://www.openssl.org/)\" THIS SOFTWARE IS PROVIDED BY THE OpenSSL PROJECT \"AS IS\" AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE OpenSSL PROJECT OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ==================================================================== This product includes cryptographic software written by Eric Young (eay@cryptsoft.com). This product includes software written by Tim Hudson (tjh@cryptsoft.com). Original SSLeay License \u00b6 Copyright (C) 1995-1998 Eric Young (eay@cryptsoft.com) All rights reserved. This package is an SSL implementation written by Eric Young (eay@cryptsoft.com). The implementation was written so as to conform with Netscapes SSL. This library is free for commercial and non-commercial use as long as the following conditions are aheared to. The following conditions apply to all code found in this distribution, be it the RC4, RSA, lhash, DES, etc., code; not just the SSL code. The SSL documentation included with this distribution is covered by the same copyright terms except that the holder is Tim Hudson (tjh@cryptsoft.com). Copyright remains Eric Young's, and as such any Copyright notices in the code are not to be removed. If this package is used in a product, Eric Young should be given attribution as the author of the parts of the library used. This can be in the form of a textual message at program startup or in documentation (online or textual) provided with the package. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. All advertising materials mentioning features or use of this software must display the following acknowledgement: \"This product includes cryptographic software written by Eric Young (eay@cryptsoft.com)\" The word 'cryptographic' can be left out if the rouines from the library being used are not cryptographic related :-). If you include any Windows specific code (or a derivative thereof) from the apps directory (application code) you must include an acknowledgement: \"This product includes software written by Tim Hudson (tjh@cryptsoft.com)\" THIS SOFTWARE IS PROVIDED BY ERIC YOUNG \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The licence and distribution terms for any publically available version or derivative of this code cannot be changed. i.e. this code cannot simply be copied and put under another distribution licence [including the GNU Public Licence.]","title":"OpenSSL License"},{"location":"licenses/OpenSSL_License/#openssl-combined-license","text":"","title":"OpenSSL Combined License"},{"location":"licenses/OpenSSL_License/#license-issues","text":"The OpenSSL toolkit stays under a dual license, i.e. both the conditions of the OpenSSL License and the original SSLeay license apply to the toolkit. See below for the actual license texts. Actually both licenses are BSD-style Open Source licenses. In case of any license issues related to OpenSSL please contact openssl-core@openssl.org.","title":"LICENSE ISSUES"},{"location":"licenses/OpenSSL_License/#openssl-license","text":"==================================================================== Copyright (c) 1998-2008 The OpenSSL Project. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. All advertising materials mentioning features or use of this software must display the following acknowledgment: \"This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit. (http://www.openssl.org/)\" The names \"OpenSSL Toolkit\" and \"OpenSSL Project\" must not be used to endorse or promote products derived from this software without prior written permission. For written permission, please contact openssl-core@openssl.org. Products derived from this software may not be called \"OpenSSL\" nor may \"OpenSSL\" appear in their names without prior written permission of the OpenSSL Project. Redistributions of any form whatsoever must retain the following acknowledgment: \"This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit (http://www.openssl.org/)\" THIS SOFTWARE IS PROVIDED BY THE OpenSSL PROJECT \"AS IS\" AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE OpenSSL PROJECT OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ==================================================================== This product includes cryptographic software written by Eric Young (eay@cryptsoft.com). This product includes software written by Tim Hudson (tjh@cryptsoft.com).","title":"OpenSSL License"},{"location":"licenses/OpenSSL_License/#original-ssleay-license","text":"Copyright (C) 1995-1998 Eric Young (eay@cryptsoft.com) All rights reserved. This package is an SSL implementation written by Eric Young (eay@cryptsoft.com). The implementation was written so as to conform with Netscapes SSL. This library is free for commercial and non-commercial use as long as the following conditions are aheared to. The following conditions apply to all code found in this distribution, be it the RC4, RSA, lhash, DES, etc., code; not just the SSL code. The SSL documentation included with this distribution is covered by the same copyright terms except that the holder is Tim Hudson (tjh@cryptsoft.com). Copyright remains Eric Young's, and as such any Copyright notices in the code are not to be removed. If this package is used in a product, Eric Young should be given attribution as the author of the parts of the library used. This can be in the form of a textual message at program startup or in documentation (online or textual) provided with the package. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. All advertising materials mentioning features or use of this software must display the following acknowledgement: \"This product includes cryptographic software written by Eric Young (eay@cryptsoft.com)\" The word 'cryptographic' can be left out if the rouines from the library being used are not cryptographic related :-). If you include any Windows specific code (or a derivative thereof) from the apps directory (application code) you must include an acknowledgement: \"This product includes software written by Tim Hudson (tjh@cryptsoft.com)\" THIS SOFTWARE IS PROVIDED BY ERIC YOUNG \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The licence and distribution terms for any publically available version or derivative of this code cannot be changed. i.e. this code cannot simply be copied and put under another distribution licence [including the GNU Public Licence.]","title":"Original SSLeay License"},{"location":"overview/fault/","text":"Fault Model \u00b6 DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. DAOS internal pool and container metadata are replicated via a robust consensus algorithm. DAOS objects are then safely replicated or erasure-coded by transparently leveraging the DAOS distributed transaction mechanisms internally. The purpose of this section is to provide details on how DAOS achieves fault tolerance and guarantees object resilience. Hierarchical Fault Domains \u00b6 A fault domain is a set of servers sharing the same point of failure and which are thus likely to fail altogether. DAOS assumes that fault domains are hierarchical and do not overlap. The actual hierarchy and fault domain membership must be supplied by an external database used by DAOS to generate the pool map. Pool metadata are replicated on several nodes from different high-level fault domains for high availability, whereas object data is replicated or erasure-coded over a variable number of fault domains depending on the selected object class. Fault Detection \u00b6 DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure will make an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Fault Isolation \u00b6 Once detected, the faulty target or servers (effectivelly a set of targets) must be excluded from the pool map. This process is triggered either manually by the administrator or automatically. Upon exclusion, the new version of the pool map is eagerly pushed to all storage targets. At this point, the pool enters a degraded mode that might require extra processing on access (e.g. reconstructing data out of erasure code). Consequently, DAOS client and storage nodes retry RPC indefinitely until they find an alternative replacement target from the new pool map. At this point, all outstanding communications with the evicted target are aborted, and no further messages should be sent to the target until it is explicitly reintegrated (possibly only after maintenance action). All storage targets are promptly notified of pool map changes by the pool service. This is not the case for client nodes, which are lazily informed of pool map invalidation each time they communicate with servers. To do so, clients pack in every RPC their current pool map version. Servers reply not only with the current pool map version. Consequently, when a DAOS client experiences RPC timeout, it regularly communicates with the other DAOS target to guarantee that its pool map is always current. Clients will then eventually be informed of the target exclusion and enter into degraded mode. This mechanism guarantees global node eviction and that all nodes eventually share the same view of target aliveness. Fault Recovery \u00b6 Upon exclusion from the pool map, each target starts the rebuild process automatically to restore data redundancy. First, each target creates a list of local objects impacted by the target exclusion. This is done by scanning a local object table maintained by the underlying storage layer. Then for each impacted object, the location of the new object shard is determined and redundancy of the object restored for the whole history (i.e., snapshots). Once all impacted objects have been rebuilt, the pool map is updated a second time to report the target as failed out. This marks the end of collective rebuild process and the exit from degraded mode for this particular fault. At this point, the pool has fully recovered from the fault and client nodes can now read from the rebuilt object shards. This rebuild process is executed online while applications continue accessing and updating objects.","title":"Fault Model"},{"location":"overview/fault/#fault-model","text":"DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. DAOS internal pool and container metadata are replicated via a robust consensus algorithm. DAOS objects are then safely replicated or erasure-coded by transparently leveraging the DAOS distributed transaction mechanisms internally. The purpose of this section is to provide details on how DAOS achieves fault tolerance and guarantees object resilience.","title":"Fault Model"},{"location":"overview/fault/#hierarchical-fault-domains","text":"A fault domain is a set of servers sharing the same point of failure and which are thus likely to fail altogether. DAOS assumes that fault domains are hierarchical and do not overlap. The actual hierarchy and fault domain membership must be supplied by an external database used by DAOS to generate the pool map. Pool metadata are replicated on several nodes from different high-level fault domains for high availability, whereas object data is replicated or erasure-coded over a variable number of fault domains depending on the selected object class.","title":"Hierarchical Fault Domains"},{"location":"overview/fault/#fault-detection","text":"DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure will make an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed.","title":"Fault Detection"},{"location":"overview/fault/#fault-isolation","text":"Once detected, the faulty target or servers (effectivelly a set of targets) must be excluded from the pool map. This process is triggered either manually by the administrator or automatically. Upon exclusion, the new version of the pool map is eagerly pushed to all storage targets. At this point, the pool enters a degraded mode that might require extra processing on access (e.g. reconstructing data out of erasure code). Consequently, DAOS client and storage nodes retry RPC indefinitely until they find an alternative replacement target from the new pool map. At this point, all outstanding communications with the evicted target are aborted, and no further messages should be sent to the target until it is explicitly reintegrated (possibly only after maintenance action). All storage targets are promptly notified of pool map changes by the pool service. This is not the case for client nodes, which are lazily informed of pool map invalidation each time they communicate with servers. To do so, clients pack in every RPC their current pool map version. Servers reply not only with the current pool map version. Consequently, when a DAOS client experiences RPC timeout, it regularly communicates with the other DAOS target to guarantee that its pool map is always current. Clients will then eventually be informed of the target exclusion and enter into degraded mode. This mechanism guarantees global node eviction and that all nodes eventually share the same view of target aliveness.","title":"Fault Isolation"},{"location":"overview/fault/#fault-recovery","text":"Upon exclusion from the pool map, each target starts the rebuild process automatically to restore data redundancy. First, each target creates a list of local objects impacted by the target exclusion. This is done by scanning a local object table maintained by the underlying storage layer. Then for each impacted object, the location of the new object shard is determined and redundancy of the object restored for the whole history (i.e., snapshots). Once all impacted objects have been rebuilt, the pool map is updated a second time to report the target as failed out. This marks the end of collective rebuild process and the exit from degraded mode for this particular fault. At this point, the pool has fully recovered from the fault and client nodes can now read from the rebuilt object shards. This rebuild process is executed online while applications continue accessing and updating objects.","title":"Fault Recovery"},{"location":"overview/security/","text":"Security Model \u00b6 DAOS uses a flexible security model that seperates authentication from authorization. It is designed to have very minimal impact on the I/O path. Authentication \u00b6 The DAOS security model is designed to support different authentication methods. By default, a local agent runs on the client node and authenticats the user process through AUTH_SYS. Authentication can be handle by a third party service like munge or Kerberos. Authorization \u00b6 DAOS supports a subset of the NFSv4 ACLs for both pools and containers through the properties API.","title":"Security Model"},{"location":"overview/security/#security-model","text":"DAOS uses a flexible security model that seperates authentication from authorization. It is designed to have very minimal impact on the I/O path.","title":"Security Model"},{"location":"overview/security/#authentication","text":"The DAOS security model is designed to support different authentication methods. By default, a local agent runs on the client node and authenticats the user process through AUTH_SYS. Authentication can be handle by a third party service like munge or Kerberos.","title":"Authentication"},{"location":"overview/security/#authorization","text":"DAOS supports a subset of the NFSv4 ACLs for both pools and containers through the properties API.","title":"Authorization"},{"location":"overview/storage/","text":"Storage Model \u00b6 We consider a data center with hundreds of thousands of compute nodes interconnected via a scalable high-performance fabric (i.e., Ethernet, RoCE or Infiniband), where all or a subset of the nodes, called storage nodes, have direct access to byte-addressable storage-class memory (SCM) and, optionally, block-based NVMe storage. The DAOS server is a multi-tenant daemon running on a Linux instance (i.e., natively on the physical node or in a VM or container) of each storage node and exporting through the network the locally-attached storage. Inside a DAOS server, the storage is statically partitioned across multiple targets to optimize concurrency. To avoid contention, each target has its private storage, own pool of service threads and dedicated network context that can be directly addressed over the fabric independently of the other targets hosted on the same storage node. The number of targets exported by a DAOS server instance is configurable and depends on the underlying hardware (i.e., number of SCM modules, CPUs, NVMe SSDs, ...). A target is the unit of fault. All DAOS servers connected to the same fabric are grouped to form a DAOS system, identified by a system name. Membership of the DAOS servers is recorded into the system map that assigns a unique integer rank to each server. Two different systems comprise two disjoint sets of servers and do not coordinate with each other. The figure below represents the fundamental abstractions of the DAOS storage model. A DAOS pool is a storage reservation distributed across a collection of targets. The actual space allocated to the pool on each target is called a pool shard. The total space allocated to a pool is decided at creation time and can be expanded over time by resizing all the pool shards (within the limit of the storage capacity dedicated to each target) or by spanning more targets (i.e., adding more pool shards). A pool offers storage virtualization and is the unit of provisioning and isolation. DAOS pools cannot span across multiple systems. A pool can host multiple transactional object store called DAOS containers. Each container is a private object address space, which can be modified transactionally and independently of the other containers stored in the same pool. A container is the unit of snapshot and data management. DAOS objects belonging to a container can be distributed across any target of the pool for both performance and resilience and can be accessed through different APIs to represent structured, semi-structured and unstructured data efficiently The table below shows the targeted level of scalability for each DAOS concept. DAOS Concept Order of Magnitude System 10 5 Servers (hundreds of thousands) and 10 2 Pools (hundreds) Server 10 1 Targets (tens) Pool 10 2 Containers (hundreds) Container 10 9 Objects (billions) DAOS Target \u00b6 A target is typically associated with a single-ported SCM module and NVMe SSD attached to a single storage node. Moreover, a target does not implement any internal data protection mechanism against storage media failure. As a result, a target is a single point of failure. A dynamic state is associated with each target and is set to either up and running, or down and not available. A target is the unit of performance. Hardware components associated with the target, such as the backend storage medium, the server, and the network, have limited capability and capacity. Target performance parameters such as bandwidth and latency are exported to the upper layers. DAOS Pool \u00b6 A pool is identified by a unique UUID and maintains target memberships in a persistent versioned list called the pool map. The membership is definitive and consistent, and membership changes are sequentially numbered. The pool map not only records the list of active targets, it also contains the storage topology under the form of a tree that is used to identify targets sharing common hardware components. For instance, the first level of the tree can represent targets sharing the same motherboard, and then the second level can represent all motherboards sharing the same rack and finally the third level can represent all racks in the same cage. This framework effectively represents hierarchical fault domains, which are then used to avoid placing redundant data on targets subject to correlated failures. At any point in time, new targets can be added to the pool map, and failed ones can be excluded. Moreover, the pool map is fully versioned, which effectively assigns a unique sequence to each modification of the map, more particularly for failed node removal. A pool shard is a reservation of persistent memory optionally combined with a pre-allocated space on NVMe storage on a specific target. It has a fixed capacity and fails operations when full. Current space usage can be queried at any time and reports the total amount of bytes used by any data type stored in the pool shard. Upon target failure and exclusion from the pool map, data redundancy inside the pool is automatically restored online. This self-healing process is known as rebuild. Rebuild progress is recorded regularly in special logs in the pool stored in persistent memory to address cascading failures. When new targets are added, data is automatically migrated to the newly added targets to redistribute space usage equally among all the members. This process is known as space rebalancing and uses dedicated persistent logs as well to support interruption and restart. A pool is a set of targets spread across different storage nodes over which data and metadata are distributed to achieve horizontal scalability, and replicated or erasure-coded to ensure durability and availability. When creating a pool, a set of system properties must be defined to configure the different features supported by the pool. Also, user can define their attributes that will be stored persistently. A pool is only accessible to authenticated and authorized applications. Multiple security frameworks could be supported, from NFSv4 access control lists to third party-based authentication (such as Kerberos). Security is enforced when connecting to the pool. Upon successful connection to the pool, a connection context is returned to the application process. As detailed previously, a pool stores many different sorts of persistent metadata, such as the pool map, authentication, and authorization information, user attributes, properties and rebuild logs. Such metadata are critical and require the highest level of resiliency. Therefore, the pool metadata are replicated on a few nodes from distinct high-level fault domains. For very large configurations with hundreds of thousands of storage nodes, only a very small fraction of those nodes (in the order of tens) run the pool metadata service. With a limited number of storage nodes, DAOS can afford to rely on a consensus algorithm to reach agreement and to guarantee consistency in the presence of faults and to avoid split-brain syndrome. To access a pool, a user process should connect to this pool and pass the security checks. Once granted, a pool connection can be shared (via local2global() and global2local() operations) with any or all of its peer application processes (similar to the openg() POSIX extension). This collective connect mechanism allows avoiding metadata request storm when a massively distributed job is run on the datacenter. A pool connection is then revoked when the original process that issued the connection request disconnects from the pool. DAOS Container \u00b6 A container represents an object address space inside a pool and is identified by a UUID. The diagram below represents how the user (i.e., I/O middleware, domain-specific data format, big data or AI frameworks ...) could use the container concept to store related datasets. Like pools, containers can store user attributes, and a set of properties must be passed at container creation time to configure different features like checksums. To access a container, an application must first connect to the pool and then open the container. If the application is authorized to access the container, a container handle is returned. This includes capabilities that authorize any process in the application to access the container and its contents. The opening process may share this handle with any or all of its peers. Their capabilities are revoked either on container close. Objects in a container may have different schemas for data distribution and redundancy over targets. Dynamic or static striping, replication, or erasure code are some parameters required to define the object schema. The object class defines common schema attributes for a set of objects. Each object class is assigned a unique identifier and is associated with a given schema at the pool level. A new object class can be defined at any time with a configurable schema, which is then immutable after creation, or at least until all objects belonging to the class have been destroyed. For convenience, several object classes expected to be the most commonly used will be predefined by default when the pool is created, as shown the table below table below. Sample of Pre-defined Object Classes Object Class (RW = read/write, RM = read-mostly Redundancy Layout (SC = stripe count, RC = replica count, PC = parity count, TGT = target Small size & RW Replication static SCxRC, e.g. 1x4 Small size & RM Erasure code static SC+PC, e.g. 4+2 Large size & RW Replication static SCxRC over max #targets) Large size & RM Erasure code static SCx(SC+PC) w/ max #TGT) Unknown size & RW Replication SCxRC, e.g. 1x4 initially and grows Unknown size & RM Erasure code SC+PC, e.g. 4+2 initially and grows As shown below, each object is identified in the container by a unique 128-bit object address. The high 32 bits of the object address is reserved for DAOS to encode internal metadata such as the object class. The remaining 96 bits are managed by the user and should be unique inside the container. Those bits can be used by upper layers of the stack to encode their metadata as long as unicity is guaranteed. A per-container 64-bit scalable object ID allocator is provided in the DAOS API. The object ID to be stored by the application is the full 128-bit address which is for single use only and can be associated with only a single object schema. DAOS Object ID Structure <---------------------------------- 128 bits ----------------------------------> -------------------------------------------------------------------------------- |DAOS Internal Bits| Unique User Bits | -------------------------------------------------------------------------------- <---- 32 bits ----><------------------------- 96 bits -------------------------> A container is the basic unit of transaction and versioning. All object operations are implicitly tagged by the DAOS library with a timestamp called an epoch. The DAOS transaction API allows combining multiple object updates into a single atomic transaction with multi-version concurrency control based on epoch ordering. All the versioned updates may periodically be aggregated to reclaim space utilized by overlapping writes and to reduce metadata complexity. A snapshot is a permanent reference that can be placed on a specific epoch to prevent aggregation. Container metadata (i.e., list of snapshots, container open handles, object class, user attributes, properties, and others) are stored in persistent memory and maintained by a dedicated container metadata service that either uses the same replicated engine as the parent metadata pool service or has its own engine. This is configurable when creating a container. Like a pool, access to a container is controlled by the container handle. To acquire a valid handle, an application process must open the container and pass the security checks. This container handle may then be shared with other peer application processes via the container local2global() and global2local() operations. DAOS Object \u00b6 To avoid scaling problems and overhead common to a traditional storage system, DAOS objects are intentionally simple. No default object metadata beyond the type and schema are provided. This means that the system does not maintain time, size, owner, permissions or even track openers. To achieve high availability and horizontal scalability, many object schemas (replication/erasure code, static/dynamic striping, and others) are provided. The schema framework is flexible and easily expandable to allow for new custom schema types in the future. The layout is generated algorithmically on object open from the object identifier and the pool map. End-to-end integrity is assured by protecting object data with checksums during network transfer and storage. A DAOS object can be accessed through different APIs: Multi-level key-array API is the native object interface with locality feature. The key is split into a distribution (i.e., dkey) and an attribute (i.e., akey) key. Both the dkey and akey can be of variable length and type (i.e. a string, an integer or even a complex data structure). All entries under the same dkey are guaranteed to be collocated on the same target. The value associated with akey can be either a single variable-length value that cannot be partially overwritten or an array of fixed-length values. Both the akeys and dkeys support enumeration. Key-value API provides a simple key and variable-length value interface. It supports the traditional put, get, remove and list operations. Array API implements a one-dimensional array of fixed-size elements addressed by a 64-bit offset. A DAOS array supports arbitrary extent read, write and punch operations.","title":"Storage Model"},{"location":"overview/storage/#storage-model","text":"We consider a data center with hundreds of thousands of compute nodes interconnected via a scalable high-performance fabric (i.e., Ethernet, RoCE or Infiniband), where all or a subset of the nodes, called storage nodes, have direct access to byte-addressable storage-class memory (SCM) and, optionally, block-based NVMe storage. The DAOS server is a multi-tenant daemon running on a Linux instance (i.e., natively on the physical node or in a VM or container) of each storage node and exporting through the network the locally-attached storage. Inside a DAOS server, the storage is statically partitioned across multiple targets to optimize concurrency. To avoid contention, each target has its private storage, own pool of service threads and dedicated network context that can be directly addressed over the fabric independently of the other targets hosted on the same storage node. The number of targets exported by a DAOS server instance is configurable and depends on the underlying hardware (i.e., number of SCM modules, CPUs, NVMe SSDs, ...). A target is the unit of fault. All DAOS servers connected to the same fabric are grouped to form a DAOS system, identified by a system name. Membership of the DAOS servers is recorded into the system map that assigns a unique integer rank to each server. Two different systems comprise two disjoint sets of servers and do not coordinate with each other. The figure below represents the fundamental abstractions of the DAOS storage model. A DAOS pool is a storage reservation distributed across a collection of targets. The actual space allocated to the pool on each target is called a pool shard. The total space allocated to a pool is decided at creation time and can be expanded over time by resizing all the pool shards (within the limit of the storage capacity dedicated to each target) or by spanning more targets (i.e., adding more pool shards). A pool offers storage virtualization and is the unit of provisioning and isolation. DAOS pools cannot span across multiple systems. A pool can host multiple transactional object store called DAOS containers. Each container is a private object address space, which can be modified transactionally and independently of the other containers stored in the same pool. A container is the unit of snapshot and data management. DAOS objects belonging to a container can be distributed across any target of the pool for both performance and resilience and can be accessed through different APIs to represent structured, semi-structured and unstructured data efficiently The table below shows the targeted level of scalability for each DAOS concept. DAOS Concept Order of Magnitude System 10 5 Servers (hundreds of thousands) and 10 2 Pools (hundreds) Server 10 1 Targets (tens) Pool 10 2 Containers (hundreds) Container 10 9 Objects (billions)","title":"Storage Model"},{"location":"overview/storage/#daos-target","text":"A target is typically associated with a single-ported SCM module and NVMe SSD attached to a single storage node. Moreover, a target does not implement any internal data protection mechanism against storage media failure. As a result, a target is a single point of failure. A dynamic state is associated with each target and is set to either up and running, or down and not available. A target is the unit of performance. Hardware components associated with the target, such as the backend storage medium, the server, and the network, have limited capability and capacity. Target performance parameters such as bandwidth and latency are exported to the upper layers.","title":"DAOS Target"},{"location":"overview/storage/#daos-pool","text":"A pool is identified by a unique UUID and maintains target memberships in a persistent versioned list called the pool map. The membership is definitive and consistent, and membership changes are sequentially numbered. The pool map not only records the list of active targets, it also contains the storage topology under the form of a tree that is used to identify targets sharing common hardware components. For instance, the first level of the tree can represent targets sharing the same motherboard, and then the second level can represent all motherboards sharing the same rack and finally the third level can represent all racks in the same cage. This framework effectively represents hierarchical fault domains, which are then used to avoid placing redundant data on targets subject to correlated failures. At any point in time, new targets can be added to the pool map, and failed ones can be excluded. Moreover, the pool map is fully versioned, which effectively assigns a unique sequence to each modification of the map, more particularly for failed node removal. A pool shard is a reservation of persistent memory optionally combined with a pre-allocated space on NVMe storage on a specific target. It has a fixed capacity and fails operations when full. Current space usage can be queried at any time and reports the total amount of bytes used by any data type stored in the pool shard. Upon target failure and exclusion from the pool map, data redundancy inside the pool is automatically restored online. This self-healing process is known as rebuild. Rebuild progress is recorded regularly in special logs in the pool stored in persistent memory to address cascading failures. When new targets are added, data is automatically migrated to the newly added targets to redistribute space usage equally among all the members. This process is known as space rebalancing and uses dedicated persistent logs as well to support interruption and restart. A pool is a set of targets spread across different storage nodes over which data and metadata are distributed to achieve horizontal scalability, and replicated or erasure-coded to ensure durability and availability. When creating a pool, a set of system properties must be defined to configure the different features supported by the pool. Also, user can define their attributes that will be stored persistently. A pool is only accessible to authenticated and authorized applications. Multiple security frameworks could be supported, from NFSv4 access control lists to third party-based authentication (such as Kerberos). Security is enforced when connecting to the pool. Upon successful connection to the pool, a connection context is returned to the application process. As detailed previously, a pool stores many different sorts of persistent metadata, such as the pool map, authentication, and authorization information, user attributes, properties and rebuild logs. Such metadata are critical and require the highest level of resiliency. Therefore, the pool metadata are replicated on a few nodes from distinct high-level fault domains. For very large configurations with hundreds of thousands of storage nodes, only a very small fraction of those nodes (in the order of tens) run the pool metadata service. With a limited number of storage nodes, DAOS can afford to rely on a consensus algorithm to reach agreement and to guarantee consistency in the presence of faults and to avoid split-brain syndrome. To access a pool, a user process should connect to this pool and pass the security checks. Once granted, a pool connection can be shared (via local2global() and global2local() operations) with any or all of its peer application processes (similar to the openg() POSIX extension). This collective connect mechanism allows avoiding metadata request storm when a massively distributed job is run on the datacenter. A pool connection is then revoked when the original process that issued the connection request disconnects from the pool.","title":"DAOS Pool"},{"location":"overview/storage/#daos-container","text":"A container represents an object address space inside a pool and is identified by a UUID. The diagram below represents how the user (i.e., I/O middleware, domain-specific data format, big data or AI frameworks ...) could use the container concept to store related datasets. Like pools, containers can store user attributes, and a set of properties must be passed at container creation time to configure different features like checksums. To access a container, an application must first connect to the pool and then open the container. If the application is authorized to access the container, a container handle is returned. This includes capabilities that authorize any process in the application to access the container and its contents. The opening process may share this handle with any or all of its peers. Their capabilities are revoked either on container close. Objects in a container may have different schemas for data distribution and redundancy over targets. Dynamic or static striping, replication, or erasure code are some parameters required to define the object schema. The object class defines common schema attributes for a set of objects. Each object class is assigned a unique identifier and is associated with a given schema at the pool level. A new object class can be defined at any time with a configurable schema, which is then immutable after creation, or at least until all objects belonging to the class have been destroyed. For convenience, several object classes expected to be the most commonly used will be predefined by default when the pool is created, as shown the table below table below. Sample of Pre-defined Object Classes Object Class (RW = read/write, RM = read-mostly Redundancy Layout (SC = stripe count, RC = replica count, PC = parity count, TGT = target Small size & RW Replication static SCxRC, e.g. 1x4 Small size & RM Erasure code static SC+PC, e.g. 4+2 Large size & RW Replication static SCxRC over max #targets) Large size & RM Erasure code static SCx(SC+PC) w/ max #TGT) Unknown size & RW Replication SCxRC, e.g. 1x4 initially and grows Unknown size & RM Erasure code SC+PC, e.g. 4+2 initially and grows As shown below, each object is identified in the container by a unique 128-bit object address. The high 32 bits of the object address is reserved for DAOS to encode internal metadata such as the object class. The remaining 96 bits are managed by the user and should be unique inside the container. Those bits can be used by upper layers of the stack to encode their metadata as long as unicity is guaranteed. A per-container 64-bit scalable object ID allocator is provided in the DAOS API. The object ID to be stored by the application is the full 128-bit address which is for single use only and can be associated with only a single object schema. DAOS Object ID Structure <---------------------------------- 128 bits ----------------------------------> -------------------------------------------------------------------------------- |DAOS Internal Bits| Unique User Bits | -------------------------------------------------------------------------------- <---- 32 bits ----><------------------------- 96 bits -------------------------> A container is the basic unit of transaction and versioning. All object operations are implicitly tagged by the DAOS library with a timestamp called an epoch. The DAOS transaction API allows combining multiple object updates into a single atomic transaction with multi-version concurrency control based on epoch ordering. All the versioned updates may periodically be aggregated to reclaim space utilized by overlapping writes and to reduce metadata complexity. A snapshot is a permanent reference that can be placed on a specific epoch to prevent aggregation. Container metadata (i.e., list of snapshots, container open handles, object class, user attributes, properties, and others) are stored in persistent memory and maintained by a dedicated container metadata service that either uses the same replicated engine as the parent metadata pool service or has its own engine. This is configurable when creating a container. Like a pool, access to a container is controlled by the container handle. To acquire a valid handle, an application process must open the container and pass the security checks. This container handle may then be shared with other peer application processes via the container local2global() and global2local() operations.","title":"DAOS Container"},{"location":"overview/storage/#daos-object","text":"To avoid scaling problems and overhead common to a traditional storage system, DAOS objects are intentionally simple. No default object metadata beyond the type and schema are provided. This means that the system does not maintain time, size, owner, permissions or even track openers. To achieve high availability and horizontal scalability, many object schemas (replication/erasure code, static/dynamic striping, and others) are provided. The schema framework is flexible and easily expandable to allow for new custom schema types in the future. The layout is generated algorithmically on object open from the object identifier and the pool map. End-to-end integrity is assured by protecting object data with checksums during network transfer and storage. A DAOS object can be accessed through different APIs: Multi-level key-array API is the native object interface with locality feature. The key is split into a distribution (i.e., dkey) and an attribute (i.e., akey) key. Both the dkey and akey can be of variable length and type (i.e. a string, an integer or even a complex data structure). All entries under the same dkey are guaranteed to be collocated on the same target. The value associated with akey can be either a single variable-length value that cannot be partially overwritten or an array of fixed-length values. Both the akeys and dkeys support enumeration. Key-value API provides a simple key and variable-length value interface. It supports the traditional put, get, remove and list operations. Array API implements a one-dimensional array of fixed-size elements addressed by a 64-bit offset. A DAOS array supports arbitrary extent read, write and punch operations.","title":"DAOS Object"},{"location":"overview/terminology/","text":"Terminology \u00b6 Acronym Expansion ABT Argobots ACLs Access Control Lists BIO Blob I/O CART Collective and RPC Transport CGO Go tools that enable creation of Go packages that call C code CN Compute Node COTS Commercial off-the-shelf CPU Central Processing Unit Daemon A process offering system-level resources. DAOS Distributed Asynchronous Object Storage DCPM Intel Optane DC Persistent Memory DPDK Data Plane Development Kit dRPC DAOS Remote Procedure Call gRPC gRPC Remote Procedure Calls GURT Gurt Useful Routines and Types HLC Hybrid Logical Clock HLD High-level Design ISA-L Intel Storage Acceleration Library I/O Input/Output KV store Key-Value store libfabric A user-space library that exports the Open Fabrics Interface Mercury A user-space RPC library that can use libfabrics as a transport MTBF Mean Time Between Failures OFI Open Fabrics Interface NVM Non-Volatile Memory NVMe Non-Volatile Memory express OFI OpenFabrics Interfaces OS Operating System PM/PMEM Persistent Memory PMDK Persistent Memory Devevelopment Kit RAFT Raft is a consensus algorithm used to distribute state transitions among DAOS server nodes. RAS Reliability, Availability & Serviceability RDB Replicated Database, containing pool metadata and maintained across DAOS servers using the Raft algorithm. RDMA/RMA Remote (Direct) Memory Access RPC Remote Procedure Call SCM Storage-Class Memory SWIM Scalable Weakly-consistent Infection-style process group Membership SPDK Storage Performance Development Kit SSD Solid State Drive SWIM Scalable Weakly-consistent Infection-style process group Membership protocol ULT User Level Thread UPI Intel Ultra Path Interconnect URT A common library of Gurt Useful Routines and Types provided with CaRT. UUID Universal Unique Identifier RDG Redundancy Group VOS Versioning Object Store","title":"Terminology"},{"location":"overview/terminology/#terminology","text":"Acronym Expansion ABT Argobots ACLs Access Control Lists BIO Blob I/O CART Collective and RPC Transport CGO Go tools that enable creation of Go packages that call C code CN Compute Node COTS Commercial off-the-shelf CPU Central Processing Unit Daemon A process offering system-level resources. DAOS Distributed Asynchronous Object Storage DCPM Intel Optane DC Persistent Memory DPDK Data Plane Development Kit dRPC DAOS Remote Procedure Call gRPC gRPC Remote Procedure Calls GURT Gurt Useful Routines and Types HLC Hybrid Logical Clock HLD High-level Design ISA-L Intel Storage Acceleration Library I/O Input/Output KV store Key-Value store libfabric A user-space library that exports the Open Fabrics Interface Mercury A user-space RPC library that can use libfabrics as a transport MTBF Mean Time Between Failures OFI Open Fabrics Interface NVM Non-Volatile Memory NVMe Non-Volatile Memory express OFI OpenFabrics Interfaces OS Operating System PM/PMEM Persistent Memory PMDK Persistent Memory Devevelopment Kit RAFT Raft is a consensus algorithm used to distribute state transitions among DAOS server nodes. RAS Reliability, Availability & Serviceability RDB Replicated Database, containing pool metadata and maintained across DAOS servers using the Raft algorithm. RDMA/RMA Remote (Direct) Memory Access RPC Remote Procedure Call SCM Storage-Class Memory SWIM Scalable Weakly-consistent Infection-style process group Membership SPDK Storage Performance Development Kit SSD Solid State Drive SWIM Scalable Weakly-consistent Infection-style process group Membership protocol ULT User Level Thread UPI Intel Ultra Path Interconnect URT A common library of Gurt Useful Routines and Types provided with CaRT. UUID Universal Unique Identifier RDG Redundancy Group VOS Versioning Object Store","title":"Terminology"},{"location":"overview/transaction/","text":"Transaction Model \u00b6 The DAOS API supports distributed transactions that allow any update operations against objects belonging to the same container to be combined into a single ACID transaction. Distributed consistency is provided via a lockless optimistic concurrency control mechanism based on multi-version timestamp ordering. DAOS transactions are serializable and can be used on an ad-hoc basis for parts of the datasets that need it. The DAOS versioning mechanism allows creating persistent container snapshots which provide point-in-time distributed consistent views of a container which can be used to build producer-consumer pipeline. Epoch & Timestamp Ordering \u00b6 Each DAOS I/O operation is tagged with a timestamp called epoch. An epoch is a 64-bit integer that integrates both logical and physical clocks (see HLC paper ). The DAOS API provides helper functions to convert an epoch to traditional POSIX time (i.e., struct timespec, see clock_gettime(3)). Container Snapshot \u00b6 As shown in the figure below, the content of a container can be snapshot at any time. DAOS snapshots are very lightweight and are tagged with the epoch associated with the time when the snapshot was created. Once successfully created, a snapshot remains readable until it is explicitly destroyed. The content of a container can be rolled back to a particular snapshot. The container snapshot feature allows supporting native producer/consumer pipeline as represented in the diagram below. The producer will generate a snapshot once a consistent version of the dataset has been successfully written. The consumer applications may subscribe to container snapshot events so that new updates can be processed as the producer commits them. The immutability of the snapshots guarantees that the consumer sees consistent data, even while the producer continues with new updates. Both the producer and consumer indeed operate on different versions of the container and do not need any serialization. Once the producer generates a new version of the dataset, the consumer may query the differences between the two snapshots and process only the incremental changes. Distributed Transactions \u00b6 Unlike POSIX, the DAOS API does not impose any worst-case concurrency control mechanism to address conflicting I/O operations. Instead, individual I/O operations are tagged with a different epoch and applied in epoch order, regardless of execution order. This baseline model delivers the maximum scalability and performance to data models and applications that do not generate conflicting I/O workload. Typical examples are collective MPI-IO operations, POSIX file read/write or HDF5 dataset read/write. For parts of the data model that require conflict serialization, DAOS provides distributed serializable transaction based on multi-version concurrency control. Transactions are typically needed when different user process can overwrite the value associated with a dkey/akey pair. Examples are a SQL database over DAOS or a consistent POSIX namespace accessed concurrently by uncoordinated clients. All I/O operations (include reads) submitted in the context of the same operation will use the same epoch. The DAOS transaction mechanism automatically detects the traditional read/write, write/read and write/write conflicts and aborts one of the conflicting transactions that have to be restarted by the user (i.e., transaction fails to commit with -DER_RESTART). In the initial implementation, the transaction API has the following limitations that will be addressed in future DAOS versions: no support for the array API transactional object update and key-value put operations are not visible via object fetch/list and key-value get/list operations executed in the context of the same transaction.","title":"Transaction Model"},{"location":"overview/transaction/#transaction-model","text":"The DAOS API supports distributed transactions that allow any update operations against objects belonging to the same container to be combined into a single ACID transaction. Distributed consistency is provided via a lockless optimistic concurrency control mechanism based on multi-version timestamp ordering. DAOS transactions are serializable and can be used on an ad-hoc basis for parts of the datasets that need it. The DAOS versioning mechanism allows creating persistent container snapshots which provide point-in-time distributed consistent views of a container which can be used to build producer-consumer pipeline.","title":"Transaction Model"},{"location":"overview/transaction/#epoch-timestamp-ordering","text":"Each DAOS I/O operation is tagged with a timestamp called epoch. An epoch is a 64-bit integer that integrates both logical and physical clocks (see HLC paper ). The DAOS API provides helper functions to convert an epoch to traditional POSIX time (i.e., struct timespec, see clock_gettime(3)).","title":"Epoch &amp; Timestamp Ordering"},{"location":"overview/transaction/#container-snapshot","text":"As shown in the figure below, the content of a container can be snapshot at any time. DAOS snapshots are very lightweight and are tagged with the epoch associated with the time when the snapshot was created. Once successfully created, a snapshot remains readable until it is explicitly destroyed. The content of a container can be rolled back to a particular snapshot. The container snapshot feature allows supporting native producer/consumer pipeline as represented in the diagram below. The producer will generate a snapshot once a consistent version of the dataset has been successfully written. The consumer applications may subscribe to container snapshot events so that new updates can be processed as the producer commits them. The immutability of the snapshots guarantees that the consumer sees consistent data, even while the producer continues with new updates. Both the producer and consumer indeed operate on different versions of the container and do not need any serialization. Once the producer generates a new version of the dataset, the consumer may query the differences between the two snapshots and process only the incremental changes.","title":"Container Snapshot"},{"location":"overview/transaction/#distributed-transactions","text":"Unlike POSIX, the DAOS API does not impose any worst-case concurrency control mechanism to address conflicting I/O operations. Instead, individual I/O operations are tagged with a different epoch and applied in epoch order, regardless of execution order. This baseline model delivers the maximum scalability and performance to data models and applications that do not generate conflicting I/O workload. Typical examples are collective MPI-IO operations, POSIX file read/write or HDF5 dataset read/write. For parts of the data model that require conflict serialization, DAOS provides distributed serializable transaction based on multi-version concurrency control. Transactions are typically needed when different user process can overwrite the value associated with a dkey/akey pair. Examples are a SQL database over DAOS or a consistent POSIX namespace accessed concurrently by uncoordinated clients. All I/O operations (include reads) submitted in the context of the same operation will use the same epoch. The DAOS transaction mechanism automatically detects the traditional read/write, write/read and write/write conflicts and aborts one of the conflicting transactions that have to be restarted by the user (i.e., transaction fails to commit with -DER_RESTART). In the initial implementation, the transaction API has the following limitations that will be addressed in future DAOS versions: no support for the array API transactional object update and key-value put operations are not visible via object fetch/list and key-value get/list operations executed in the context of the same transaction.","title":"Distributed Transactions"},{"location":"overview/use_cases/","text":"Use Cases \u00b6 This section provides a non-exhaustive list of use cases presenting how the DAOS storage model and stack could be used on a real HPC cluster. This document contains the following sections: Storage Management and Workflow Integration Workflow Execution Bulk Synchronous Checkpoint Producer/Consumer Concurrent Producers Storage Node Failure and Resilvering Storage Management & Workflow Integration \u00b6 In this section, we consider two different cluster configurations: Cluster A: All or a majority of the compute nodes have local persistent memory. In other words, each compute node is also a storage node. Cluster B: Storage nodes are dedicated to storage and disseminated across the fabric. They are not used for computation and thus do not run any application code. At boot time, each storage node starts the DAOS server that instantiates service threads. In cluster A, the DAOS threads are bound to the noisy cores and interact with the FWK if mOS is used. In cluster B, the DAOS server can use all the cores of the storage node. The DAOS server then loads the storage management module. This module scans for local storage on the node and reports the result to a designated master DAOS server that aggregates information about the used and available storage across the cluster. The management module also retrieves the fault domain hierarchy (from a database or specific service) and integrates this with the storage information. The resource manager then uses the DAOS management API to query available storage and allocate a certain amount of storage (i.e. persistent memory) for a new workflow that is to be scheduled. In cluster A, this allocation request may list the compute nodes where the workflow is supposed to run, whereas in case B, it may ask for storage nearby some allocated compute nodes. Once successfully allocated, the master server will initialize a DAOS pool covering the allocated storage by formatting the VOS layout (i.e. fallocate(1) a PMEM file & create VOS super block) and starting the pool service which will initiate the Raft engine in charge of the pool membership and metadata. At this point, the DAOS pool is ready to be handed off to the actual workflow. When the workflow starts, one rank connects to the DAOS pool, then uses local2global() to generate a global connection handle and shares it with all the other application ranks that use global2local() to create a local connection handle. At that point, new containers can be created and existing ones opened collectively or individually by the application tasks. Workflow Execution \u00b6 We consider the workflow represented in the figure below. Each green box represents a different container. All containers are stored in the same DAOS pool represented by the grey box. The simulation reads data from the input container and writes raw timesteps to another container. It also regularly dumps checkpoints to a dedicated ckpt container. The down-sample job reads the raw timesteps and generates sampled timesteps to be analyzed by the post-process which stores analysis data into yet another container. Bulk Synchronous Checkpoint \u00b6 Defensive I/O is used to manage a large simulation run over a period of time larger than the platform's mean time between failure (MTBF). The simulation regularly dumps the current computation state to a dedicated container used to guarantee forward progress in the event of failures. This section elaborates on how checkponting could be implemented on top of the DAOS storage stack. We first consider the traditional approach relying on blocking barriers and then a more loosely coupled execution. Blocking Barrier When the simulation job starts, one task opens the checkpoint container and fetches the current global HCE. It thens obtains an epoch hold and shares the data (the container handle, the current LHE and global HCE) with peer tasks. Each task checks for the latest computation state saved to the checkpoint container by reading with an epoch equal to the global HCE and resumes computation from where it was last checkpointed. To checkpoint, each task executes a barrier to synchronize with the other tasks, writes its current computation state to the checkpoint container at epoch LHE, flushes all updates and finally executes another barrier. Once all tasks have completed the last barrier, one designated task (e.g. rank 0) commits the LHE which is then increased by one on successful commit. This process is repeated regularly until the simulation successfully completes. Non-blocking Barrier We now consider another approach to checkpointing where the execution is more loosely coupled. As in the previous case, one task is responsible for opening the checkpoint container, fetching the global HCE, obtaining an epoch hold and sharing the data with the other peer tasks. However, tasks can now checkpoint their computation state at their own pace without waiting for each other. After the computation of N timesteps, each task dumps its state to the checkpoint container at epoch LHE+1, flushes the changes and calls a non-blocking barrier (e.g. MPI_Ibarrier()) once done. Then after another N timesteps, the new checkpoint is written with epoch LHE+2 and so on. For each checkpoint, the epoch number is incremented. Moreover, each task regularly calls MPI_Test() to check for barrier completion which allows them to recycle the MPI_Request. Upon barrier completion, one designated task (typically rank 0) also commits the associated epoch number. All epochs are guaranteed to be committed in sequence and each committed epoch is a new consistent checkpoint to restart from. On failure, checkpointed states that have been written by individual tasks, but not committed, are automatically rolled back. Producer/Consumer \u00b6 In the previous figure , we have two examples of producer/consumer. The down-sample job consumes raw timesteps generated by the simulation job and produces sampled timesteps analysed by the post-process job. The DAOS stack provides specific mechanims for producer/consumer workflow which even allows the consumer to dumps the result of its analysis into the same container as the producer. Private Container The down-sample job opens the sampled timesteps container, fetchs the current global HCE, obtains an epoch hold and writes new sampled data to this container at epoch LHE. While this is occurring, the post process job opens the container storing analyzed data for write, checks for the latest analyzed timesteps and obtains an epoch hold on this container. It then opens the sampled timesteps container for read, and checks whether the next time-step to be consumed is ready. If not, it waits for a new global HCE to be committed (notified by asynchronous event completion on the event queue) and checks again. When the requested time-step is available, the down-sample job processes input data for this new time-step, dumps the results in its own container and updates the latest analyzed time-step in its metadata. It then commits updates to its output container and waits again for a new epoch to be committed and repeats the same process. Another approach is for the producer job to create explicit snapshots for epochs of interest and have the analysis job waiting and processing snapshots. This avoid processing every single committed epoch. Shared Container We now assume that the container storing the sampled timesteps and the one storing the analyzed data are a single container. In other words, the down-sample job consumes input data and writes output data to the same container. The down-sample job opens the shared container, obtains an hold and dumps new sampled timesteps to the container. As before, the post-process job also opens the container, fetches the latest analyzed timestep, but does not obtain an epoch hold until a new global HCE is ready. Once the post-process job is notified of a new global HCE, it can analyze the new sampled timesteps, obtain an hold and write its analysed data to the same container. Once this is done, the post-process job flushes its updates, commits the held epoch and releases the held epoch. At that point, it can wait again for a new global HCE to be generated by the down-sample job. Concurrent Producers \u00b6 In the previous section, we consider a producer and a consumer job concurrently reading and writing into the same container, but in disjoint objects. We now consider a workflow composed of concurrent producer jobs modifying the same container in a conflicting and uncoordinated manner. This effectively means that the two producers can update the same key of the same KV object or document store or overlapping extents of the same byte array. This model requires the implementation of a concurrency-control mechanism (not part of DAOS) to coordinate conflicting accesses. This section presents an example of such a mechanism based on locking, but alternative approaches can also be considered. A workflow is composed of two applications using a distributed lock manager to serialize contended accesses to DAOS objects. Each application individually opens the same container and grabs an epoch hold whenever it wants to modify some objects in the container. Prior to modifying an object, an application should acquire a write lock on the object. This lock carries a lock value block (LVB) storing the last epoch number in which this object was last modified and committed. Once the lock is acquired, the writer must: read from an epoch equal to the greatest of the epoch specified in the LVB and the handle LRE. submit new writes with an epoch higher than the one in the LVB and the currently held epoch. After all the I/O operations have been completed, flushed, and committed by the application, the LVB is updated with the committed epoch in which the object was modified, and the lock can finally be released. Storage Node Failure and Resilvering \u00b6 In this section, we consider a workflow connected to a DAOS pool and one storage node that suddenly fails. Both DAOS clients and servers communicating with the failed server experience RPC timeouts and inform the RAS system. Failing RPCs are resent repeatedly until the RAS system or the pool metadata service itself decides to declare the storage node dead and evicts it from the pool map. The pool map update, along with the new version, is propagated to all the storage nodes that lazily (in RPC replies) inform clients that a new pool map version is available. Both clients and servers are thus eventually informed of the failure and enter into recovery mode. Server nodes will cooperate to restore redundancy on different servers for the impacted objects, whereas clients will enter in degraded mode and read from other replicas, or reconstruct data from erasure code. This rebuild process is executed online while the container is still being accessed and modified. Once redundancy has been restored for all objects, the poolmap is updated again to inform everyone that the system has recovered from the fault and the system can exit from degraded mode.","title":"Use Cases"},{"location":"overview/use_cases/#use-cases","text":"This section provides a non-exhaustive list of use cases presenting how the DAOS storage model and stack could be used on a real HPC cluster. This document contains the following sections: Storage Management and Workflow Integration Workflow Execution Bulk Synchronous Checkpoint Producer/Consumer Concurrent Producers Storage Node Failure and Resilvering","title":"Use Cases"},{"location":"overview/use_cases/#storage-management-workflow-integration","text":"In this section, we consider two different cluster configurations: Cluster A: All or a majority of the compute nodes have local persistent memory. In other words, each compute node is also a storage node. Cluster B: Storage nodes are dedicated to storage and disseminated across the fabric. They are not used for computation and thus do not run any application code. At boot time, each storage node starts the DAOS server that instantiates service threads. In cluster A, the DAOS threads are bound to the noisy cores and interact with the FWK if mOS is used. In cluster B, the DAOS server can use all the cores of the storage node. The DAOS server then loads the storage management module. This module scans for local storage on the node and reports the result to a designated master DAOS server that aggregates information about the used and available storage across the cluster. The management module also retrieves the fault domain hierarchy (from a database or specific service) and integrates this with the storage information. The resource manager then uses the DAOS management API to query available storage and allocate a certain amount of storage (i.e. persistent memory) for a new workflow that is to be scheduled. In cluster A, this allocation request may list the compute nodes where the workflow is supposed to run, whereas in case B, it may ask for storage nearby some allocated compute nodes. Once successfully allocated, the master server will initialize a DAOS pool covering the allocated storage by formatting the VOS layout (i.e. fallocate(1) a PMEM file & create VOS super block) and starting the pool service which will initiate the Raft engine in charge of the pool membership and metadata. At this point, the DAOS pool is ready to be handed off to the actual workflow. When the workflow starts, one rank connects to the DAOS pool, then uses local2global() to generate a global connection handle and shares it with all the other application ranks that use global2local() to create a local connection handle. At that point, new containers can be created and existing ones opened collectively or individually by the application tasks.","title":"Storage Management &amp; Workflow Integration"},{"location":"overview/use_cases/#workflow-execution","text":"We consider the workflow represented in the figure below. Each green box represents a different container. All containers are stored in the same DAOS pool represented by the grey box. The simulation reads data from the input container and writes raw timesteps to another container. It also regularly dumps checkpoints to a dedicated ckpt container. The down-sample job reads the raw timesteps and generates sampled timesteps to be analyzed by the post-process which stores analysis data into yet another container.","title":"Workflow Execution"},{"location":"overview/use_cases/#bulk-synchronous-checkpoint","text":"Defensive I/O is used to manage a large simulation run over a period of time larger than the platform's mean time between failure (MTBF). The simulation regularly dumps the current computation state to a dedicated container used to guarantee forward progress in the event of failures. This section elaborates on how checkponting could be implemented on top of the DAOS storage stack. We first consider the traditional approach relying on blocking barriers and then a more loosely coupled execution. Blocking Barrier When the simulation job starts, one task opens the checkpoint container and fetches the current global HCE. It thens obtains an epoch hold and shares the data (the container handle, the current LHE and global HCE) with peer tasks. Each task checks for the latest computation state saved to the checkpoint container by reading with an epoch equal to the global HCE and resumes computation from where it was last checkpointed. To checkpoint, each task executes a barrier to synchronize with the other tasks, writes its current computation state to the checkpoint container at epoch LHE, flushes all updates and finally executes another barrier. Once all tasks have completed the last barrier, one designated task (e.g. rank 0) commits the LHE which is then increased by one on successful commit. This process is repeated regularly until the simulation successfully completes. Non-blocking Barrier We now consider another approach to checkpointing where the execution is more loosely coupled. As in the previous case, one task is responsible for opening the checkpoint container, fetching the global HCE, obtaining an epoch hold and sharing the data with the other peer tasks. However, tasks can now checkpoint their computation state at their own pace without waiting for each other. After the computation of N timesteps, each task dumps its state to the checkpoint container at epoch LHE+1, flushes the changes and calls a non-blocking barrier (e.g. MPI_Ibarrier()) once done. Then after another N timesteps, the new checkpoint is written with epoch LHE+2 and so on. For each checkpoint, the epoch number is incremented. Moreover, each task regularly calls MPI_Test() to check for barrier completion which allows them to recycle the MPI_Request. Upon barrier completion, one designated task (typically rank 0) also commits the associated epoch number. All epochs are guaranteed to be committed in sequence and each committed epoch is a new consistent checkpoint to restart from. On failure, checkpointed states that have been written by individual tasks, but not committed, are automatically rolled back.","title":"Bulk Synchronous Checkpoint"},{"location":"overview/use_cases/#producerconsumer","text":"In the previous figure , we have two examples of producer/consumer. The down-sample job consumes raw timesteps generated by the simulation job and produces sampled timesteps analysed by the post-process job. The DAOS stack provides specific mechanims for producer/consumer workflow which even allows the consumer to dumps the result of its analysis into the same container as the producer. Private Container The down-sample job opens the sampled timesteps container, fetchs the current global HCE, obtains an epoch hold and writes new sampled data to this container at epoch LHE. While this is occurring, the post process job opens the container storing analyzed data for write, checks for the latest analyzed timesteps and obtains an epoch hold on this container. It then opens the sampled timesteps container for read, and checks whether the next time-step to be consumed is ready. If not, it waits for a new global HCE to be committed (notified by asynchronous event completion on the event queue) and checks again. When the requested time-step is available, the down-sample job processes input data for this new time-step, dumps the results in its own container and updates the latest analyzed time-step in its metadata. It then commits updates to its output container and waits again for a new epoch to be committed and repeats the same process. Another approach is for the producer job to create explicit snapshots for epochs of interest and have the analysis job waiting and processing snapshots. This avoid processing every single committed epoch. Shared Container We now assume that the container storing the sampled timesteps and the one storing the analyzed data are a single container. In other words, the down-sample job consumes input data and writes output data to the same container. The down-sample job opens the shared container, obtains an hold and dumps new sampled timesteps to the container. As before, the post-process job also opens the container, fetches the latest analyzed timestep, but does not obtain an epoch hold until a new global HCE is ready. Once the post-process job is notified of a new global HCE, it can analyze the new sampled timesteps, obtain an hold and write its analysed data to the same container. Once this is done, the post-process job flushes its updates, commits the held epoch and releases the held epoch. At that point, it can wait again for a new global HCE to be generated by the down-sample job.","title":"Producer/Consumer"},{"location":"overview/use_cases/#concurrent-producers","text":"In the previous section, we consider a producer and a consumer job concurrently reading and writing into the same container, but in disjoint objects. We now consider a workflow composed of concurrent producer jobs modifying the same container in a conflicting and uncoordinated manner. This effectively means that the two producers can update the same key of the same KV object or document store or overlapping extents of the same byte array. This model requires the implementation of a concurrency-control mechanism (not part of DAOS) to coordinate conflicting accesses. This section presents an example of such a mechanism based on locking, but alternative approaches can also be considered. A workflow is composed of two applications using a distributed lock manager to serialize contended accesses to DAOS objects. Each application individually opens the same container and grabs an epoch hold whenever it wants to modify some objects in the container. Prior to modifying an object, an application should acquire a write lock on the object. This lock carries a lock value block (LVB) storing the last epoch number in which this object was last modified and committed. Once the lock is acquired, the writer must: read from an epoch equal to the greatest of the epoch specified in the LVB and the handle LRE. submit new writes with an epoch higher than the one in the LVB and the currently held epoch. After all the I/O operations have been completed, flushed, and committed by the application, the LVB is updated with the committed epoch in which the object was modified, and the lock can finally be released.","title":"Concurrent Producers"},{"location":"overview/use_cases/#storage-node-failure-and-resilvering","text":"In this section, we consider a workflow connected to a DAOS pool and one storage node that suddenly fails. Both DAOS clients and servers communicating with the failed server experience RPC timeouts and inform the RAS system. Failing RPCs are resent repeatedly until the RAS system or the pool metadata service itself decides to declare the storage node dead and evicts it from the pool map. The pool map update, along with the new version, is propagated to all the storage nodes that lazily (in RPC replies) inform clients that a new pool map version is available. Both clients and servers are thus eventually informed of the failure and enter into recovery mode. Server nodes will cooperate to restore redundancy on different servers for the impacted objects, whereas clients will enter in degraded mode and read from other replicas, or reconstruct data from erasure code. This rebuild process is executed online while the container is still being accessed and modified. Once redundancy has been restored for all objects, the poolmap is updated again to inform everyone that the system has recovered from the fault and the system can exit from degraded mode.","title":"Storage Node Failure and Resilvering"},{"location":"release/releaseNote_v1_0/","text":"DAOS Version 1.0 Release Notes \u00b6 We are pleased to announce the release of DAOS version 1.0, a key DAOS milestone focused on a newly created high-performance object store that is ultimately shifting the HPC paradigm. Recently awarded the top spot in the IO500 10-node Challenge , DAOS is fully optimized for Intel\u00ae architecture and non-volatile memory (NVM). This release is targeted towards benchmarking, partner integration, evaluation, and brings support for the following features: NVMe & DCPMM Support Per-pool ACL Unified Namespace via dfuse MPI-IO Driver HDF5 Support Basic POSIX I/O Support Replication & Self-Healing (Preview) Certificate Support This release is not yet intended for production use. DAOS is an open-source, software-defined storage (SDS) and storage-as-a-service (STaaS) platform. This technology was developed from the ground up for nextgen NVM technologies like Intel\u00ae 3D NAND Technology and Intel\u00ae Optane\u2122 technology. DAOS operates end-to-end in user-space for ease of deployment and to maximize IOPS, bandwidth, and minimize latency. DAOS is a scale-out solution allowing small and medium deployments to grow as much as required (i.e., 100\u2019s of PB with millions of client nodes and thousands of storage servers) to serve the most demanding scientific, Big Data and AI workloads. The primary storage for the first US Exascale cluster (The Aurora system at Argonne National Laboratory - ANL) will be a 230PB DAOS tier composed to deliver an aggregated bandwidth of at least 25TB/s. Intel is collaborating closely with different partners to propose DAOS-based solutions for production deployments. DAOS is gaining a lot of traction in the enterprise and hyper scaling clients. Designed to address evolving storage needs, DAOS is a new storage platform the convergence of traditional modeling and simulation, data science analytics, and artificial intelligence. Visit the DAOS github.io page for more information. All DAOS project source code is maintained in the https://github.com/daos-stack/daos repository. Software Dependencies \u00b6 Reference the Software Installation section of the DAOS Administration Guide for software requirements. Distribution Packages \u00b6 DAOS RPMs are currently being developed for DAOS v1.0. Instructions for obtaining the RPMs will be provided in this document at a future date, and in the DAOS mailing list . Build Prerequisites \u00b6 To build DAOS and its dependencies, several software packages must be installed on the system. This includes scons, libuuid, cmocka, ipmctl, and several other packages usually available on all the Linux distributions. Moreover, a Go version of at least 1.10 is required. CentOS is currently the only supported Linux distribution. An exhaustive list of packages for each supported Linux distribution is maintained in the Docker files. Refer to the Software Installation section of the DAOS Administration Guide for more details. Hardware Support \u00b6 Processor Requirements \u00b6 DAOS requires a 64-bit processor architecture and is primarily developed on Intel 64 architecture. The DAOS software and the libraries it depends on (e.g., ISA-L, SPDK, PMDK, and DPDK) can take advantage of Intel SSE and AVX extensions. DAOS is also regularly tested on 64-bit ARM processors configured in Little Endian mode. The same build instructions that are used for x86-64 are applicable for ARM builds as well. DAOS and its dependencies will make the necessary adjustments automatically in their respective build systems for ARM platforms. Network Requirements\u00b6 \u00b6 The DAOS network layer relies on libfabrics and supports OFI providers for Ethernet/sockets, InfiniBand/verbs,. An RDMA-capable fabric is preferred for better performance. DAOS can support multiple rails by binding different instances of the DAOS server to individual network cards. The DAOS control plane provides methods for administering and managing the DAOS servers using a secure socket layer interface. An additional out-of-band network connecting the nodes in the DAOS service cluster is required for DAOS administration. Management traffic between clients and servers use IP over Fabric. Storage Requirements\u00b6 \u00b6 DAOS requires each storage node to have direct access to storage-class memory (SCM). While DAOS is primarily tested and tuned for Intel\u00a9 Optane\u2122 Persistent Memory, the DAOS software stack is built over the Persistent Memory Development Kit (PMDK) and the DAX feature of the Linux operating systems as described in the SNIA NVM Programming Model1. As a result, the open-source DAOS software stack should be able to run transparently over any storage-class memory supported by the PMDK. The storage node should be equipped with NVMe (non-volatile memory express) SSDs to provide capacity. HDDs, as well as SATA and SAS SSDs, are not supported by DAOS. Both NVMe 3D-NAND and Optane SSDs are supported.. NVMe-oF devices are also supported by the user-space storage stack but have never been tested. A recommended ratio of 6% SCM to SSD capacity will guarantee that DAOS has enough space in SCM to store its internal metadata (e.g., pool metadata, SSD block allocation tracking). For testing purposes, SCM can be emulated with DRAM by mounting a tmpfs filesystem, and NVMe SSDs can be also emulated with DRAM or a loopback file. DAOS Testing \u00b6 DAOS 1.0 validation efforts were directed towards anticipated initial use cases including: system integration, application porting, benchmarking, initial evaluations, and the like. This release is not intended for large scale production use and testing to date has not focused on use cases or hardware typically found in HPC production environments. Testing has been completed in the following areas: Testing has been performed on Centos 7.7 and SLES 15 with Centos being used in the majority of the test cycles. Testing has been conducted using Intel Xeon processors, Intel 3D NAND and Optane based NVMe SSDs and Optane persistent memory modules although generally storage density has not been at production levels. DAOS uses the libfabric network abstraction layer and testing has been performed on a number of network specific providers, including the IB verbs, OPA PSM2, socket and TCP providers. Network testing is on-going and none of the above providers should be considered fully tested at this time. All DAOS 1.0 supported functionality has been tested with an emphasis on use cases with positive outcomes \u2013error cases (e.g. DAOS server failure) have limited test cycles at this time. Maximum scale-out of DAOS servers during test runs was 128. Maximum scale-out of DAOS clients was 2048. Soak testing with an emphasis on I/O jobs in combination with basic administrative actions has been run and found to be error free for periods up to 48 hours. As with functional testing the focus has been on positive path testing with failure paths and fault injection largely relegated to a future release. Version 1.0 major features \u00b6 NVMe and DCPMM Support \u00b6 DAOS supports two tiers of storage: Storage Class Memory (SCM) and NVMe SSDs. Each DAOS server will be equipped with SCM (for byte-granular application data and metadata) along with NVMe SSDs (for DAOS application bulk data, ie >4KB). Similar to how PMDK is used to facilitate access to SCM, the Storage Performance Development Kit (SPDK) is used to provide seamless and efficient access to NVMe SSDs. DAOS utilizes the significant performance increase of the SPDK user space NVMe driver over the standard NVMe kernel driver. As a part of the NVMe support, an extent-based block allocator was implemented and designed specifically for DAOS NVMe block device space management, including efficient management of smaller 4KB block allocations. A server module was also implemented for issuing I/O over NVMe SSDs. This involves internally managing a per-xstream DMA-safe buffer for SPDK DMA transfer over NVMe SSDs. The module also persistently tracks important server metadata. This per-server metadata includes the health state of each NVMe SSD, as well as the mapping between NVMe SSDs, DAOS server xstreams, DAOS pools, and allocated blocks IDs. Two other key components include faulty device detection and device health monitoring. DAOS handles storing NVMe SSD health data, including raw NVMe device health as well as I/O error and checksum errors counters. If and when an I/O error occurs, an event notification will be sent to the console to notify the administrator. Management utility commands are also available for administrators to query all NVMe device health stats to gauge the general health of the system. DAOS currently only supports manual faulty device events, with future work including auto-detection upon configurable faulty criteria. This would involve an administrator manually setting the device state of an NVMe SSD to \u201cFAULTY\u201d using the management utility, which will trigger the appropriate rebuild of data and exclusion of the faulty device from the system. Reintegration and hotplug of NVMe SSDs are not currently supported in DAOS 1.0, but will be a part of a future release. MPI-IO ROMIO Driver \u00b6 The MPI standard includes specification for a low level IO interface for parallel access to files. The MPI-IO standard loosens some of the POSIX semantics that are not required for many applications and defines routines and methods for parallel access to files from processes in an MPI communicator. Several existing applications and other high level middleware IO libraries use the MPI-IO standard as a backend for data access. ROMIO is the de-facto MPI-IO implementation that is released with the MPICH library and is used by most MPI implementations to support the MPI-IO standard. ROMIO exposes an ADIO module interface for different backends. A DAOS backend was implemented as an alternative to the POSIX backend for direct user-space access to the DAOS storage stack through the MPI-IO interface. The DAOS ROMIO driver is distributed with the MPICH ROMIO source code. HDF5 Support \u00b6 HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. This DAOS release supports the HDF5 format, API, and tools through the HDF5 (sec2) POSIX backend or the HDF5 MPI-IO backend. HDF5 also provides a new DAOS VOL for mapping the HDF5 data model and API directly over the DAOS data model, bypassing the byte array serialization required over the POSIX and MPI-IO backends. The HDF5 DAOS VOL is not fully supported in this DAOS release and should be used as a prototype for basic testing of this new HDF5 feature. Basic POSIX I/O Support \u00b6 POSIX IO is the main building block for all applications and IO libraries. The DAOS library provides POSIX support through the DFS (DAOS File System) library. The DFS API provides an encapsulated namespace with a POSIX like API directly on top of the DAOS API. Applications can link directly to the DFS library and use the DFS API for direct user-space access to the DAOS stack through the DFS API. A FUSE plugin over the DFS library (dfuse) is also provided to support direct access to POSIX calls through FUSE over the DAOS stack. The primary support provided in this release provides loose POSIX consistency in terms of metadata and assumes applications generate conflict-free operations, otherwise, the behavior is undefined. FUSE adds stricter consistency on top of the DFS layer, but that consistency is limited to a node local instance of the dfuse mount. Replication and self-healing (preview) \u00b6 In DAOS, if the data is replicated with multiple copies on different targets, once one of the targets fail, the data on it will be rebuilt on the other targets. This reduces the data redundancy that would be impacted by the target failure. Self-healing in DAOS 1.0 is a preview feature, and it will not be enabled automatically when targets are failed; instead it can only be enabled manually by the dmg exclude command. In future versions, DAOS will support erasure coding to protect the data. At that time, the rebuild process will be updated accordingly. Unified NameSpace (UNS) in DAOS via dfuse \u00b6 Unified NameSpace is the ability to create relationships linking paths in the namespace tree to other DAOS containers, allowing greater flexibility of use and new workflows. UNS entry points can be created that behave in a similar way to hard links or submounts where an attempt to traverse the entry point via dfuse or DAOS aware tools will automatically and seamlessly follow the link to the specified container. UNS entry points are created by providing a path to the DAOS container create command, and can exist either within existing containers or at locations within the regular POSIX filesystem. Per-Pool ACL \u00b6 Access to DAOS data is controlled through a simple identity based access control mechanism specified on a per pool basis. The access credentials for a DAOS client process are determined by the agent running on the compute node. This credential is validated against the Access Control List present on the DAOS server to determine the level of access granted to a specific user. The ACL mechanism support access permissions on an individual, and group basis as well as a mechanism for supporting fall through users. As of DAOS 1.0 the set of permissions on a pool are very simple but will be extended when container based ACLs are introduced in a future version. Certificate Support \u00b6 Securing transport of data and authorizing actions by components make up the backbone of the security model used in the DAOS control plane. All control plane components communicate with each other using mutual TLS authentication(mTLS) backed by certificates signed by a per cluster certificate authority. In addition to securing the communications between components certificates are also used to authenticate the various components and ensure that requests only come from components who are authorized to perform those actions. Certificates are also used to secure the administrative interface to the cluster ensuring only admins with the proper credentials can perform administrative actions. Document updates \u00b6 All documents supporting the https://daos-stack.github.io/ site have been refreshed and re-organized for this release. Design documents in the DAOS Source repository have also been refreshed for this release. Known Issues \u00b6 DAOS-4020 - dmg storage scan reports incorrect NUMA socket ID The ndctl package maintainers have confirmed that this is a kernel regression that has been resolved in CentOS 8. Some DAOS nodes may not report the correct NUMA socket ID when running a \"dmg storage scan.\" This appears to be a regression in the CentOS7.7 kernel rather than an ndctl issue. This is due to different versions of ndctl provisioning different JSON namespace details (storage scan command reads the \"numa_node\" field). Fixed Issues Details \u00b6 First release, this section will be updated in subsequent releases Change Log No changes for this first release.","title":"Release Notes"},{"location":"release/releaseNote_v1_0/#daos-version-10-release-notes","text":"We are pleased to announce the release of DAOS version 1.0, a key DAOS milestone focused on a newly created high-performance object store that is ultimately shifting the HPC paradigm. Recently awarded the top spot in the IO500 10-node Challenge , DAOS is fully optimized for Intel\u00ae architecture and non-volatile memory (NVM). This release is targeted towards benchmarking, partner integration, evaluation, and brings support for the following features: NVMe & DCPMM Support Per-pool ACL Unified Namespace via dfuse MPI-IO Driver HDF5 Support Basic POSIX I/O Support Replication & Self-Healing (Preview) Certificate Support This release is not yet intended for production use. DAOS is an open-source, software-defined storage (SDS) and storage-as-a-service (STaaS) platform. This technology was developed from the ground up for nextgen NVM technologies like Intel\u00ae 3D NAND Technology and Intel\u00ae Optane\u2122 technology. DAOS operates end-to-end in user-space for ease of deployment and to maximize IOPS, bandwidth, and minimize latency. DAOS is a scale-out solution allowing small and medium deployments to grow as much as required (i.e., 100\u2019s of PB with millions of client nodes and thousands of storage servers) to serve the most demanding scientific, Big Data and AI workloads. The primary storage for the first US Exascale cluster (The Aurora system at Argonne National Laboratory - ANL) will be a 230PB DAOS tier composed to deliver an aggregated bandwidth of at least 25TB/s. Intel is collaborating closely with different partners to propose DAOS-based solutions for production deployments. DAOS is gaining a lot of traction in the enterprise and hyper scaling clients. Designed to address evolving storage needs, DAOS is a new storage platform the convergence of traditional modeling and simulation, data science analytics, and artificial intelligence. Visit the DAOS github.io page for more information. All DAOS project source code is maintained in the https://github.com/daos-stack/daos repository.","title":"DAOS Version 1.0 Release Notes"},{"location":"release/releaseNote_v1_0/#software-dependencies","text":"Reference the Software Installation section of the DAOS Administration Guide for software requirements.","title":"Software Dependencies"},{"location":"release/releaseNote_v1_0/#distribution-packages","text":"DAOS RPMs are currently being developed for DAOS v1.0. Instructions for obtaining the RPMs will be provided in this document at a future date, and in the DAOS mailing list .","title":"Distribution Packages"},{"location":"release/releaseNote_v1_0/#build-prerequisites","text":"To build DAOS and its dependencies, several software packages must be installed on the system. This includes scons, libuuid, cmocka, ipmctl, and several other packages usually available on all the Linux distributions. Moreover, a Go version of at least 1.10 is required. CentOS is currently the only supported Linux distribution. An exhaustive list of packages for each supported Linux distribution is maintained in the Docker files. Refer to the Software Installation section of the DAOS Administration Guide for more details.","title":"Build Prerequisites"},{"location":"release/releaseNote_v1_0/#hardware-support","text":"","title":"Hardware Support"},{"location":"release/releaseNote_v1_0/#processor-requirements","text":"DAOS requires a 64-bit processor architecture and is primarily developed on Intel 64 architecture. The DAOS software and the libraries it depends on (e.g., ISA-L, SPDK, PMDK, and DPDK) can take advantage of Intel SSE and AVX extensions. DAOS is also regularly tested on 64-bit ARM processors configured in Little Endian mode. The same build instructions that are used for x86-64 are applicable for ARM builds as well. DAOS and its dependencies will make the necessary adjustments automatically in their respective build systems for ARM platforms.","title":"Processor Requirements"},{"location":"release/releaseNote_v1_0/#network-requirements","text":"The DAOS network layer relies on libfabrics and supports OFI providers for Ethernet/sockets, InfiniBand/verbs,. An RDMA-capable fabric is preferred for better performance. DAOS can support multiple rails by binding different instances of the DAOS server to individual network cards. The DAOS control plane provides methods for administering and managing the DAOS servers using a secure socket layer interface. An additional out-of-band network connecting the nodes in the DAOS service cluster is required for DAOS administration. Management traffic between clients and servers use IP over Fabric.","title":"Network Requirements\u00b6"},{"location":"release/releaseNote_v1_0/#storage-requirements","text":"DAOS requires each storage node to have direct access to storage-class memory (SCM). While DAOS is primarily tested and tuned for Intel\u00a9 Optane\u2122 Persistent Memory, the DAOS software stack is built over the Persistent Memory Development Kit (PMDK) and the DAX feature of the Linux operating systems as described in the SNIA NVM Programming Model1. As a result, the open-source DAOS software stack should be able to run transparently over any storage-class memory supported by the PMDK. The storage node should be equipped with NVMe (non-volatile memory express) SSDs to provide capacity. HDDs, as well as SATA and SAS SSDs, are not supported by DAOS. Both NVMe 3D-NAND and Optane SSDs are supported.. NVMe-oF devices are also supported by the user-space storage stack but have never been tested. A recommended ratio of 6% SCM to SSD capacity will guarantee that DAOS has enough space in SCM to store its internal metadata (e.g., pool metadata, SSD block allocation tracking). For testing purposes, SCM can be emulated with DRAM by mounting a tmpfs filesystem, and NVMe SSDs can be also emulated with DRAM or a loopback file.","title":"Storage Requirements\u00b6"},{"location":"release/releaseNote_v1_0/#daos-testing","text":"DAOS 1.0 validation efforts were directed towards anticipated initial use cases including: system integration, application porting, benchmarking, initial evaluations, and the like. This release is not intended for large scale production use and testing to date has not focused on use cases or hardware typically found in HPC production environments. Testing has been completed in the following areas: Testing has been performed on Centos 7.7 and SLES 15 with Centos being used in the majority of the test cycles. Testing has been conducted using Intel Xeon processors, Intel 3D NAND and Optane based NVMe SSDs and Optane persistent memory modules although generally storage density has not been at production levels. DAOS uses the libfabric network abstraction layer and testing has been performed on a number of network specific providers, including the IB verbs, OPA PSM2, socket and TCP providers. Network testing is on-going and none of the above providers should be considered fully tested at this time. All DAOS 1.0 supported functionality has been tested with an emphasis on use cases with positive outcomes \u2013error cases (e.g. DAOS server failure) have limited test cycles at this time. Maximum scale-out of DAOS servers during test runs was 128. Maximum scale-out of DAOS clients was 2048. Soak testing with an emphasis on I/O jobs in combination with basic administrative actions has been run and found to be error free for periods up to 48 hours. As with functional testing the focus has been on positive path testing with failure paths and fault injection largely relegated to a future release.","title":"DAOS Testing"},{"location":"release/releaseNote_v1_0/#version-10-major-features","text":"","title":"Version 1.0 major features"},{"location":"release/releaseNote_v1_0/#nvme-and-dcpmm-support","text":"DAOS supports two tiers of storage: Storage Class Memory (SCM) and NVMe SSDs. Each DAOS server will be equipped with SCM (for byte-granular application data and metadata) along with NVMe SSDs (for DAOS application bulk data, ie >4KB). Similar to how PMDK is used to facilitate access to SCM, the Storage Performance Development Kit (SPDK) is used to provide seamless and efficient access to NVMe SSDs. DAOS utilizes the significant performance increase of the SPDK user space NVMe driver over the standard NVMe kernel driver. As a part of the NVMe support, an extent-based block allocator was implemented and designed specifically for DAOS NVMe block device space management, including efficient management of smaller 4KB block allocations. A server module was also implemented for issuing I/O over NVMe SSDs. This involves internally managing a per-xstream DMA-safe buffer for SPDK DMA transfer over NVMe SSDs. The module also persistently tracks important server metadata. This per-server metadata includes the health state of each NVMe SSD, as well as the mapping between NVMe SSDs, DAOS server xstreams, DAOS pools, and allocated blocks IDs. Two other key components include faulty device detection and device health monitoring. DAOS handles storing NVMe SSD health data, including raw NVMe device health as well as I/O error and checksum errors counters. If and when an I/O error occurs, an event notification will be sent to the console to notify the administrator. Management utility commands are also available for administrators to query all NVMe device health stats to gauge the general health of the system. DAOS currently only supports manual faulty device events, with future work including auto-detection upon configurable faulty criteria. This would involve an administrator manually setting the device state of an NVMe SSD to \u201cFAULTY\u201d using the management utility, which will trigger the appropriate rebuild of data and exclusion of the faulty device from the system. Reintegration and hotplug of NVMe SSDs are not currently supported in DAOS 1.0, but will be a part of a future release.","title":"NVMe and DCPMM Support"},{"location":"release/releaseNote_v1_0/#mpi-io-romio-driver","text":"The MPI standard includes specification for a low level IO interface for parallel access to files. The MPI-IO standard loosens some of the POSIX semantics that are not required for many applications and defines routines and methods for parallel access to files from processes in an MPI communicator. Several existing applications and other high level middleware IO libraries use the MPI-IO standard as a backend for data access. ROMIO is the de-facto MPI-IO implementation that is released with the MPICH library and is used by most MPI implementations to support the MPI-IO standard. ROMIO exposes an ADIO module interface for different backends. A DAOS backend was implemented as an alternative to the POSIX backend for direct user-space access to the DAOS storage stack through the MPI-IO interface. The DAOS ROMIO driver is distributed with the MPICH ROMIO source code.","title":"MPI-IO ROMIO Driver"},{"location":"release/releaseNote_v1_0/#hdf5-support","text":"HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. This DAOS release supports the HDF5 format, API, and tools through the HDF5 (sec2) POSIX backend or the HDF5 MPI-IO backend. HDF5 also provides a new DAOS VOL for mapping the HDF5 data model and API directly over the DAOS data model, bypassing the byte array serialization required over the POSIX and MPI-IO backends. The HDF5 DAOS VOL is not fully supported in this DAOS release and should be used as a prototype for basic testing of this new HDF5 feature.","title":"HDF5 Support"},{"location":"release/releaseNote_v1_0/#basic-posix-io-support","text":"POSIX IO is the main building block for all applications and IO libraries. The DAOS library provides POSIX support through the DFS (DAOS File System) library. The DFS API provides an encapsulated namespace with a POSIX like API directly on top of the DAOS API. Applications can link directly to the DFS library and use the DFS API for direct user-space access to the DAOS stack through the DFS API. A FUSE plugin over the DFS library (dfuse) is also provided to support direct access to POSIX calls through FUSE over the DAOS stack. The primary support provided in this release provides loose POSIX consistency in terms of metadata and assumes applications generate conflict-free operations, otherwise, the behavior is undefined. FUSE adds stricter consistency on top of the DFS layer, but that consistency is limited to a node local instance of the dfuse mount.","title":"Basic POSIX I/O Support"},{"location":"release/releaseNote_v1_0/#replication-and-self-healing-preview","text":"In DAOS, if the data is replicated with multiple copies on different targets, once one of the targets fail, the data on it will be rebuilt on the other targets. This reduces the data redundancy that would be impacted by the target failure. Self-healing in DAOS 1.0 is a preview feature, and it will not be enabled automatically when targets are failed; instead it can only be enabled manually by the dmg exclude command. In future versions, DAOS will support erasure coding to protect the data. At that time, the rebuild process will be updated accordingly.","title":"Replication and self-healing (preview)"},{"location":"release/releaseNote_v1_0/#unified-namespace-uns-in-daos-via-dfuse","text":"Unified NameSpace is the ability to create relationships linking paths in the namespace tree to other DAOS containers, allowing greater flexibility of use and new workflows. UNS entry points can be created that behave in a similar way to hard links or submounts where an attempt to traverse the entry point via dfuse or DAOS aware tools will automatically and seamlessly follow the link to the specified container. UNS entry points are created by providing a path to the DAOS container create command, and can exist either within existing containers or at locations within the regular POSIX filesystem.","title":"Unified NameSpace (UNS) in DAOS via dfuse"},{"location":"release/releaseNote_v1_0/#per-pool-acl","text":"Access to DAOS data is controlled through a simple identity based access control mechanism specified on a per pool basis. The access credentials for a DAOS client process are determined by the agent running on the compute node. This credential is validated against the Access Control List present on the DAOS server to determine the level of access granted to a specific user. The ACL mechanism support access permissions on an individual, and group basis as well as a mechanism for supporting fall through users. As of DAOS 1.0 the set of permissions on a pool are very simple but will be extended when container based ACLs are introduced in a future version.","title":"Per-Pool ACL"},{"location":"release/releaseNote_v1_0/#certificate-support","text":"Securing transport of data and authorizing actions by components make up the backbone of the security model used in the DAOS control plane. All control plane components communicate with each other using mutual TLS authentication(mTLS) backed by certificates signed by a per cluster certificate authority. In addition to securing the communications between components certificates are also used to authenticate the various components and ensure that requests only come from components who are authorized to perform those actions. Certificates are also used to secure the administrative interface to the cluster ensuring only admins with the proper credentials can perform administrative actions.","title":"Certificate Support"},{"location":"release/releaseNote_v1_0/#document-updates","text":"All documents supporting the https://daos-stack.github.io/ site have been refreshed and re-organized for this release. Design documents in the DAOS Source repository have also been refreshed for this release.","title":"Document updates"},{"location":"release/releaseNote_v1_0/#known-issues","text":"DAOS-4020 - dmg storage scan reports incorrect NUMA socket ID The ndctl package maintainers have confirmed that this is a kernel regression that has been resolved in CentOS 8. Some DAOS nodes may not report the correct NUMA socket ID when running a \"dmg storage scan.\" This appears to be a regression in the CentOS7.7 kernel rather than an ndctl issue. This is due to different versions of ndctl provisioning different JSON namespace details (storage scan command reads the \"numa_node\" field).","title":"Known Issues"},{"location":"release/releaseNote_v1_0/#fixed-issues-details","text":"First release, this section will be updated in subsequent releases Change Log No changes for this first release.","title":"Fixed Issues Details"},{"location":"user/container/","text":"Container Management \u00b6 DAOS containers are the unit of data management for users. Container Creation/Destroy \u00b6 Containers can be created and destroyed through the daos_cont_create/destroy() functions exported by the DAOS API. A user tool called daos is also provided to manage containers. To create a container: $ daos cont create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 --svc=0 Successfully created container 008123fc-6b6c-4768-a88a-a2a5ef34a1a2 The container type (i.e., POSIX or HDF5) can be passed via the --type option. As shown below, the pool UUID, container UUID, and container attributes can be stored in the extended attributes of a POSIX file or directory for convenience. Then subsequent invocations of the daos tools need to reference the path to the POSIX file or directory. $ daos cont create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 \\ --svc=0 --path=/tmp/mycontainer --type=POSIX --oclass=large \\ --chunk_size=4K Successfully created container 419b7562-5bb8-453f-bd52-917c8f5d80d1 type POSIX $ daos container query --svc=0 --path=/tmp/mycontainer Pool UUID: a171434a-05a5-4671-8fe2-615aa0d05094 Container UUID: 419b7562-5bb8-453f-bd52-917c8f5d80d1 Number of snapshots: 0 Latest Persistent Snapshot: 0 DAOS Unified Namespace Attributes on path /tmp/mycontainer: Container Type: POSIX Object Class: large Chunk Size: 4096 Container Properties \u00b6 At creation time, a list of container properties can be specified: Container Property Description DAOS_PROP_CO_LABEL A string that a user can associate with a container. e.g., \"Cat Pics\" or \"ResNet-50 training data\" DAOS_PROP_CO_LAYOUT_TYPE The container type (POSIX, MPI-IO, HDF5, ...) DAOS_PROP_CO_LAYOUT_VER A version of the layout that can be used by I/O middleware and application to handle interoperability. DAOS_PROP_CO_REDUN_FAC The redundancy factor that drives the minimal data protection required for objects stored in the container. e.g., RF1 means no data protection, RF3 only allows 3-way replication or erasure code N+2. DAOS_PROP_CO_REDUN_LVL The fault domain level that should be used to place data redundancy information (e.g., storage nodes, racks...). This information will be eventually consumed to determine object placement. While those properties are currently stored persistently with container metadata, many of them are still under development. The ability to modify some of these properties on an existing container will also be provided in a future release. Data Integrity \u00b6 Checksum configuration is done per container and is disabled by default. To enable and configure checksums, the following container properties are used during container create. DAOS_PROP_CO_CSUM : Type of checksum algorithm to use. Supported values are DAOS_PROP_CO_CSUM_OFF, // default DAOS_PROP_CO_CSUM_CRC16, DAOS_PROP_CO_CSUM_CRC32, DAOS_PROP_CO_CSUM_CRC64, DAOS_PROP_CO_CSUM_CHUNK_SIZE : defines the chunk size used for creating checksums of array types. (default is 32K). DAOS_PROP_CO_CSUM_SERVER_VERIFY : Because of the probable decrease to IOPS, in most cases, it is not desired to verify checksums on an object update on the server side. It is sufficient for the client to verify on a fetch because any data corruption, whether on the object update, storage, or fetch, will be caught. However, there is an advantage to knowing if corruption happens on an update. The update would fail right away, indicating to the client to retry the RPC or report an error to upper levels. Note Note that currently, once a container is created, its checksum configuration cannot be changed. Snapshot & Rollback \u00b6 Similar to container create/destroy, a container can be snapshotted through the DAOS API by calling daos_cont_create_snap(). Additional functions are provided to destroy and list container snapshots. The API also provides the ability to subscribe to container snapshot events and to rollback the content of a container to a previous snapshot, but those operations are not yet fully implemented. This section will be updated once support for container snapshot is supported by the daos tool. The DAOS_PROP_CO_SNAPSHOT_MAX property is used to limit the maximum number of snapshots to retain. When a new snapshot is taken, and the threshold is reached, the oldest snapshot will be automatically deleted. Rolling back the content of a container to a snapshot is planned for future DAOS versions. User Attributes \u00b6 Similar to POSIX extended attributes, users can attach some metadata to each container through the daos_cont_{list/get/set}_attr() API. Access Control Lists \u00b6 Client user and group access for containers is controlled by Access Control Lists (ACLs) . Access-controlled container accesses include: Opening the container for access. Reading and writing data in the container. Reading and writing objects. Getting, setting, and listing user attributes. Getting, setting, and listing snapshots. Deleting the container (if the pool does not grant the user permission). Getting and setting container properties. Getting and modifying the container ACL. Modifying the container's owner. This is reflected in the set of supported container permissions . Pool vs. Container Permissions \u00b6 In general, pool permissions are separate from container permissions, and access to one does not guarantee access to the other. However, a user must have permission to connect to a container's pool before they can access the container in any way, regardless of their permissions on that container. Once the user has connected to a pool, container access decisions are based on the individual container ACL. A user need not have read/write access to a pool in order to open a container with read/write access, for example. There is one situation in which the pool can grant a container-level permission: Container deletion. If a user has Delete permission on a pool, this grants them the ability to delete any container in the pool, regardless of their permissions on that container. If the user does not have Delete permission on the pool, they will only be able to delete containers for which they have been explicitly granted Delete permission in the container's ACL. Creating Containers with Custom ACL \u00b6 To create a container with a custom ACL: $ daos cont create --pool=<UUID> --svc=<rank> --acl-file=<path> The ACL file format is detailed in the ACL section . Displaying a Container's ACL \u00b6 To view a container's ACL: $ daos cont get-acl --pool=<UUID> --svc=<rank> --cont=<UUID> The output is in the same string format used in the ACL file during creation, with one ACE per line. Modifying a Container's ACL \u00b6 For all of these commands using an ACL file, the ACL file must be in the format noted above for container creation. Overwriting the ACL \u00b6 To replace a container's ACL with a new ACL: $ daos cont overwrite-acl --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --acl-file=<path> Adding and Updating ACEs \u00b6 To add or update multiple entries in an existing container ACL: $ daos cont update-acl --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --acl-file=<path> To add or update a single entry in an existing container ACL: $ daos cont update-acl --pool=<UUID> --svc=<rank> --cont=<UUID> --entry <ACE> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one. Removing an ACE \u00b6 To delete an entry for a given principal in an existing container ACL: $ daos cont delete-acl --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --principal=<principal> The principal corresponds to the principal portion of an ACE that was set during container creation or a previous container ACL operation. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ GROUP@ EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the container will be decided based on the remaining ACL rules. Ownership \u00b6 The ownership of the container corresponds to the special principals OWNER@ and GROUP@ in the ACL. These values are a part of the container properties. They may be set on container creation and changed later. The owner-user ( OWNER@ ) always has set-ACL and get-ACL permissions, even if they are not explicitly granted by the ACL. This applies regardless of the other permissions they are granted by ACE(s) in the ACL. The owner-group ( GROUP@ ) has no special permissions outside what they are granted by the ACL. Creating Containers with Specific Ownership \u00b6 The default owner user and group are the effective user and group of the user creating the container. However, a specific user and/or group may be specified at container creation time. $ daos cont create --pool=<UUID> --svc=<rank> --user=<owner-user> \\ --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals . Changing Ownership \u00b6 To change the owner user: $ daos cont set-owner --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --user=<owner-user> To change the owner group: $ daos cont set-owner --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals . Compression & Encryption \u00b6 The DAOS_PROP_CO_COMPRESS and DAOS_PROP_CO_ENCRYPT properties are reserved for configuring respectively online compression and encryption. These features are currently not on the roadmap.","title":"Container Management"},{"location":"user/container/#container-management","text":"DAOS containers are the unit of data management for users.","title":"Container Management"},{"location":"user/container/#container-creationdestroy","text":"Containers can be created and destroyed through the daos_cont_create/destroy() functions exported by the DAOS API. A user tool called daos is also provided to manage containers. To create a container: $ daos cont create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 --svc=0 Successfully created container 008123fc-6b6c-4768-a88a-a2a5ef34a1a2 The container type (i.e., POSIX or HDF5) can be passed via the --type option. As shown below, the pool UUID, container UUID, and container attributes can be stored in the extended attributes of a POSIX file or directory for convenience. Then subsequent invocations of the daos tools need to reference the path to the POSIX file or directory. $ daos cont create --pool=a171434a-05a5-4671-8fe2-615aa0d05094 \\ --svc=0 --path=/tmp/mycontainer --type=POSIX --oclass=large \\ --chunk_size=4K Successfully created container 419b7562-5bb8-453f-bd52-917c8f5d80d1 type POSIX $ daos container query --svc=0 --path=/tmp/mycontainer Pool UUID: a171434a-05a5-4671-8fe2-615aa0d05094 Container UUID: 419b7562-5bb8-453f-bd52-917c8f5d80d1 Number of snapshots: 0 Latest Persistent Snapshot: 0 DAOS Unified Namespace Attributes on path /tmp/mycontainer: Container Type: POSIX Object Class: large Chunk Size: 4096","title":"Container Creation/Destroy"},{"location":"user/container/#container-properties","text":"At creation time, a list of container properties can be specified: Container Property Description DAOS_PROP_CO_LABEL A string that a user can associate with a container. e.g., \"Cat Pics\" or \"ResNet-50 training data\" DAOS_PROP_CO_LAYOUT_TYPE The container type (POSIX, MPI-IO, HDF5, ...) DAOS_PROP_CO_LAYOUT_VER A version of the layout that can be used by I/O middleware and application to handle interoperability. DAOS_PROP_CO_REDUN_FAC The redundancy factor that drives the minimal data protection required for objects stored in the container. e.g., RF1 means no data protection, RF3 only allows 3-way replication or erasure code N+2. DAOS_PROP_CO_REDUN_LVL The fault domain level that should be used to place data redundancy information (e.g., storage nodes, racks...). This information will be eventually consumed to determine object placement. While those properties are currently stored persistently with container metadata, many of them are still under development. The ability to modify some of these properties on an existing container will also be provided in a future release.","title":"Container Properties"},{"location":"user/container/#data-integrity","text":"Checksum configuration is done per container and is disabled by default. To enable and configure checksums, the following container properties are used during container create. DAOS_PROP_CO_CSUM : Type of checksum algorithm to use. Supported values are DAOS_PROP_CO_CSUM_OFF, // default DAOS_PROP_CO_CSUM_CRC16, DAOS_PROP_CO_CSUM_CRC32, DAOS_PROP_CO_CSUM_CRC64, DAOS_PROP_CO_CSUM_CHUNK_SIZE : defines the chunk size used for creating checksums of array types. (default is 32K). DAOS_PROP_CO_CSUM_SERVER_VERIFY : Because of the probable decrease to IOPS, in most cases, it is not desired to verify checksums on an object update on the server side. It is sufficient for the client to verify on a fetch because any data corruption, whether on the object update, storage, or fetch, will be caught. However, there is an advantage to knowing if corruption happens on an update. The update would fail right away, indicating to the client to retry the RPC or report an error to upper levels. Note Note that currently, once a container is created, its checksum configuration cannot be changed.","title":"Data Integrity"},{"location":"user/container/#snapshot-rollback","text":"Similar to container create/destroy, a container can be snapshotted through the DAOS API by calling daos_cont_create_snap(). Additional functions are provided to destroy and list container snapshots. The API also provides the ability to subscribe to container snapshot events and to rollback the content of a container to a previous snapshot, but those operations are not yet fully implemented. This section will be updated once support for container snapshot is supported by the daos tool. The DAOS_PROP_CO_SNAPSHOT_MAX property is used to limit the maximum number of snapshots to retain. When a new snapshot is taken, and the threshold is reached, the oldest snapshot will be automatically deleted. Rolling back the content of a container to a snapshot is planned for future DAOS versions.","title":"Snapshot &amp; Rollback"},{"location":"user/container/#user-attributes","text":"Similar to POSIX extended attributes, users can attach some metadata to each container through the daos_cont_{list/get/set}_attr() API.","title":"User Attributes"},{"location":"user/container/#access-control-lists","text":"Client user and group access for containers is controlled by Access Control Lists (ACLs) . Access-controlled container accesses include: Opening the container for access. Reading and writing data in the container. Reading and writing objects. Getting, setting, and listing user attributes. Getting, setting, and listing snapshots. Deleting the container (if the pool does not grant the user permission). Getting and setting container properties. Getting and modifying the container ACL. Modifying the container's owner. This is reflected in the set of supported container permissions .","title":"Access Control Lists"},{"location":"user/container/#pool-vs-container-permissions","text":"In general, pool permissions are separate from container permissions, and access to one does not guarantee access to the other. However, a user must have permission to connect to a container's pool before they can access the container in any way, regardless of their permissions on that container. Once the user has connected to a pool, container access decisions are based on the individual container ACL. A user need not have read/write access to a pool in order to open a container with read/write access, for example. There is one situation in which the pool can grant a container-level permission: Container deletion. If a user has Delete permission on a pool, this grants them the ability to delete any container in the pool, regardless of their permissions on that container. If the user does not have Delete permission on the pool, they will only be able to delete containers for which they have been explicitly granted Delete permission in the container's ACL.","title":"Pool vs. Container Permissions"},{"location":"user/container/#creating-containers-with-custom-acl","text":"To create a container with a custom ACL: $ daos cont create --pool=<UUID> --svc=<rank> --acl-file=<path> The ACL file format is detailed in the ACL section .","title":"Creating Containers with Custom ACL"},{"location":"user/container/#displaying-a-containers-acl","text":"To view a container's ACL: $ daos cont get-acl --pool=<UUID> --svc=<rank> --cont=<UUID> The output is in the same string format used in the ACL file during creation, with one ACE per line.","title":"Displaying a Container's ACL"},{"location":"user/container/#modifying-a-containers-acl","text":"For all of these commands using an ACL file, the ACL file must be in the format noted above for container creation.","title":"Modifying a Container's ACL"},{"location":"user/container/#overwriting-the-acl","text":"To replace a container's ACL with a new ACL: $ daos cont overwrite-acl --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --acl-file=<path>","title":"Overwriting the ACL"},{"location":"user/container/#adding-and-updating-aces","text":"To add or update multiple entries in an existing container ACL: $ daos cont update-acl --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --acl-file=<path> To add or update a single entry in an existing container ACL: $ daos cont update-acl --pool=<UUID> --svc=<rank> --cont=<UUID> --entry <ACE> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one.","title":"Adding and Updating ACEs"},{"location":"user/container/#removing-an-ace","text":"To delete an entry for a given principal in an existing container ACL: $ daos cont delete-acl --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --principal=<principal> The principal corresponds to the principal portion of an ACE that was set during container creation or a previous container ACL operation. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ GROUP@ EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the container will be decided based on the remaining ACL rules.","title":"Removing an ACE"},{"location":"user/container/#ownership","text":"The ownership of the container corresponds to the special principals OWNER@ and GROUP@ in the ACL. These values are a part of the container properties. They may be set on container creation and changed later. The owner-user ( OWNER@ ) always has set-ACL and get-ACL permissions, even if they are not explicitly granted by the ACL. This applies regardless of the other permissions they are granted by ACE(s) in the ACL. The owner-group ( GROUP@ ) has no special permissions outside what they are granted by the ACL.","title":"Ownership"},{"location":"user/container/#creating-containers-with-specific-ownership","text":"The default owner user and group are the effective user and group of the user creating the container. However, a specific user and/or group may be specified at container creation time. $ daos cont create --pool=<UUID> --svc=<rank> --user=<owner-user> \\ --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals .","title":"Creating Containers with Specific Ownership"},{"location":"user/container/#changing-ownership","text":"To change the owner user: $ daos cont set-owner --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --user=<owner-user> To change the owner group: $ daos cont set-owner --pool=<UUID> --svc=<rank> --cont=<UUID> \\ --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals .","title":"Changing Ownership"},{"location":"user/container/#compression-encryption","text":"The DAOS_PROP_CO_COMPRESS and DAOS_PROP_CO_ENCRYPT properties are reserved for configuring respectively online compression and encryption. These features are currently not on the roadmap.","title":"Compression &amp; Encryption"},{"location":"user/hpc/","text":"HPC I/O Middleware Support \u00b6 Several HPC I/O middleware libraries have been ported to the native API. MPI-IO \u00b6 DAOS has its own MPI-IO ROMIO ADIO driver located in a MPICH fork on GitHub . This driver has been merged in the upstream MPICH repo. To build the MPI-IO driver: export MPI_LIB=\"\" download the mpich repo from above ./autogen.sh mkdir build; cd build ../configure --prefix=dir --enable-fortran=all --enable-romio --enable-cxx --enable-g=all --enable-debuginfo --with-device=ch3:sock --with-file-system=ufs+daos --with-daos=dir --with-cart=dir make -j8; make install Switch the PATH and LD_LIBRARY_PATH to where you want to build your client apps or libs that use MPI to the installed MPICH. Build any client (HDF5, ior, mpi test suites) normally with the mpicc and mpich library installed above (see child pages). To run an example: Launch DAOS server(s) and create a pool. This will return a pool uuid \"puuid\" and service rank list \"svcl\". Create a POSIX type container: daos cont create --pool=puuid --svc=svcl --type=POSIX This will return a container uuid \"cuuid\". At the client side, the following environment variables need to be set: export DAOS_POOL=puuid; export DAOS_SVCL=svcl; export DAOS_CONT=cuuid Alternatively, the unified namespace mode can be used instead. Run the client application or test. Limitations to the current implementation include: Reading Holes does not return 0, but leaves the buffer untouched. No support for MPI file atomicity, preallocate, shared file pointers. HDF5 \u00b6 A HDF5 DAOS connector is available. Please refer to the DAOS VOL connector user guide 1 for instructions on how to build and use it. https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/docs/users_guide.pdf \u21a9","title":"HPC I/O Middleware"},{"location":"user/hpc/#hpc-io-middleware-support","text":"Several HPC I/O middleware libraries have been ported to the native API.","title":"HPC I/O Middleware Support"},{"location":"user/hpc/#mpi-io","text":"DAOS has its own MPI-IO ROMIO ADIO driver located in a MPICH fork on GitHub . This driver has been merged in the upstream MPICH repo. To build the MPI-IO driver: export MPI_LIB=\"\" download the mpich repo from above ./autogen.sh mkdir build; cd build ../configure --prefix=dir --enable-fortran=all --enable-romio --enable-cxx --enable-g=all --enable-debuginfo --with-device=ch3:sock --with-file-system=ufs+daos --with-daos=dir --with-cart=dir make -j8; make install Switch the PATH and LD_LIBRARY_PATH to where you want to build your client apps or libs that use MPI to the installed MPICH. Build any client (HDF5, ior, mpi test suites) normally with the mpicc and mpich library installed above (see child pages). To run an example: Launch DAOS server(s) and create a pool. This will return a pool uuid \"puuid\" and service rank list \"svcl\". Create a POSIX type container: daos cont create --pool=puuid --svc=svcl --type=POSIX This will return a container uuid \"cuuid\". At the client side, the following environment variables need to be set: export DAOS_POOL=puuid; export DAOS_SVCL=svcl; export DAOS_CONT=cuuid Alternatively, the unified namespace mode can be used instead. Run the client application or test. Limitations to the current implementation include: Reading Holes does not return 0, but leaves the buffer untouched. No support for MPI file atomicity, preallocate, shared file pointers.","title":"MPI-IO"},{"location":"user/hpc/#hdf5","text":"A HDF5 DAOS connector is available. Please refer to the DAOS VOL connector user guide 1 for instructions on how to build and use it. https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/docs/users_guide.pdf \u21a9","title":"HDF5"},{"location":"user/interface/","text":"Native Programming Interface \u00b6 Building against the DAOS library \u00b6 To build application or I/O middleware against the native DAOS API, include the daos.h header file in your program and link with -Ldaos . Examples are available under src/tests. DAOS API Reference \u00b6 libdaos is written in C and uses Doxygen comments that are added to C header files. The Doxygen documentation is available here . Python Bindings \u00b6 A python module called PyDAOS provides the DAOS python to python users. pydaos \u00b6 pydaos provides a native DAOS python interface exported by a C module. It integrates the DAOS key-value store API with python dictionnaries. Only strings are supported for both the key and value for now. Key-value pair can be inserted/looked up once at a time (see put/get) or in bulk (see bput/bget) taking a python dict as an input. The bulk operations are issued in parallel (up to 16 operations in flight) to maximize the operation rate. Key-value pair are deleted via the put/bput operations by setting the value to either None or the empty string. Once deleted, the key won't be reported during iteration. It also supports the del operation via 'del dkv.key'. The DAOS KV objects behave like a python dictionary and supports: 'dkv[key]' which invokes 'dkv.get(key)' 'dkv[key] = val' which invokes 'dkv.put(key, val)' 'for key in dkv:' allows to walk through the key space via the support of python iterators 'if key is in dkv:' allows to test whether a give key is present in the DAOS KV store. 'len(dkv)' returns the number of key-value pairs 'bool(dkv)' reports 'False' if there is no key-value pairs in the DAOS KV and 'True' otherwise. Python iterators are supported, which means that \"for key in kvobj:\" will allow you to walk through the key space. For each method, a PyDError exception is raised with proper DAOS error code (in string format) if the operation cannot be completed. Both python 2.7 and 3.x are supported. pydaos.raw \u00b6 The pydaos.raw submodule provides access to DAOS API functionality via Ctypes and was developed with an emphasis on test use cases. While the majority of unit tests are written in C, higher-level tests are written primarily using the Python API. Interfaces are provided for accessing DAOS management and DAOS API functionality from Python. This higher level interface allows a faster turnaround time on implementing test cases for DAOS. Layout \u00b6 The Python API is split into several files based on functionality: The Python object API: daos_api.py . The mapping of C structures to Python classes daos_cref.py High-level abstraction classes exist to manipulate DAOS storage: class DaosPool(object) class DaosContainer(object) class DaosObj(object) class IORequest(object) DaosPool is a Python class representing a DAOS pool. All pool-related functionality is exposed from this class. Operations such as creating/destroying a pool, connecting to a pool, and adding a target to a storage pool are supported. DaosContainer is a Python class representing a DAOS container. As with the DaosPool class, all container-related functionality is exposed here. This class also exposes abstracted wrapper functions for the flow of creating and committing an object to a DAOS container. DaosObj is a Python class representing a DAOS object. Functionality such as creating/deleting objects in a container, 'punching' objects (delete an object from the specified transaction only), and object query. IORequest is a Python class representing a read or write request against a DAOS object. Several classes exist for management purposes as well: class DaosContext(object) class DaosLog class DaosApiError(Exception) DaosContext is a wrapper around the DAOS libraries. It is initialized with the path where DAOS libraries can be found. DaosLog exposes functionality to write messages to the DAOS client log. DaosApiError is a custom exception class raised by the API internally in the event of a failed DAOS action. Most functions exposed in the DAOS C API support both synchronous and asynchronous execution, and the Python API exposes this same functionality. Each API takes an input event. DaosEvent is the Python representation of this event. If the input event is NULL , the call is synchronous. If an event is supplied, the function will return immediately after submitting API requests to the underlying stack and the user can poll and query the event for completion. Ctypes \u00b6 Ctypes is a built-in Python module for interfacing Python with existing libraries written in C/C++. The Python API is built as an object-oriented wrapper around the DAOS libraries utilizing ctypes. Ctypes documentation can be found here https://docs.python.org/3/library/ctypes.html The following demonstrates a simplified example of creating a Python wrapper for the C function daos_pool_tgt_exclude_out , with each input parameter to the C function being cast via ctypes. This also demonstrates struct representation via ctypes: // daos_exclude.c #include <stdio.h> int daos_pool_tgt_exclude_out(const uuid_t uuid, const char *grp, const d_rank_list_t *svc, struct d_tgt_list *tgts, daos_event_t *ev); All input parameters must be represented via ctypes. If a struct is required as an input parameter, a corresponding Python class can be created. For struct d_tgt_list : struct d_tgt_list { d_rank_t *tl_ranks; int32_t *tl_tgts; uint32_t tl_nr; }; class DTgtList(ctypes.Structure): _fields_ = [(\"tl_ranks\", ctypes.POINTER(ctypes.c_uint32)), (\"tl_tgts\", ctypes.POINTER(ctypes.c_int32)), (\"tl_nr\", ctypes.c_uint32)] The shared object containing daos_pool_tgt_exclude_out can then be imported and the function called directly: # api.py import ctypes import uuid import conversion # utility library to convert C <---> Python UUIDs # init python variables p_uuid = str(uuid.uuid4()) p_tgts = 2 p_ranks = DaosPool.__pylist_to_array([2]) # cast python variables via ctypes as necessary c_uuid = str_to_c_uuid(p_uuid) c_grp = ctypes.create_string_buffer(b\"daos_group_name\") c_svc = ctypes.POINTER(2) # ensure pointers are cast/passed as such c_tgt_list = ctypes.POINTER(DTgtList(p_ranks, p_tgts, 2))) # again, DTgtList must be passed as pointer # load the shared object my_lib = ctypes.CDLL('/full/path/to/daos_exclude.so') # now call it my_lib.daos_pool_tgt_exclude_out(c_uuid, c_grp, c_svc, c_tgt_list, None) Error Handling \u00b6 The API was designed using the EAFP ( E asier to A sk F orgiveness than get P ermission) idiom. A given function will raise a custom exception on error state, DaosApiError . A user of the API is expected to catch and handle this exception as needed: # catch and log try: daos_some_action() except DaosApiError as e: self.d_log.ERROR(\"My DAOS action encountered an error!\") Logging \u00b6 The Python DAOS API exposes functionality to log messages to the DAOS client log. Messages can be logged as INFO , DEBUG , WARN , or ERR log levels. The DAOS log object must be initialized with the environmental context in which to run: from pydaos.raw import DaosLog self.d_log = DaosLog(self.context) self.d_log.INFO(\"FYI\") self.d_log.DEBUG(\"Debugging code\") self.d_log.WARNING(\"Be aware, may be issues\") self.d_log.ERROR(\"Something went very wrong\") Go Bindings \u00b6 API bindings for Go 2 are also available. https://github.com/daos-stack/daos/blob/master/src/client/pydaos/raw/README.md \u21a9 https://godoc.org/github.com/daos-stack/go-daos/pkg/daos \u21a9","title":"Native Programming Interface"},{"location":"user/interface/#native-programming-interface","text":"","title":"Native Programming Interface"},{"location":"user/interface/#building-against-the-daos-library","text":"To build application or I/O middleware against the native DAOS API, include the daos.h header file in your program and link with -Ldaos . Examples are available under src/tests.","title":"Building against the DAOS library"},{"location":"user/interface/#daos-api-reference","text":"libdaos is written in C and uses Doxygen comments that are added to C header files. The Doxygen documentation is available here .","title":"DAOS API Reference"},{"location":"user/interface/#python-bindings","text":"A python module called PyDAOS provides the DAOS python to python users.","title":"Python Bindings"},{"location":"user/interface/#pydaos","text":"pydaos provides a native DAOS python interface exported by a C module. It integrates the DAOS key-value store API with python dictionnaries. Only strings are supported for both the key and value for now. Key-value pair can be inserted/looked up once at a time (see put/get) or in bulk (see bput/bget) taking a python dict as an input. The bulk operations are issued in parallel (up to 16 operations in flight) to maximize the operation rate. Key-value pair are deleted via the put/bput operations by setting the value to either None or the empty string. Once deleted, the key won't be reported during iteration. It also supports the del operation via 'del dkv.key'. The DAOS KV objects behave like a python dictionary and supports: 'dkv[key]' which invokes 'dkv.get(key)' 'dkv[key] = val' which invokes 'dkv.put(key, val)' 'for key in dkv:' allows to walk through the key space via the support of python iterators 'if key is in dkv:' allows to test whether a give key is present in the DAOS KV store. 'len(dkv)' returns the number of key-value pairs 'bool(dkv)' reports 'False' if there is no key-value pairs in the DAOS KV and 'True' otherwise. Python iterators are supported, which means that \"for key in kvobj:\" will allow you to walk through the key space. For each method, a PyDError exception is raised with proper DAOS error code (in string format) if the operation cannot be completed. Both python 2.7 and 3.x are supported.","title":"pydaos"},{"location":"user/interface/#pydaosraw","text":"The pydaos.raw submodule provides access to DAOS API functionality via Ctypes and was developed with an emphasis on test use cases. While the majority of unit tests are written in C, higher-level tests are written primarily using the Python API. Interfaces are provided for accessing DAOS management and DAOS API functionality from Python. This higher level interface allows a faster turnaround time on implementing test cases for DAOS.","title":"pydaos.raw"},{"location":"user/interface/#layout","text":"The Python API is split into several files based on functionality: The Python object API: daos_api.py . The mapping of C structures to Python classes daos_cref.py High-level abstraction classes exist to manipulate DAOS storage: class DaosPool(object) class DaosContainer(object) class DaosObj(object) class IORequest(object) DaosPool is a Python class representing a DAOS pool. All pool-related functionality is exposed from this class. Operations such as creating/destroying a pool, connecting to a pool, and adding a target to a storage pool are supported. DaosContainer is a Python class representing a DAOS container. As with the DaosPool class, all container-related functionality is exposed here. This class also exposes abstracted wrapper functions for the flow of creating and committing an object to a DAOS container. DaosObj is a Python class representing a DAOS object. Functionality such as creating/deleting objects in a container, 'punching' objects (delete an object from the specified transaction only), and object query. IORequest is a Python class representing a read or write request against a DAOS object. Several classes exist for management purposes as well: class DaosContext(object) class DaosLog class DaosApiError(Exception) DaosContext is a wrapper around the DAOS libraries. It is initialized with the path where DAOS libraries can be found. DaosLog exposes functionality to write messages to the DAOS client log. DaosApiError is a custom exception class raised by the API internally in the event of a failed DAOS action. Most functions exposed in the DAOS C API support both synchronous and asynchronous execution, and the Python API exposes this same functionality. Each API takes an input event. DaosEvent is the Python representation of this event. If the input event is NULL , the call is synchronous. If an event is supplied, the function will return immediately after submitting API requests to the underlying stack and the user can poll and query the event for completion.","title":"Layout"},{"location":"user/interface/#ctypes","text":"Ctypes is a built-in Python module for interfacing Python with existing libraries written in C/C++. The Python API is built as an object-oriented wrapper around the DAOS libraries utilizing ctypes. Ctypes documentation can be found here https://docs.python.org/3/library/ctypes.html The following demonstrates a simplified example of creating a Python wrapper for the C function daos_pool_tgt_exclude_out , with each input parameter to the C function being cast via ctypes. This also demonstrates struct representation via ctypes: // daos_exclude.c #include <stdio.h> int daos_pool_tgt_exclude_out(const uuid_t uuid, const char *grp, const d_rank_list_t *svc, struct d_tgt_list *tgts, daos_event_t *ev); All input parameters must be represented via ctypes. If a struct is required as an input parameter, a corresponding Python class can be created. For struct d_tgt_list : struct d_tgt_list { d_rank_t *tl_ranks; int32_t *tl_tgts; uint32_t tl_nr; }; class DTgtList(ctypes.Structure): _fields_ = [(\"tl_ranks\", ctypes.POINTER(ctypes.c_uint32)), (\"tl_tgts\", ctypes.POINTER(ctypes.c_int32)), (\"tl_nr\", ctypes.c_uint32)] The shared object containing daos_pool_tgt_exclude_out can then be imported and the function called directly: # api.py import ctypes import uuid import conversion # utility library to convert C <---> Python UUIDs # init python variables p_uuid = str(uuid.uuid4()) p_tgts = 2 p_ranks = DaosPool.__pylist_to_array([2]) # cast python variables via ctypes as necessary c_uuid = str_to_c_uuid(p_uuid) c_grp = ctypes.create_string_buffer(b\"daos_group_name\") c_svc = ctypes.POINTER(2) # ensure pointers are cast/passed as such c_tgt_list = ctypes.POINTER(DTgtList(p_ranks, p_tgts, 2))) # again, DTgtList must be passed as pointer # load the shared object my_lib = ctypes.CDLL('/full/path/to/daos_exclude.so') # now call it my_lib.daos_pool_tgt_exclude_out(c_uuid, c_grp, c_svc, c_tgt_list, None)","title":"Ctypes"},{"location":"user/interface/#error-handling","text":"The API was designed using the EAFP ( E asier to A sk F orgiveness than get P ermission) idiom. A given function will raise a custom exception on error state, DaosApiError . A user of the API is expected to catch and handle this exception as needed: # catch and log try: daos_some_action() except DaosApiError as e: self.d_log.ERROR(\"My DAOS action encountered an error!\")","title":"Error Handling"},{"location":"user/interface/#logging","text":"The Python DAOS API exposes functionality to log messages to the DAOS client log. Messages can be logged as INFO , DEBUG , WARN , or ERR log levels. The DAOS log object must be initialized with the environmental context in which to run: from pydaos.raw import DaosLog self.d_log = DaosLog(self.context) self.d_log.INFO(\"FYI\") self.d_log.DEBUG(\"Debugging code\") self.d_log.WARNING(\"Be aware, may be issues\") self.d_log.ERROR(\"Something went very wrong\")","title":"Logging"},{"location":"user/interface/#go-bindings","text":"API bindings for Go 2 are also available. https://github.com/daos-stack/daos/blob/master/src/client/pydaos/raw/README.md \u21a9 https://godoc.org/github.com/daos-stack/go-daos/pkg/daos \u21a9","title":"Go Bindings"},{"location":"user/posix/","text":"POSIX Namespace \u00b6 A regular POSIX namespace can be encapsulated into a DAOS container. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed to applications or I/O frameworks either directly (e.g., for frameworks Spark or TensorFlow, or benchmark like IOR or mdtest that support different a storage backend plugin), or transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottlenecks by delivering full OS bypass for POSIX read/write operations. libdfs \u00b6 DFS stands for DAOS File System and is a library that allows a DAOS container to be accessed as a hierarchical POSIX namespace. It supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and not implemented on a per-file or per-directory basis. setuid() and setgid() programs, as well as supplementary groups, are currently not supported. While libdfs can be tested from a single instance (i.e., single process or client node if used through DFuse), special care is required when the same POSIX container is mounted concurrently by multiple processes. Concurrent DFS mounts are not recommended. Support for concurrency control is under development and will be documented here once ready. DFuse \u00b6 DFuse provides File System access to DAOS through the standard libc/kernel/VFS POSIX infrastructure. This allows existing applications to use DAOS without modification and provides a path to upgrade those applications to native DAOS support. Additionally, DFuse provides an Interception Library to transparently allow POSIX clients to talk directly to DAOS servers providing OS-Bypass for I/O without modifying or recompiling of the application. DFuse builds heavily on DFS and data written via DFuse can be access by DFS and vice versa. DFuse Daemon \u00b6 The dfuse daemon runs a single instance per node to provide a user POSIX access to DAOS, and it should be run with the credentials of the user and typically will be started and stopped on each compute node as part of the prolog and epilog scripts of any resource manager or scheduler in use. One DFuse daemon per node can process requests for multiple clients. A single DFuse instance can provide access to multiple pools and containers concurrently, or can be limited to a single pool, or a single container. Restrictions \u00b6 DFuse is limited to a single user. Access to the filesystem from other users, including root will not be honored, and as a consequence of this, the chown and chgrp calls are not supported. Hard links and special device files, except symbolic links, are not supported, nor are any ACLs. DFuse can run in the foreground, keeping the terminal window open, or it can daemonize to run like a system daemon, however, to do this and still be able to access DAOS it needs to daemonize before calling daos_init() which in turns means it cannot report some kinds of startup errors either on stdout/stderr or via its return code. When initially starting with DFuse it is recommended to run in foreground mode ( --foreground ) to better observe any failures. Inodes are managed on the local node by the DFuse, so while inode numbers will be consistent on a node for the duration of the session, they are not guaranteed to be consistent across restarts of DFuse or across nodes. It is not possible to see pool/container listings through DFuse, so if readdir, ls or others are used for this, DFuse will return ENOTSUP. Launching \u00b6 DFuse should be run with the credentials (user/group) of the user that will be accessing it, and that owns any pools that will be used. There are two mandatory command-line options, these are: Command-line Option Description --svc=<ranks> service replicas --mountpoint=<path> path to mount dfuse The mount point specified should be en empty directory on the local node that is owned by the user. Additionally, there are several optional command-line options: Command-line Option Description --pool=<uuid> pool uuid to connect to --container=<uuid> container uuid to open --sys-name=<name> DAOS server name --foreground run in foreground --singlethreaded run single threaded When DFuse starts, it will register a single mount with the kernel at the location specified by the --mountpoint option, and this mount will be visable in /proc/mounts, and possibly the output of df. The contents of multiple pools/containers will be accessible via this single kernel mountpoint. Pool/Container Paths \u00b6 DFuse will only create one kernel level mount point regardless of how it is launched, but how POSIX containers are represented within that varies depending on the options. If both a pool and container uuids are specified on the command line then the mount point will map to the root of the container itself so files can be accessed by simply concatenating the mount point and the name of the file, relative to the root of the container. If neither a pool or container is specified then pools and container can be accessed by the path <mount point>/<pool uuid>/<container uuid> however it should be noted that readdir() and therefore ls do not work on either mount points or directories representing pools here so the pool and container uuids will have to be provided from an external source. If a pool uuid is specified but not a container uuid the containers can be accessed by the path <mount point>/<container uuid It is anticipated that in most cases, both pool and container uuids shall be used, so the mount point itself will map directly onto a POSIX container. Links into other Containers \u00b6 It is possible to link to other containers in DFuse, where subdirectories within a container resolve not to regular directories as normal, but rather to the root of entirely different POSIX containers. To create a new container and link it into the namespace of an existing one use the following command. $ daos container create --svc <svc> --type POSIX --pool <pool uuid> --path <path to entry point> The pool uuid should already exist, and the path should specify a location somewhere within a DFuse mount point that resolves to a POSIX container. Once a link is created it can be accessed through the new path, and following the link is virtually transparent. No container uuid is required. if one is not supplied, it will be created. To destroy a container again, the following command should be used. $ daos container destroy --svc --path <path to entry point> This will both remove the link between the containers and remove the container that was linked to. There is no support for adding links to already existing containers or removing links to containers without also removing the container itself. Information about a container, for example, the presence of an entry point between containers, or the pool and container uuids of the container linked to can be read with the following command: $ daos container info --svc --path <path to entry point> Enabling Caching \u00b6 DFuse in normal mode simply provides a communication path between the kernel and DAOS. However, this can come with a performance impact, and to help alleviate this it is possible to turn on caching, both within dfuse itself and by allowing the kernel to cache certain data. Where and when data is cached there is no attempt made to invalidate the caches based on changes to DAOS, other than simple timeouts. Enabling this option will turn on the following features: Kernel caching of dentries. Kernel caching of negative dentries Kernel caching of inodes (file sizes, permissions etc) Kernel caching of file contents Readahead in dfuse and inserting data into kernel cache. MMAP write optimization To turn on caching use the --enable-caching command-line option for dfuse. This will enable the feature for all accessed containers. When this option is used, the containers accessed should only be accessed from one node, so it may be necessary to create a container per node in this model. Stopping DFuse \u00b6 When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos When this is done, the local DFuse daemon should shut down the mount point, disconnect from the DAOS servers, and exit. You can also verify that the mount point is no longer listed in the /proc/mounts file Interception Library \u00b6 An interception library called libioil is available to work with DDuse. This library works in conjunction with DFuse and allows the interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any application changes. This provides kernel-bypass for I/O data leading to improved performance. To use this set the LD_PRELOAD to point to the shared library in the DOAS install directory: LD_PRELOAD=/path/to/daos/install/lib/libioil.so Support for libioil is currently planned for DAOS v1.2.","title":"POSIX Namespace"},{"location":"user/posix/#posix-namespace","text":"A regular POSIX namespace can be encapsulated into a DAOS container. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed to applications or I/O frameworks either directly (e.g., for frameworks Spark or TensorFlow, or benchmark like IOR or mdtest that support different a storage backend plugin), or transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottlenecks by delivering full OS bypass for POSIX read/write operations.","title":"POSIX Namespace"},{"location":"user/posix/#libdfs","text":"DFS stands for DAOS File System and is a library that allows a DAOS container to be accessed as a hierarchical POSIX namespace. It supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and not implemented on a per-file or per-directory basis. setuid() and setgid() programs, as well as supplementary groups, are currently not supported. While libdfs can be tested from a single instance (i.e., single process or client node if used through DFuse), special care is required when the same POSIX container is mounted concurrently by multiple processes. Concurrent DFS mounts are not recommended. Support for concurrency control is under development and will be documented here once ready.","title":"libdfs"},{"location":"user/posix/#dfuse","text":"DFuse provides File System access to DAOS through the standard libc/kernel/VFS POSIX infrastructure. This allows existing applications to use DAOS without modification and provides a path to upgrade those applications to native DAOS support. Additionally, DFuse provides an Interception Library to transparently allow POSIX clients to talk directly to DAOS servers providing OS-Bypass for I/O without modifying or recompiling of the application. DFuse builds heavily on DFS and data written via DFuse can be access by DFS and vice versa.","title":"DFuse"},{"location":"user/posix/#dfuse-daemon","text":"The dfuse daemon runs a single instance per node to provide a user POSIX access to DAOS, and it should be run with the credentials of the user and typically will be started and stopped on each compute node as part of the prolog and epilog scripts of any resource manager or scheduler in use. One DFuse daemon per node can process requests for multiple clients. A single DFuse instance can provide access to multiple pools and containers concurrently, or can be limited to a single pool, or a single container.","title":"DFuse Daemon"},{"location":"user/posix/#restrictions","text":"DFuse is limited to a single user. Access to the filesystem from other users, including root will not be honored, and as a consequence of this, the chown and chgrp calls are not supported. Hard links and special device files, except symbolic links, are not supported, nor are any ACLs. DFuse can run in the foreground, keeping the terminal window open, or it can daemonize to run like a system daemon, however, to do this and still be able to access DAOS it needs to daemonize before calling daos_init() which in turns means it cannot report some kinds of startup errors either on stdout/stderr or via its return code. When initially starting with DFuse it is recommended to run in foreground mode ( --foreground ) to better observe any failures. Inodes are managed on the local node by the DFuse, so while inode numbers will be consistent on a node for the duration of the session, they are not guaranteed to be consistent across restarts of DFuse or across nodes. It is not possible to see pool/container listings through DFuse, so if readdir, ls or others are used for this, DFuse will return ENOTSUP.","title":"Restrictions"},{"location":"user/posix/#launching","text":"DFuse should be run with the credentials (user/group) of the user that will be accessing it, and that owns any pools that will be used. There are two mandatory command-line options, these are: Command-line Option Description --svc=<ranks> service replicas --mountpoint=<path> path to mount dfuse The mount point specified should be en empty directory on the local node that is owned by the user. Additionally, there are several optional command-line options: Command-line Option Description --pool=<uuid> pool uuid to connect to --container=<uuid> container uuid to open --sys-name=<name> DAOS server name --foreground run in foreground --singlethreaded run single threaded When DFuse starts, it will register a single mount with the kernel at the location specified by the --mountpoint option, and this mount will be visable in /proc/mounts, and possibly the output of df. The contents of multiple pools/containers will be accessible via this single kernel mountpoint.","title":"Launching"},{"location":"user/posix/#poolcontainer-paths","text":"DFuse will only create one kernel level mount point regardless of how it is launched, but how POSIX containers are represented within that varies depending on the options. If both a pool and container uuids are specified on the command line then the mount point will map to the root of the container itself so files can be accessed by simply concatenating the mount point and the name of the file, relative to the root of the container. If neither a pool or container is specified then pools and container can be accessed by the path <mount point>/<pool uuid>/<container uuid> however it should be noted that readdir() and therefore ls do not work on either mount points or directories representing pools here so the pool and container uuids will have to be provided from an external source. If a pool uuid is specified but not a container uuid the containers can be accessed by the path <mount point>/<container uuid It is anticipated that in most cases, both pool and container uuids shall be used, so the mount point itself will map directly onto a POSIX container.","title":"Pool/Container Paths"},{"location":"user/posix/#links-into-other-containers","text":"It is possible to link to other containers in DFuse, where subdirectories within a container resolve not to regular directories as normal, but rather to the root of entirely different POSIX containers. To create a new container and link it into the namespace of an existing one use the following command. $ daos container create --svc <svc> --type POSIX --pool <pool uuid> --path <path to entry point> The pool uuid should already exist, and the path should specify a location somewhere within a DFuse mount point that resolves to a POSIX container. Once a link is created it can be accessed through the new path, and following the link is virtually transparent. No container uuid is required. if one is not supplied, it will be created. To destroy a container again, the following command should be used. $ daos container destroy --svc --path <path to entry point> This will both remove the link between the containers and remove the container that was linked to. There is no support for adding links to already existing containers or removing links to containers without also removing the container itself. Information about a container, for example, the presence of an entry point between containers, or the pool and container uuids of the container linked to can be read with the following command: $ daos container info --svc --path <path to entry point>","title":"Links into other Containers"},{"location":"user/posix/#enabling-caching","text":"DFuse in normal mode simply provides a communication path between the kernel and DAOS. However, this can come with a performance impact, and to help alleviate this it is possible to turn on caching, both within dfuse itself and by allowing the kernel to cache certain data. Where and when data is cached there is no attempt made to invalidate the caches based on changes to DAOS, other than simple timeouts. Enabling this option will turn on the following features: Kernel caching of dentries. Kernel caching of negative dentries Kernel caching of inodes (file sizes, permissions etc) Kernel caching of file contents Readahead in dfuse and inserting data into kernel cache. MMAP write optimization To turn on caching use the --enable-caching command-line option for dfuse. This will enable the feature for all accessed containers. When this option is used, the containers accessed should only be accessed from one node, so it may be necessary to create a container per node in this model.","title":"Enabling Caching"},{"location":"user/posix/#stopping-dfuse","text":"When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos When this is done, the local DFuse daemon should shut down the mount point, disconnect from the DAOS servers, and exit. You can also verify that the mount point is no longer listed in the /proc/mounts file","title":"Stopping DFuse"},{"location":"user/posix/#interception-library","text":"An interception library called libioil is available to work with DDuse. This library works in conjunction with DFuse and allows the interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any application changes. This provides kernel-bypass for I/O data leading to improved performance. To use this set the LD_PRELOAD to point to the shared library in the DOAS install directory: LD_PRELOAD=/path/to/daos/install/lib/libioil.so Support for libioil is currently planned for DAOS v1.2.","title":"Interception Library"},{"location":"user/spark/","text":"Getting Started with the DAOS Hadoop Filesystem \u00b6 Here, we describe the steps required to build and deploy the DAOS Hadoop filesystem, and the configurations to access DAOS in Spark and Hadoop. We assume DAOS servers and agents have already been deployed in the environment. Otherwise, they can be deployed by following the DAOS installation guide . Build DAOS Hadoop Filesystem \u00b6 The DAOS Java and Hadoop filesystem implementation have been merged into the DAOS repository. Below are the steps to build the Java jar files for the DAOS Java and DAOS Hadoop filesystem. These jar files are required when running Spark and Hadoop. You can ignore this section if you already have the pre-built jars. $ git clone https://github.com/daos-stack/daos.git $ cd daos $ git checkout <desired branch or commit> ## assume DAOS is built and installed to <daos_install> directory $ cd src/client/java $ mvn clean package -DskipITs -Ddaos.install.path=<daos_install> After build, the package daos-java- -assemble.tgz will be available under distribution/target. Deploy DAOS Hadoop Filesystem \u00b6 After unzipping daos-java-<version>-assemble.tgz , you will get the following files. daos-java-<version>.jar and hadoop-daos-<version>.jar These files need to be deployed on every compute node that runs Spark. Place them in a directory, e.g., $SPARK_HOME/jars for Spark and $HADOOP_HOME/share/hadoop/common/lib for Hadoop, which are accessible to all the nodes or copy them to every node. daos-site-example.xml The file contains DAOS configuration and needs to be properly configured with the DAOS pool UUID, container UUID, and a few other settings. Rename it to daos-site.xml and place it your application config directory, e.g., $SPARK_HOME/conf for Spark and $HADOOP_HOME/etc/hadoop for Hadoop. Configure DAOS Hadoop FileSystem \u00b6 Export all DAOS related env variables and the following env variable in your application, e.g., spark-env.sh for Spark and hadoop-env.sh for Hadoop. The following env enables signal chaining in JVM to better interoperate with DAOS native code that installs its own signal handlers. It ensures that signal calls are intercepted so that they do not actually replace the JVM's signal handlers if the handlers conflict with those already installed by the JVM. Instead, these calls save the new signal handlers, or \"chain\" them behind the JVM-installed handlers. Later, when any of these signals are raised and found not to be targeted at the JVM, the DAOS's handlers are invoked. $ export LD_PRELOAD=<YOUR JDK HOME>/jre/lib/amd64/libjsig.so Configure daos-site.xml. If the DAOS pool and container have not been created, we can use the following command to create them and get the pool UUID, container UUID, and service replicas. $ dmg pool create --scm-size=<scm size> --nvme-size=<nvme size> $ daos cont create --pool <pool UUID> --svc <service replicas> --type POSIX After that, configure daos-site.xml with the pool and container created. <configuration> ... <property> <name>fs.daos.pool.uuid</name> <value>your pool UUID</value> <description>UUID of DAOS pool</description> </property> <property> <name>fs.daos.container.uuid</name> <value>your container UUID</value> <description>UUID of DAOS container created with \"--type posix\"</description> </property> <property> <name>fs.daos.pool.svc</name> <value>your pool service replicas</value> <description>service list separated by \":\" if more than one service</description> </property> ... </configuration> The default pool and container are configured by fs.daos.pool.uuid and fs.daos.container.uuid . The default DAOS filesystem can be accessed by URI daos://default:1 in Spark and Hadoop. In HDFS, the URI is composed by a master host name (or IP address) and a port for example hdfs:// :8020. In DAOS, we don't use host name and port to connect, instead we use pool UUID and container UUID to specify the DFS filesystem. We do not put the UUIDs in URI as UUID is not a valid port number. Instead, the hostname default maps to the default pool configured by fs.daos.pool.uuid and the port 1 maps to the default container configured by fs.daos.container.uuid . It is also possible to configure multiple pools and containers in the daos-site.xml and use different URI to access them. For example, to access another container in the default pool using URI daos://default:2 , we can configure the container UUID in c2.fs.daos.container.uuid . To access another pool and container using daos://pool1:3 , we can configure the pool UUID in pool1.fs.daos.pool.uuid and container UUID in c3.fs.daos.container.uuid . See examples, \"daos://default:1\" reads values of \"fs.daos.pool.uuid\" and \"fs.daos.container.uuid\" \"daos://default:2\" reads values of \"fs.daos.pool.uuid\" and \"c2.fs.daos.container.uuid\" \"daos://pool1:3\" reads values of \"pool1.fs.daos.pool.uuid\" and \"c3.fs.daos.container.uuid\" Different URIs represent different DAOS filesystem and they can also be configured with different settings like the read buffer size, etc. For example, to configure the filesystem represented by daos://default:2 , we use property name prefixed with c2, i.e., *c2.fs.daos.* . To configure the filesystem represented by daos://pool1:3 , we use property name prefixed with pool1c3, i.e., *pool1c3.fs.daos.* . If no specific configurations are set, they fall back to the configuration set for the default pool and container started with *fs.daos*. . One tricky example is to access same DAOS filesystem with two Hadoop FileSystem instances. One instance is configured with preload enabled in the daos-site.xml. The other instance is preload disabled. With above design, you can use two different URIs, daos://default:1 and daos://default:2 . In the daos-site.xml, you can set fs.daos.container.uuid and c2.fs.daos.container.uuid to same the container UUID. Then set fs.daos.preload.size to a value greater than 0 and c2.fs.daos.preload.size to 0. Configure Spark to Use DAOS \u00b6 To access DAOS Hadoop filesystem in Spark, add the jar files to the classpath of the Spark executor and driver. This can be configured in Spark's configuration file spark-defaults.conf. spark.executor.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar spark.driver.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar Access DAOS in Spark \u00b6 All Spark APIs that work with the Hadoop filesystem will work with DAOS. We use the daos:// URI to access files stored in DAOS. For example, to read people.json file from the root directory of DAOS filesystem, we can use the following pySpark code: df = spark.read.json(\"daos://default:1/people.json\") Configure Hadoop to Use DAOS \u00b6 Edit $HADOOP_HOME/etc/hadoop/core-site.xml to change fs.defaultFS to \u201cdaos://default:1\u201d. Then append below configuration to this file and $HADOOP_HOME/etc/hadoop/yarn-site.xml. <property> <name>fs.AbstractFileSystem.daos.impl</name> <value>io.daos.fs.hadoop.DaosAbsFsImpl</value> </property> Then replicate daos-site.xml, core-site.xml and yarn-site.xml to other nodes. Access DAOS in Hadoop \u00b6 If everything goes well, you should see \u201c/user\u201d directory being listed after issuing below command. $ hadoop fs -ls / You can also play around with other Hadoop commands, like -copyFromLocal and -copyToLocal. You can also start Yarn and run some mapreduce jobs on Yarn. Just make sure you have DAOS URI, \u201cdaos://default:1/\u201d, set correctly in your job. Known Issues \u00b6 If you use Omni-path PSM2 provider in DAOS, you'll get connection issue in Yarn container due to PSM2 resource not being released properly in time.","title":"Spark"},{"location":"user/spark/#getting-started-with-the-daos-hadoop-filesystem","text":"Here, we describe the steps required to build and deploy the DAOS Hadoop filesystem, and the configurations to access DAOS in Spark and Hadoop. We assume DAOS servers and agents have already been deployed in the environment. Otherwise, they can be deployed by following the DAOS installation guide .","title":"Getting Started with the DAOS Hadoop Filesystem"},{"location":"user/spark/#build-daos-hadoop-filesystem","text":"The DAOS Java and Hadoop filesystem implementation have been merged into the DAOS repository. Below are the steps to build the Java jar files for the DAOS Java and DAOS Hadoop filesystem. These jar files are required when running Spark and Hadoop. You can ignore this section if you already have the pre-built jars. $ git clone https://github.com/daos-stack/daos.git $ cd daos $ git checkout <desired branch or commit> ## assume DAOS is built and installed to <daos_install> directory $ cd src/client/java $ mvn clean package -DskipITs -Ddaos.install.path=<daos_install> After build, the package daos-java- -assemble.tgz will be available under distribution/target.","title":"Build DAOS Hadoop Filesystem"},{"location":"user/spark/#deploy-daos-hadoop-filesystem","text":"After unzipping daos-java-<version>-assemble.tgz , you will get the following files. daos-java-<version>.jar and hadoop-daos-<version>.jar These files need to be deployed on every compute node that runs Spark. Place them in a directory, e.g., $SPARK_HOME/jars for Spark and $HADOOP_HOME/share/hadoop/common/lib for Hadoop, which are accessible to all the nodes or copy them to every node. daos-site-example.xml The file contains DAOS configuration and needs to be properly configured with the DAOS pool UUID, container UUID, and a few other settings. Rename it to daos-site.xml and place it your application config directory, e.g., $SPARK_HOME/conf for Spark and $HADOOP_HOME/etc/hadoop for Hadoop.","title":"Deploy DAOS Hadoop Filesystem"},{"location":"user/spark/#configure-daos-hadoop-filesystem","text":"Export all DAOS related env variables and the following env variable in your application, e.g., spark-env.sh for Spark and hadoop-env.sh for Hadoop. The following env enables signal chaining in JVM to better interoperate with DAOS native code that installs its own signal handlers. It ensures that signal calls are intercepted so that they do not actually replace the JVM's signal handlers if the handlers conflict with those already installed by the JVM. Instead, these calls save the new signal handlers, or \"chain\" them behind the JVM-installed handlers. Later, when any of these signals are raised and found not to be targeted at the JVM, the DAOS's handlers are invoked. $ export LD_PRELOAD=<YOUR JDK HOME>/jre/lib/amd64/libjsig.so Configure daos-site.xml. If the DAOS pool and container have not been created, we can use the following command to create them and get the pool UUID, container UUID, and service replicas. $ dmg pool create --scm-size=<scm size> --nvme-size=<nvme size> $ daos cont create --pool <pool UUID> --svc <service replicas> --type POSIX After that, configure daos-site.xml with the pool and container created. <configuration> ... <property> <name>fs.daos.pool.uuid</name> <value>your pool UUID</value> <description>UUID of DAOS pool</description> </property> <property> <name>fs.daos.container.uuid</name> <value>your container UUID</value> <description>UUID of DAOS container created with \"--type posix\"</description> </property> <property> <name>fs.daos.pool.svc</name> <value>your pool service replicas</value> <description>service list separated by \":\" if more than one service</description> </property> ... </configuration> The default pool and container are configured by fs.daos.pool.uuid and fs.daos.container.uuid . The default DAOS filesystem can be accessed by URI daos://default:1 in Spark and Hadoop. In HDFS, the URI is composed by a master host name (or IP address) and a port for example hdfs:// :8020. In DAOS, we don't use host name and port to connect, instead we use pool UUID and container UUID to specify the DFS filesystem. We do not put the UUIDs in URI as UUID is not a valid port number. Instead, the hostname default maps to the default pool configured by fs.daos.pool.uuid and the port 1 maps to the default container configured by fs.daos.container.uuid . It is also possible to configure multiple pools and containers in the daos-site.xml and use different URI to access them. For example, to access another container in the default pool using URI daos://default:2 , we can configure the container UUID in c2.fs.daos.container.uuid . To access another pool and container using daos://pool1:3 , we can configure the pool UUID in pool1.fs.daos.pool.uuid and container UUID in c3.fs.daos.container.uuid . See examples, \"daos://default:1\" reads values of \"fs.daos.pool.uuid\" and \"fs.daos.container.uuid\" \"daos://default:2\" reads values of \"fs.daos.pool.uuid\" and \"c2.fs.daos.container.uuid\" \"daos://pool1:3\" reads values of \"pool1.fs.daos.pool.uuid\" and \"c3.fs.daos.container.uuid\" Different URIs represent different DAOS filesystem and they can also be configured with different settings like the read buffer size, etc. For example, to configure the filesystem represented by daos://default:2 , we use property name prefixed with c2, i.e., *c2.fs.daos.* . To configure the filesystem represented by daos://pool1:3 , we use property name prefixed with pool1c3, i.e., *pool1c3.fs.daos.* . If no specific configurations are set, they fall back to the configuration set for the default pool and container started with *fs.daos*. . One tricky example is to access same DAOS filesystem with two Hadoop FileSystem instances. One instance is configured with preload enabled in the daos-site.xml. The other instance is preload disabled. With above design, you can use two different URIs, daos://default:1 and daos://default:2 . In the daos-site.xml, you can set fs.daos.container.uuid and c2.fs.daos.container.uuid to same the container UUID. Then set fs.daos.preload.size to a value greater than 0 and c2.fs.daos.preload.size to 0.","title":"Configure DAOS Hadoop FileSystem"},{"location":"user/spark/#configure-spark-to-use-daos","text":"To access DAOS Hadoop filesystem in Spark, add the jar files to the classpath of the Spark executor and driver. This can be configured in Spark's configuration file spark-defaults.conf. spark.executor.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar spark.driver.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar","title":"Configure Spark to Use DAOS"},{"location":"user/spark/#access-daos-in-spark","text":"All Spark APIs that work with the Hadoop filesystem will work with DAOS. We use the daos:// URI to access files stored in DAOS. For example, to read people.json file from the root directory of DAOS filesystem, we can use the following pySpark code: df = spark.read.json(\"daos://default:1/people.json\")","title":"Access DAOS in Spark"},{"location":"user/spark/#configure-hadoop-to-use-daos","text":"Edit $HADOOP_HOME/etc/hadoop/core-site.xml to change fs.defaultFS to \u201cdaos://default:1\u201d. Then append below configuration to this file and $HADOOP_HOME/etc/hadoop/yarn-site.xml. <property> <name>fs.AbstractFileSystem.daos.impl</name> <value>io.daos.fs.hadoop.DaosAbsFsImpl</value> </property> Then replicate daos-site.xml, core-site.xml and yarn-site.xml to other nodes.","title":"Configure Hadoop to Use DAOS"},{"location":"user/spark/#access-daos-in-hadoop","text":"If everything goes well, you should see \u201c/user\u201d directory being listed after issuing below command. $ hadoop fs -ls / You can also play around with other Hadoop commands, like -copyFromLocal and -copyToLocal. You can also start Yarn and run some mapreduce jobs on Yarn. Just make sure you have DAOS URI, \u201cdaos://default:1/\u201d, set correctly in your job.","title":"Access DAOS in Hadoop"},{"location":"user/spark/#known-issues","text":"If you use Omni-path PSM2 provider in DAOS, you'll get connection issue in Yarn container due to PSM2 resource not being released properly in time.","title":"Known Issues"}]}