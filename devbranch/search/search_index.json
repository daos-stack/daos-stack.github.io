{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DAOS Introduction \u00b6 The Distributed Asynchronous Object Storage (DAOS) is an open-source object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next-generation NVM technology, like Intel \u00a9 Optane \u2122 Persistent Memory and NVM express (NVMe), while presenting a key-value storage interface on top of commodity hardware that provides features, such as, transactional non-blocking I/O, advanced data protection with self-healing, end-to-end data integrity, fine-grained data control, and elastic storage, to optimize performance and cost. The included document versions are associated with DAOS v2.0 (development), and may also describe features that are currently under development for the next DAOS release. Refer to the following documentation for architecture and description: Document Description DAOS Overview Terminology, Storage, Transaction, Fault and the Security models are presented. Administration Guide System administration topics are covered in the Administration Guide. User Guide Documentation for users including the different interfaces that are supported. Developer Guide Overview of the DAOS internal code structure and major algorithms for DAOS developers. Community Wiki This is the main community repository for DAOS information. Links to discover, use and contribute to DAOS are available from this page. Community Roadmap The DAOS development roadmap is found here. Note that the information contained on the roadmap may change at any time.","title":"Home"},{"location":"#daos-introduction","text":"The Distributed Asynchronous Object Storage (DAOS) is an open-source object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next-generation NVM technology, like Intel \u00a9 Optane \u2122 Persistent Memory and NVM express (NVMe), while presenting a key-value storage interface on top of commodity hardware that provides features, such as, transactional non-blocking I/O, advanced data protection with self-healing, end-to-end data integrity, fine-grained data control, and elastic storage, to optimize performance and cost. The included document versions are associated with DAOS v2.0 (development), and may also describe features that are currently under development for the next DAOS release. Refer to the following documentation for architecture and description: Document Description DAOS Overview Terminology, Storage, Transaction, Fault and the Security models are presented. Administration Guide System administration topics are covered in the Administration Guide. User Guide Documentation for users including the different interfaces that are supported. Developer Guide Overview of the DAOS internal code structure and major algorithms for DAOS developers. Community Wiki This is the main community repository for DAOS information. Links to discover, use and contribute to DAOS are available from this page. Community Roadmap The DAOS development roadmap is found here. Note that the information contained on the roadmap may change at any time.","title":"DAOS Introduction"},{"location":"debugging/","text":"DAOS Debugging \u00b6 DAOS uses the debug system defined in CaRT , specifically the GURT library. Both server and client default log is stdout , unless otherwise set by D_LOG_FILE environment variable (client) or log_file config parameter (server). Registered Subsystems/Facilities \u00b6 The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging for. By default all subsystems are enabled ( DD_SUBSYS=all ). - DAOS Facilities: [common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, eio, tests] - Common Facilities (GURT): [MISC, MEM] - CaRT Facilities: [RPC, BULK, CORPC, GRP, LM, HG, ST, IV] Priority Logging \u00b6 All macros which output logs have a priority level, shown in descending order below. - D_FATAL(fmt, ...) FATAL - D_CRIT(fmt, ...) CRIT - D_ERROR(fmt, ...) ERR - D_WARN(fmt, ...) WARN - D_NOTE(fmt, ...) NOTE - D_INFO(fmt, ...) INFO - D_DEBUG(mask, fmt, ...) DEBUG The priority level that outputs to stderr can be set with DD_STDERR . By default in DAOS (specific to project), this is set to CRIT ( DD_STDERR=CRIT ) meaning that all CRIT and more severe log messages will dump to stderr. This however is separate from the priority of logging to /tmp/daos.log . The priority level of logging can be set with D_LOG_MASK , which by default is set to INFO ( D_LOG_MASK=INFO ), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well ( D_LOG_MASK=\"DEBUG,MEM=ERR\" ). Debug Masks/Streams: \u00b6 DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). In order to accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default ( DD_MASK=all ). - DAOS Debug Masks: - md = metadata operations - pl = placement operations - mgmt = pool management - epc = epoch system - df = durable format - rebuild = rebuild process - daos_default = (group mask) io, md, pl, and rebuild operations - Common Debug Masks (GURT): - any = generic messages, no classification - trace = function trace, tree/hash/lru operations - mem = memory operations - net = network operations - io = object I/O - test = test programs Common Use Cases \u00b6 Generic setup for all messages (default settings) $ export D_LOG_MASK=DEBUG $ export DD_SUBSYS=all $ export DD_MASK=all Disable all logs for performance tuning $ export D_LOG_MASK=ERR # -> will only log error messages from all facilities $ export D_LOG_MASK=FATAL # -> will only log system fatal messages Disable a noisy debug logging subsystem $ export D_LOG_MASK=DEBUG,MEM=ERR # -> disables MEM facility by restricting all logs # from that facility to ERROR or higher priority only Enable a subset of facilities of interest $ export DD_SUBSYS=rpc,tests $ export D_LOG_MASK=DEBUG # -> required to see logs for RPC and TESTS less severe # than INFO (majority of log messages) Fine-tune the debug messages by setting a debug mask $ export D_LOG_MASK=DEBUG $ export DD_MASK=mgmt # -> only logs DEBUG messages related to pool management See the DAOS Environment Variables documentation for more info about debug system environment.","title":"DAOS Debugging"},{"location":"debugging/#daos-debugging","text":"DAOS uses the debug system defined in CaRT , specifically the GURT library. Both server and client default log is stdout , unless otherwise set by D_LOG_FILE environment variable (client) or log_file config parameter (server).","title":"DAOS Debugging"},{"location":"debugging/#registered-subsystemsfacilities","text":"The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging for. By default all subsystems are enabled ( DD_SUBSYS=all ). - DAOS Facilities: [common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, eio, tests] - Common Facilities (GURT): [MISC, MEM] - CaRT Facilities: [RPC, BULK, CORPC, GRP, LM, HG, ST, IV]","title":"Registered Subsystems/Facilities"},{"location":"debugging/#priority-logging","text":"All macros which output logs have a priority level, shown in descending order below. - D_FATAL(fmt, ...) FATAL - D_CRIT(fmt, ...) CRIT - D_ERROR(fmt, ...) ERR - D_WARN(fmt, ...) WARN - D_NOTE(fmt, ...) NOTE - D_INFO(fmt, ...) INFO - D_DEBUG(mask, fmt, ...) DEBUG The priority level that outputs to stderr can be set with DD_STDERR . By default in DAOS (specific to project), this is set to CRIT ( DD_STDERR=CRIT ) meaning that all CRIT and more severe log messages will dump to stderr. This however is separate from the priority of logging to /tmp/daos.log . The priority level of logging can be set with D_LOG_MASK , which by default is set to INFO ( D_LOG_MASK=INFO ), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well ( D_LOG_MASK=\"DEBUG,MEM=ERR\" ).","title":"Priority Logging"},{"location":"debugging/#debug-masksstreams","text":"DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). In order to accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default ( DD_MASK=all ). - DAOS Debug Masks: - md = metadata operations - pl = placement operations - mgmt = pool management - epc = epoch system - df = durable format - rebuild = rebuild process - daos_default = (group mask) io, md, pl, and rebuild operations - Common Debug Masks (GURT): - any = generic messages, no classification - trace = function trace, tree/hash/lru operations - mem = memory operations - net = network operations - io = object I/O - test = test programs","title":"Debug Masks/Streams:"},{"location":"debugging/#common-use-cases","text":"Generic setup for all messages (default settings) $ export D_LOG_MASK=DEBUG $ export DD_SUBSYS=all $ export DD_MASK=all Disable all logs for performance tuning $ export D_LOG_MASK=ERR # -> will only log error messages from all facilities $ export D_LOG_MASK=FATAL # -> will only log system fatal messages Disable a noisy debug logging subsystem $ export D_LOG_MASK=DEBUG,MEM=ERR # -> disables MEM facility by restricting all logs # from that facility to ERROR or higher priority only Enable a subset of facilities of interest $ export DD_SUBSYS=rpc,tests $ export D_LOG_MASK=DEBUG # -> required to see logs for RPC and TESTS less severe # than INFO (majority of log messages) Fine-tune the debug messages by setting a debug mask $ export D_LOG_MASK=DEBUG $ export DD_MASK=mgmt # -> only logs DEBUG messages related to pool management See the DAOS Environment Variables documentation for more info about debug system environment.","title":"Common Use Cases"},{"location":"QSG/","text":"The Distributed Asynchronous Object Storage (DAOS) is an open-source object store designed from the ground up for massively distributed Non Volatile Memory (NVM). DAOS takes advantage of next-generation NVM technology, like Storage Class Memory (SCM) and NVM express (NVMe), while presenting a key-value storage interface on top of commodity hardware that provides features, such as, transactional non-blocking I/O, advanced data protection with self-healing, end-to-end data integrity, fine-grained data control, and elastic storage, to optimize performance and cost. The included document versions are associated with DAOS v1.0, and may also describe features that are currently under development for the next DAOS release. Refer to the following documentation for architecture and description: Document Description DAOS Overview Terminology, Storage, Transaction, Fault and the Security models are presented. Administration Guide System administration topics are covered in the Administration Guide. User Guide Documentation for users including the different interfaces that are supported. Developer Guide Overview of the DAOS internal code structure and major algorithms for DAOS developers. Community Wiki This is the main community repository for DAOS information. Links to discover, use and contribute to DAOS are available from this page. Community Roadmap The DAOS development roadmap is found here. Note that the information contained on the roadmap may change at any time.","title":"Index"},{"location":"QSG/CentosQSG/","text":"Quickstart Centos 7.9 with POSIX \u00b6 Introduction \u00b6 This is a quick start guide to help set up daos and use it with POSIX on Centos 7.9. This document covers any pre-requisites required before starting, pointer to setting up of daos system which consists of installation of daos rpms, setting up of various configuration files needed by daos, getting daos servers and agents up and running. Once the daos system is ready for use, this document can help set up dfuse mount point in order to take advantage of daos support for POSIX, help with some example runs of daos tests and benchmarking tools like ior and mdtest along with some examples of how to move data between a POSIX file system and daos containers (and vise versa) and finally cleaning up your daos setup. We are using a set of 2 servers each with PMEM and SSDs connected via storage network and 2 client nodes without pmem/ssd but on the storage network. We are also using another node as admin node. This can either be a separate node or any of your client nodes as well. All nodes have a base Centos 7.9 install. Requirements \u00b6 Set environment variables for list of servers, clients and admin node. export ADMIN_NODE=node-1 export SERVER_NODES=node-2,node-3 export CLIENT_NODES=node-4,node-5 export ALL_NODES=\\$ADMIN_NODE,\\$SERVER_NODES,\\$CLIENT_NODES All nodes have Centos 7.9 installed Set password less ssh amongst all nodes Make sure to have sudo privileges on all nodes Enable IOMMU If using nvme, enable IOMMU on server nodes: https://daos-stack.github.io/admin/predeployment_check/#enable-iommu-optional Install pdsh on admin node # Centos sudo yum install -y pdsh Set-Up \u00b6 Please refer here for initial set up which consists of rpm installation, generate and set up certificates, setting up config files, starting servers and agents.","title":"CentosQSG"},{"location":"QSG/CentosQSG/#quickstart-centos-79-with-posix","text":"","title":"Quickstart Centos 7.9 with POSIX"},{"location":"QSG/CentosQSG/#introduction","text":"This is a quick start guide to help set up daos and use it with POSIX on Centos 7.9. This document covers any pre-requisites required before starting, pointer to setting up of daos system which consists of installation of daos rpms, setting up of various configuration files needed by daos, getting daos servers and agents up and running. Once the daos system is ready for use, this document can help set up dfuse mount point in order to take advantage of daos support for POSIX, help with some example runs of daos tests and benchmarking tools like ior and mdtest along with some examples of how to move data between a POSIX file system and daos containers (and vise versa) and finally cleaning up your daos setup. We are using a set of 2 servers each with PMEM and SSDs connected via storage network and 2 client nodes without pmem/ssd but on the storage network. We are also using another node as admin node. This can either be a separate node or any of your client nodes as well. All nodes have a base Centos 7.9 install.","title":"Introduction"},{"location":"QSG/CentosQSG/#requirements","text":"Set environment variables for list of servers, clients and admin node. export ADMIN_NODE=node-1 export SERVER_NODES=node-2,node-3 export CLIENT_NODES=node-4,node-5 export ALL_NODES=\\$ADMIN_NODE,\\$SERVER_NODES,\\$CLIENT_NODES All nodes have Centos 7.9 installed Set password less ssh amongst all nodes Make sure to have sudo privileges on all nodes Enable IOMMU If using nvme, enable IOMMU on server nodes: https://daos-stack.github.io/admin/predeployment_check/#enable-iommu-optional Install pdsh on admin node # Centos sudo yum install -y pdsh","title":"Requirements"},{"location":"QSG/CentosQSG/#set-up","text":"Please refer here for initial set up which consists of rpm installation, generate and set up certificates, setting up config files, starting servers and agents.","title":"Set-Up"},{"location":"QSG/admin/","text":"DAOS System Administration \u00b6 System RAS Events \u00b6 Reliability, Availability and Serviceability (RAS) related events are communicated and logged within DAOS. RAS Event Structure \u00b6 The following table describes the structure of a DAOS RAS event including descriptions of mandatory and optional fields. Field Optional/Mandatory Description ID Mandatory Unique event identifier referenced in the manual. Type Mandatory Event type of STATE_CHANGE causes an update to the Management Service (MS) database in addition to event being written to SYSLOG. INFO_ONLY type events are only written to SYSLOG. Timestamp Mandatory Resolution at the microseconds and include the timezone offset to avoid locality issues. Severity Mandatory Indicates event severity, Error/Warning/Notice. Msg Mandatory Human readable message. HID Optional Identify hardware component involved in the event. E.g. PCI address for SSD, network interface Rank Optional DAOS rank involved in the event. PID Optional Identifier of the process involved in the RAS event TID Optional Identifier of the thread involved in the RAS event. JOBID Optional Identifier of the job involved in the RAS event. Hostname Optional Hostname of the node involved in the event. PUUID Optional Pool UUID involved in the event, if any. CUUID Optional Container UUID involved in the event, if relevant. OID Optional Object identifier involved in the event, if relevant. Control Operation Optional Recommended automatic action, if any. Data Optional Specific instance data treated as a blob. RAS Event IDs \u00b6 The following table lists supported DAOS RAS events including IDs, type, severity, message, description and cause. Event Event type Severity Message Description Cause engine_format required INFO_ONLY NOTICE DAOS engine <idx> requires a <type> format Indicates engine is waiting for allocated storage to be formatted on formatted on instance <idx> with dmg tool. <type> can be either SCM or Metadata. DAOS server attempts to bring-up an engine which has unformatted storage. engine_died STATE_CHANGE ERROR DAOS engine <idx> exited exited unexpectedly: <error> Indicates engine instance <idx> unexpectedly. describes the exit state returned from exited daos_engine process. N/A engine_asserted STATE_CHANGE ERROR TBD Indicates engine instance threw a runtime assertion, causing a crash. An unexpected internal state resulted in assert failure. engine_clock_drift INFO_ONLY ERROR clock drift detected Indicates CART comms layer has detected clock skew between engines. NTP may not be syncing clocks across DAOS system. pool_rebuild_started INFO_ONLY NOTICE Pool rebuild started. Indicates a pool rebuild has started. Event data field contains pool map version and pool operation identifier. When a pool rank becomes unavailable a rebuild will be triggered. pool_rebuild_finished INFO_ONLY NOTICE Pool rebuild finished. Indicates a pool rebuild has finished successfully. Event data field includes the pool map version and pool operation identifier. N/A pool_rebuild_failed INFO_ONLY ERROR Pool rebuild failed: <rc>. Indicates a pool rebuild has failed. Event data field includes the pool map version and pool operation identifier. <rc> provides a string representation of DER code. N/A pool_replicas_updated STATE_CHANGE NOTICE List of pool service replica ranks has been updated. Indicates a pool service replica list has changed. The event contains the new service replica list in a custom payload. When a pool service replica rank becomes unavailable a new rank is selected to replace it (if available). pool_durable_format_incompat INFO_ONLY ERROR incompatible layout version: <current> not in [<min>, <max>] Indicates the given pool's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with pool data in local storage that has an incompatible layout version. container_durable_format_incompat INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>] Indicates the given container's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with container data in local storage that has an incompatible layout version. rdb_durable_format_incompatible INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>]] Indicates the given rdb's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with rdb data in local storage that has an incompatible layout version. swim_rank_alive STATE_CHANGE NOTICE TBD The SWIM protocol has detected the specified rank is responsive. A remote DAOS engine has become responsive. swim_rank_dead STATE_CHANGE NOTICE SWIM rank marked as dead. The SWIM protocol has detected the specified rank is unresponsive. A remote DAOS engine has become unresponsive. system_start_failed INFO_ONLY ERROR System startup failed, <errors> Indicates that a user initiated controlled startup failed. <errors> shows which ranks failed. Ranks failed to start. system_stop_failed INFO_ONLY ERROR System shutdown failed during <action> action, <errors> Indicates that a user initiated controlled shutdown failed. <action> identifies the failing shutdown action and <errors> shows which ranks failed. Ranks failed to stop. System Monitoring \u00b6 System monitoring and telemetry data will be provided as part of the control plane and will be documented in a future revision. Storage Operations \u00b6 Space Utilization \u00b6 To query SCM and NVMe storage space usage and show how much space is available to create new DAOS pools with, run the following command: bash-4.2$ dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- wolf-71 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % wolf-72 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % The command output shows online DAOS storage utilization, only including storage statistics for devices that have been formatted by DAOS control-plane and assigned to a currently running rank of the DAOS system. This represents the storage that can host DAOS pools. Note that the table values are per-host (storage server) and SCM/NVMe capacity pool component values specified in dmg pool create are per rank. If multiple ranks (I/O processes) have been configured per host in the server configuration file daos_server.yml then the values supplied to dmg pool create should be a maximum of the SCM/NVMe free space divided by the number of ranks per host. For example if 2.0 TB SCM and 10.0 TB NVMe free space is reported by dmg storage query usage and the server configuration file used to start the system specifies 2 I/O processes (2 \"server\" sections), the maximum pool size that can be specified is approximately dmg pool create -s 1T -n 5T (may need to specify slightly below the maximum to take account of negligible metadata overhead). Storage Scrubbing \u00b6 Support for end-to-end data integrity is planned for DAOS v1.2 and background checksum scrubbing for v2.2. Once available, that functionality will be documented here. SSD Management \u00b6 Health Monitoring \u00b6 Useful admin dmg commands to query NVMe SSD health: Query Per-Server Metadata: dmg storage query (list-devices|list-pools) dmg storage scan --nvme-meta shows mapping of metadata to NVMe controllers The NVMe storage query list-devices and list-pools commands query the persistently stored SMD device and pool tables respectively. The device table maps the internal device UUID to attached VOS target IDs. The rank number of the server where the device is located is also listed, along with the current device state. The current device states are the following: - NORMAL: a fully, functional device in-use by DAOS - EVICTED: the device is no longer in-use by DAOS - UNPLUGGED: the device is currently unplugged from the system (may or not be evicted) - NEW: the device is plugged and available, and not currently in-use by DAOS The transport address is also listed for the device. This is either the PCIe address for normal NVMe SSDs, or the BDF format address of the backing NVMe SSDs behind a VMD (Volume Management Device) address. In the example below, the last two listed devices are both VMD devices with transport addresses in the BDF format behind the VMD address 0000:5d:05.5. The pool table maps the DAOS pool UUID to attached VOS target IDs and will list all of the server ranks that the pool is distributed on. With the additional verbose flag, the mapping of SPDK blob IDs to VOS target IDs will also be displayed. $ dmg -l boro-11,boro-13 storage query list-devices ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 2] Rank:0 State:NORMAL UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b [TrAddr:0000:8b:00.0] Targets:[1 3] Rank:0 State:NORMAL UUID:051b77e4-1524-4662-9f32-f8e4d2542c2d [TrAddr:0000:8c:00.0] Targets:[] Rank:0 State:NEW UUID:81905b24-be44-4106-8ff9-03002e9dd86a [TrAddr:5d0505:01:00.0] Targets:[0 2] Rank:1 State:EVICTED UUID:2ccb8afb-5d32-454e-86e3-762ec5dca7be [TrAddr:5d0505:03:00.0] Targets:[1 3] Rank:1 State:NORMAL $ dmg -l boro-11,boro-13 storage query list-pools ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Rank:1 Targets:[0 1 2 3] $ dmg -l boro-11,boro-13 storage query list-pools --verbose ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Blobs:[4294967404 4294967405 4294967407 4294967406] Rank:1 Targets:[0 1 2 3] Blobs:[4294967410 4294967411 4294967413 4294967412] Query Storage Device Health Data: dmg storage query (device-health|target-health) dmg storage scan --nvme-health shows NVMe controller health stats The NVMe storage query device-health and target-health commands query the device health data, including NVMe SSD health stats and in-memory I/O error and checksum error counters. The server rank and device state are also listed. The device health data can either be queried by device UUID (device-health command) or by VOS target ID along with the server rank (target-health command). The same device health information is displayed with both command options. $ dmg -l boro-11 storage query device-health --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 or $ dmg -l boro-11 storage query target-health --rank=0 --tgtid=0 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 1 2 3] Rank:0 State:NORMAL Health Stats: Temperature:289K(15C) Controller Busy Time:0s Power Cycles:0 Power On Duration:0s Unsafe Shutdowns:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK Eviction and Hotplug \u00b6 Manually Evict an NVMe SSD: dmg storage set nvme-faulty To manually evict an NVMe SSD (auto eviction will be supported in a future release), the device state needs to be set to \"FAULTY\" by running the following command: $ dmg -l boro-11 storage set nvme-faulty --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:FAULTY The device state will transition from \"NORMAL\" to \"FAULTY\" (shown above), which will trigger the faulty device reaction (all targets on the SSD will be rebuilt and the SSD will remain evicted until device replacement occurs). Note Full NVMe hot plug capability will be available and supported in DAOS 2.2 release. Use is currently intended for testing only and is not supported for production. Replace an Evicted SSD with a New Device: dmg storage replace nvme To replace an NVMe SSD with an evicted device and reintegrate it into use with DAOS, run the following command: $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=80c9f1be-84b9-4318-a1be-c416c96ca48b ------- boro-11 ------- Devices UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b Targets:[] Rank:1 State:NORMAL The old, now replaced device will remain in an \"EVICTED\" state until it is unplugged. The new device will transition from a \"NEW\" state to a \"NORMAL\" state (shown above). Reuse a FAULTY Device: dmg storage replace nvme In order to reuse a device that was previously set as FAULTY and evicted from the DAOS system, an admin can run the following command (setting the old device UUID to be the new device UUID): $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:NORMAL The FAULTY device will transition from an \"EVICTED\" state back to a \"NORMAL\" state, and will again be available for use with DAOS. The use case of this command will mainly be for testing, or for accidental device eviction. Identification \u00b6 The SSD identification feature is simply a way to quickly and visually locate a device. It requires the use of Intel VMD (Volume Management Device), which needs to be physically available on the hardware as well as enabled in the system BIOS. The feature supports two LED device events: locating a healthy device and locating an evicted device. Locate a Healthy SSD: dmg storage identify vmd To quickly identify an SSD in question, an administrator can run the following command: $ dmg -l boro-11 storage identify vmd --uuid=6fccb374-413b-441a-bfbe-860099ac5e8d If a non-VMD device UUID is used with the command, the following error will occur: localhost DAOS error (-1010): DER_NOSYS The status LED on the VMD device is now set to an \"IDENTIFY\" state, represented by a quick, 4Hz blinking amber light. The device will quickly blink by default for about 60 seconds and then return to the default \"OFF\" state. The LED event duration can be customized by setting the VMD_LED_PERIOD environment variable if a duration other than the default value is desired. Locate an Evicted SSD: If an NVMe SSD is evicted, the status LED on the VMD device is set to a \"FAULT\" state, represented by a solidly ON amber light. No additional command apart from the SSD eviction command would be needed, and this would visually indicate that the device needs to be replaced and is no longer in use by DAOS. The LED of the VMD device would remain in this state until replaced by a new device. System Operations \u00b6 The DAOS Control Server acting as the access point records details of DAOS I/O Server instances that join the DAOS system. Once an I/O Engine has joined the DAOS system, it is identified by a unique system \"rank\". Multiple ranks can reside on the same host machine, accessible via the same network address. A DAOS system can be shutdown and restarted to perform maintenance and/or reboot hosts. Pool data and state will be maintained providing no changes are made to the rank's metadata stored on persistent memory. Storage reformat can also be performed after system shutdown. Pools will be removed and storage wiped. System commands will be handled by the DAOS Server listening at the access point address specified as the first entry in the DMG config file \"hostlist\" parameter. See daos_control.yml for details. The \"access point\" address should be the same as that specified in the server config file daos_server.yml specified when starting daos_server instances. Warning Controlled start/stop/reformat have some known limitations. Whilst individual system instances can be stopped, if a subset is restarted, existing pools will not be automatically integrated with restarted instances. Membership \u00b6 The system membership can be queried using the command: $ dmg system query [--verbose] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] --verbose flag gives more information on each rank Output table will provide system rank mappings to host address and instance UUID, in addition to rank state. Shutdown \u00b6 When up and running, the entire system can be shutdown with the command: $ dmg system stop [--force] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS Control Servers will continue to operate and listen on the management network. Start \u00b6 To start the system after a controlled shutdown run the command: $ dmg system start [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS I/O Engines will be started. Reformat \u00b6 To reformat the system after a controlled shutdown run the command: $ dmg storage format --reformat --reformat flag indicates that a reformat operation should be performed disregarding existing filesystems if no record of previously running ranks can be found, reformat is performed on hosts in dmg config file hostlist if system membership has records of previously running ranks, storage allocated to those ranks will be formatted Output table will indicate action and result. DAOS I/O Engines will be started and all DAOS pools will have been removed. Manual Fresh Start \u00b6 To reset the DAOS metadata across all hosts, the system must be reformatted. First, ensure all daos_server processes on all hosts have been stopped, then for each SCM mount specified in the config file ( scm_mount in the servers section) umount and wipe FS signatures. Example illustration with two IO instances specified in the config file: clush -w wolf-[118-121,130-133] umount /mnt/daos1 clush -w wolf-[118-121,130-133] umount /mnt/daos0 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem1 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem0 Then restart DAOS Servers and format. Fault Domain \u00b6 Details on how to drain an individual storage node or fault domain (e.g. rack) in preparation for maintenance activity and how to reintegrate it will be provided in a future revision. System Extension \u00b6 Ability to add new DAOS server instances to a pre-existing DAOS system will be documented in a future revision. Fault Management \u00b6 DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. Fault Detection & Isolation \u00b6 DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM 1 that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure makes an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Once detected, the faulty target or servers (effectively a set of targets) must be excluded from each pool membership. This process is triggered either manually by the administrator or automatically (see the next section for more information). Upon exclusion from the pool map, each target starts the collective rebuild process automatically to restore data redundancy. The rebuild process is designed to operate online while servers continue to process incoming I/O operations from applications. Tools to monitor and manage rebuild are still under development. Rebuild Throttling \u00b6 The rebuild process may consume many resources on each server and can be throttled to reduce the impact on application performance. This current logic relies on CPU cycles on the storage nodes. By default, the rebuild process is configured to consume up to 30% of the CPU cycles, leaving the other 70% for regular I/O operations. During the rebuild process, the user can set the throttle to guarantee that the rebuild will not use more resources than the user setting. The user can only set the CPU cycle for now. For example, if the user set the throttle to 50, then the rebuild will at most use 50% of the CPU cycle to do the rebuild job. The default rebuild throttle for CPU cycle is 30. This parameter can be changed via the daos_mgmt_set_params() API call and will be eventually available through the management tools. Software Upgrade \u00b6 Interoperability in DAOS is handled via protocol and schema versioning for persistent data structures. Further instructions on how to manage DAOS software upgrades will be provided in a future revision. Protocol Interoperability \u00b6 Limited protocol interoperability is provided by the DAOS storage stack. Version compatibility checks will be performed to verify that: All targets in the same pool run the same protocol version. Client libraries linked with the application may be up to one protocol version older than the targets. If a protocol version mismatch is detected among storage targets in the same pool, the entire DAOS system will fail to start up and will report failure to the control API. Similarly, the connection from clients running a protocol version incompatible with the targets will return an error. Persistent Layout \u00b6 The schema of persistent data structures may evolve from time to time to fix bugs, add new optimizations, or support new features. To that end, the persistent data structures support schema versioning. Upgrading the schema version will not be performed automatically and must be initiated by the administrator. A dedicated upgrade tool will be provided to upgrade the schema version to the latest one. All targets in the same pool must have the same schema version. Version checks are performed at system initialization time to enforce this constraint. To limit the validation matrix, each new DAOS release will be published with a list of supported schema versions. To run with the new DAOS release, administrators will then need to upgrade the DAOS system to one of the supported schema versions. New pool shards will always be formatted with the latest version. This versioning schema only applies to a data structure stored in persistent memory and not to block storage that only stores user data with no metadata. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028914 \u21a9","title":"DAOS System Administration"},{"location":"QSG/admin/#daos-system-administration","text":"","title":"DAOS System Administration"},{"location":"QSG/admin/#system-ras-events","text":"Reliability, Availability and Serviceability (RAS) related events are communicated and logged within DAOS.","title":"System RAS Events"},{"location":"QSG/admin/#ras-event-structure","text":"The following table describes the structure of a DAOS RAS event including descriptions of mandatory and optional fields. Field Optional/Mandatory Description ID Mandatory Unique event identifier referenced in the manual. Type Mandatory Event type of STATE_CHANGE causes an update to the Management Service (MS) database in addition to event being written to SYSLOG. INFO_ONLY type events are only written to SYSLOG. Timestamp Mandatory Resolution at the microseconds and include the timezone offset to avoid locality issues. Severity Mandatory Indicates event severity, Error/Warning/Notice. Msg Mandatory Human readable message. HID Optional Identify hardware component involved in the event. E.g. PCI address for SSD, network interface Rank Optional DAOS rank involved in the event. PID Optional Identifier of the process involved in the RAS event TID Optional Identifier of the thread involved in the RAS event. JOBID Optional Identifier of the job involved in the RAS event. Hostname Optional Hostname of the node involved in the event. PUUID Optional Pool UUID involved in the event, if any. CUUID Optional Container UUID involved in the event, if relevant. OID Optional Object identifier involved in the event, if relevant. Control Operation Optional Recommended automatic action, if any. Data Optional Specific instance data treated as a blob.","title":"RAS Event Structure"},{"location":"QSG/admin/#ras-event-ids","text":"The following table lists supported DAOS RAS events including IDs, type, severity, message, description and cause. Event Event type Severity Message Description Cause engine_format required INFO_ONLY NOTICE DAOS engine <idx> requires a <type> format Indicates engine is waiting for allocated storage to be formatted on formatted on instance <idx> with dmg tool. <type> can be either SCM or Metadata. DAOS server attempts to bring-up an engine which has unformatted storage. engine_died STATE_CHANGE ERROR DAOS engine <idx> exited exited unexpectedly: <error> Indicates engine instance <idx> unexpectedly. describes the exit state returned from exited daos_engine process. N/A engine_asserted STATE_CHANGE ERROR TBD Indicates engine instance threw a runtime assertion, causing a crash. An unexpected internal state resulted in assert failure. engine_clock_drift INFO_ONLY ERROR clock drift detected Indicates CART comms layer has detected clock skew between engines. NTP may not be syncing clocks across DAOS system. pool_rebuild_started INFO_ONLY NOTICE Pool rebuild started. Indicates a pool rebuild has started. Event data field contains pool map version and pool operation identifier. When a pool rank becomes unavailable a rebuild will be triggered. pool_rebuild_finished INFO_ONLY NOTICE Pool rebuild finished. Indicates a pool rebuild has finished successfully. Event data field includes the pool map version and pool operation identifier. N/A pool_rebuild_failed INFO_ONLY ERROR Pool rebuild failed: <rc>. Indicates a pool rebuild has failed. Event data field includes the pool map version and pool operation identifier. <rc> provides a string representation of DER code. N/A pool_replicas_updated STATE_CHANGE NOTICE List of pool service replica ranks has been updated. Indicates a pool service replica list has changed. The event contains the new service replica list in a custom payload. When a pool service replica rank becomes unavailable a new rank is selected to replace it (if available). pool_durable_format_incompat INFO_ONLY ERROR incompatible layout version: <current> not in [<min>, <max>] Indicates the given pool's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with pool data in local storage that has an incompatible layout version. container_durable_format_incompat INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>] Indicates the given container's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with container data in local storage that has an incompatible layout version. rdb_durable_format_incompatible INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>]] Indicates the given rdb's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with rdb data in local storage that has an incompatible layout version. swim_rank_alive STATE_CHANGE NOTICE TBD The SWIM protocol has detected the specified rank is responsive. A remote DAOS engine has become responsive. swim_rank_dead STATE_CHANGE NOTICE SWIM rank marked as dead. The SWIM protocol has detected the specified rank is unresponsive. A remote DAOS engine has become unresponsive. system_start_failed INFO_ONLY ERROR System startup failed, <errors> Indicates that a user initiated controlled startup failed. <errors> shows which ranks failed. Ranks failed to start. system_stop_failed INFO_ONLY ERROR System shutdown failed during <action> action, <errors> Indicates that a user initiated controlled shutdown failed. <action> identifies the failing shutdown action and <errors> shows which ranks failed. Ranks failed to stop.","title":"RAS Event IDs"},{"location":"QSG/admin/#system-monitoring","text":"System monitoring and telemetry data will be provided as part of the control plane and will be documented in a future revision.","title":"System Monitoring"},{"location":"QSG/admin/#storage-operations","text":"","title":"Storage Operations"},{"location":"QSG/admin/#space-utilization","text":"To query SCM and NVMe storage space usage and show how much space is available to create new DAOS pools with, run the following command: bash-4.2$ dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- wolf-71 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % wolf-72 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % The command output shows online DAOS storage utilization, only including storage statistics for devices that have been formatted by DAOS control-plane and assigned to a currently running rank of the DAOS system. This represents the storage that can host DAOS pools. Note that the table values are per-host (storage server) and SCM/NVMe capacity pool component values specified in dmg pool create are per rank. If multiple ranks (I/O processes) have been configured per host in the server configuration file daos_server.yml then the values supplied to dmg pool create should be a maximum of the SCM/NVMe free space divided by the number of ranks per host. For example if 2.0 TB SCM and 10.0 TB NVMe free space is reported by dmg storage query usage and the server configuration file used to start the system specifies 2 I/O processes (2 \"server\" sections), the maximum pool size that can be specified is approximately dmg pool create -s 1T -n 5T (may need to specify slightly below the maximum to take account of negligible metadata overhead).","title":"Space Utilization"},{"location":"QSG/admin/#storage-scrubbing","text":"Support for end-to-end data integrity is planned for DAOS v1.2 and background checksum scrubbing for v2.2. Once available, that functionality will be documented here.","title":"Storage Scrubbing"},{"location":"QSG/admin/#ssd-management","text":"","title":"SSD Management"},{"location":"QSG/admin/#health-monitoring","text":"Useful admin dmg commands to query NVMe SSD health: Query Per-Server Metadata: dmg storage query (list-devices|list-pools) dmg storage scan --nvme-meta shows mapping of metadata to NVMe controllers The NVMe storage query list-devices and list-pools commands query the persistently stored SMD device and pool tables respectively. The device table maps the internal device UUID to attached VOS target IDs. The rank number of the server where the device is located is also listed, along with the current device state. The current device states are the following: - NORMAL: a fully, functional device in-use by DAOS - EVICTED: the device is no longer in-use by DAOS - UNPLUGGED: the device is currently unplugged from the system (may or not be evicted) - NEW: the device is plugged and available, and not currently in-use by DAOS The transport address is also listed for the device. This is either the PCIe address for normal NVMe SSDs, or the BDF format address of the backing NVMe SSDs behind a VMD (Volume Management Device) address. In the example below, the last two listed devices are both VMD devices with transport addresses in the BDF format behind the VMD address 0000:5d:05.5. The pool table maps the DAOS pool UUID to attached VOS target IDs and will list all of the server ranks that the pool is distributed on. With the additional verbose flag, the mapping of SPDK blob IDs to VOS target IDs will also be displayed. $ dmg -l boro-11,boro-13 storage query list-devices ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 2] Rank:0 State:NORMAL UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b [TrAddr:0000:8b:00.0] Targets:[1 3] Rank:0 State:NORMAL UUID:051b77e4-1524-4662-9f32-f8e4d2542c2d [TrAddr:0000:8c:00.0] Targets:[] Rank:0 State:NEW UUID:81905b24-be44-4106-8ff9-03002e9dd86a [TrAddr:5d0505:01:00.0] Targets:[0 2] Rank:1 State:EVICTED UUID:2ccb8afb-5d32-454e-86e3-762ec5dca7be [TrAddr:5d0505:03:00.0] Targets:[1 3] Rank:1 State:NORMAL $ dmg -l boro-11,boro-13 storage query list-pools ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Rank:1 Targets:[0 1 2 3] $ dmg -l boro-11,boro-13 storage query list-pools --verbose ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Blobs:[4294967404 4294967405 4294967407 4294967406] Rank:1 Targets:[0 1 2 3] Blobs:[4294967410 4294967411 4294967413 4294967412] Query Storage Device Health Data: dmg storage query (device-health|target-health) dmg storage scan --nvme-health shows NVMe controller health stats The NVMe storage query device-health and target-health commands query the device health data, including NVMe SSD health stats and in-memory I/O error and checksum error counters. The server rank and device state are also listed. The device health data can either be queried by device UUID (device-health command) or by VOS target ID along with the server rank (target-health command). The same device health information is displayed with both command options. $ dmg -l boro-11 storage query device-health --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 or $ dmg -l boro-11 storage query target-health --rank=0 --tgtid=0 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 1 2 3] Rank:0 State:NORMAL Health Stats: Temperature:289K(15C) Controller Busy Time:0s Power Cycles:0 Power On Duration:0s Unsafe Shutdowns:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK","title":"Health Monitoring"},{"location":"QSG/admin/#eviction-and-hotplug","text":"Manually Evict an NVMe SSD: dmg storage set nvme-faulty To manually evict an NVMe SSD (auto eviction will be supported in a future release), the device state needs to be set to \"FAULTY\" by running the following command: $ dmg -l boro-11 storage set nvme-faulty --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:FAULTY The device state will transition from \"NORMAL\" to \"FAULTY\" (shown above), which will trigger the faulty device reaction (all targets on the SSD will be rebuilt and the SSD will remain evicted until device replacement occurs). Note Full NVMe hot plug capability will be available and supported in DAOS 2.2 release. Use is currently intended for testing only and is not supported for production. Replace an Evicted SSD with a New Device: dmg storage replace nvme To replace an NVMe SSD with an evicted device and reintegrate it into use with DAOS, run the following command: $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=80c9f1be-84b9-4318-a1be-c416c96ca48b ------- boro-11 ------- Devices UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b Targets:[] Rank:1 State:NORMAL The old, now replaced device will remain in an \"EVICTED\" state until it is unplugged. The new device will transition from a \"NEW\" state to a \"NORMAL\" state (shown above). Reuse a FAULTY Device: dmg storage replace nvme In order to reuse a device that was previously set as FAULTY and evicted from the DAOS system, an admin can run the following command (setting the old device UUID to be the new device UUID): $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:NORMAL The FAULTY device will transition from an \"EVICTED\" state back to a \"NORMAL\" state, and will again be available for use with DAOS. The use case of this command will mainly be for testing, or for accidental device eviction.","title":"Eviction and Hotplug"},{"location":"QSG/admin/#identification","text":"The SSD identification feature is simply a way to quickly and visually locate a device. It requires the use of Intel VMD (Volume Management Device), which needs to be physically available on the hardware as well as enabled in the system BIOS. The feature supports two LED device events: locating a healthy device and locating an evicted device. Locate a Healthy SSD: dmg storage identify vmd To quickly identify an SSD in question, an administrator can run the following command: $ dmg -l boro-11 storage identify vmd --uuid=6fccb374-413b-441a-bfbe-860099ac5e8d If a non-VMD device UUID is used with the command, the following error will occur: localhost DAOS error (-1010): DER_NOSYS The status LED on the VMD device is now set to an \"IDENTIFY\" state, represented by a quick, 4Hz blinking amber light. The device will quickly blink by default for about 60 seconds and then return to the default \"OFF\" state. The LED event duration can be customized by setting the VMD_LED_PERIOD environment variable if a duration other than the default value is desired. Locate an Evicted SSD: If an NVMe SSD is evicted, the status LED on the VMD device is set to a \"FAULT\" state, represented by a solidly ON amber light. No additional command apart from the SSD eviction command would be needed, and this would visually indicate that the device needs to be replaced and is no longer in use by DAOS. The LED of the VMD device would remain in this state until replaced by a new device.","title":"Identification"},{"location":"QSG/admin/#system-operations","text":"The DAOS Control Server acting as the access point records details of DAOS I/O Server instances that join the DAOS system. Once an I/O Engine has joined the DAOS system, it is identified by a unique system \"rank\". Multiple ranks can reside on the same host machine, accessible via the same network address. A DAOS system can be shutdown and restarted to perform maintenance and/or reboot hosts. Pool data and state will be maintained providing no changes are made to the rank's metadata stored on persistent memory. Storage reformat can also be performed after system shutdown. Pools will be removed and storage wiped. System commands will be handled by the DAOS Server listening at the access point address specified as the first entry in the DMG config file \"hostlist\" parameter. See daos_control.yml for details. The \"access point\" address should be the same as that specified in the server config file daos_server.yml specified when starting daos_server instances. Warning Controlled start/stop/reformat have some known limitations. Whilst individual system instances can be stopped, if a subset is restarted, existing pools will not be automatically integrated with restarted instances.","title":"System Operations"},{"location":"QSG/admin/#membership","text":"The system membership can be queried using the command: $ dmg system query [--verbose] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] --verbose flag gives more information on each rank Output table will provide system rank mappings to host address and instance UUID, in addition to rank state.","title":"Membership"},{"location":"QSG/admin/#shutdown","text":"When up and running, the entire system can be shutdown with the command: $ dmg system stop [--force] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS Control Servers will continue to operate and listen on the management network.","title":"Shutdown"},{"location":"QSG/admin/#start","text":"To start the system after a controlled shutdown run the command: $ dmg system start [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS I/O Engines will be started.","title":"Start"},{"location":"QSG/admin/#reformat","text":"To reformat the system after a controlled shutdown run the command: $ dmg storage format --reformat --reformat flag indicates that a reformat operation should be performed disregarding existing filesystems if no record of previously running ranks can be found, reformat is performed on hosts in dmg config file hostlist if system membership has records of previously running ranks, storage allocated to those ranks will be formatted Output table will indicate action and result. DAOS I/O Engines will be started and all DAOS pools will have been removed.","title":"Reformat"},{"location":"QSG/admin/#manual-fresh-start","text":"To reset the DAOS metadata across all hosts, the system must be reformatted. First, ensure all daos_server processes on all hosts have been stopped, then for each SCM mount specified in the config file ( scm_mount in the servers section) umount and wipe FS signatures. Example illustration with two IO instances specified in the config file: clush -w wolf-[118-121,130-133] umount /mnt/daos1 clush -w wolf-[118-121,130-133] umount /mnt/daos0 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem1 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem0 Then restart DAOS Servers and format.","title":"Manual Fresh Start"},{"location":"QSG/admin/#fault-domain","text":"Details on how to drain an individual storage node or fault domain (e.g. rack) in preparation for maintenance activity and how to reintegrate it will be provided in a future revision.","title":"Fault Domain"},{"location":"QSG/admin/#system-extension","text":"Ability to add new DAOS server instances to a pre-existing DAOS system will be documented in a future revision.","title":"System Extension"},{"location":"QSG/admin/#fault-management","text":"DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains.","title":"Fault Management"},{"location":"QSG/admin/#fault-detection-isolation","text":"DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM 1 that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure makes an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Once detected, the faulty target or servers (effectively a set of targets) must be excluded from each pool membership. This process is triggered either manually by the administrator or automatically (see the next section for more information). Upon exclusion from the pool map, each target starts the collective rebuild process automatically to restore data redundancy. The rebuild process is designed to operate online while servers continue to process incoming I/O operations from applications. Tools to monitor and manage rebuild are still under development.","title":"Fault Detection &amp; Isolation"},{"location":"QSG/admin/#rebuild-throttling","text":"The rebuild process may consume many resources on each server and can be throttled to reduce the impact on application performance. This current logic relies on CPU cycles on the storage nodes. By default, the rebuild process is configured to consume up to 30% of the CPU cycles, leaving the other 70% for regular I/O operations. During the rebuild process, the user can set the throttle to guarantee that the rebuild will not use more resources than the user setting. The user can only set the CPU cycle for now. For example, if the user set the throttle to 50, then the rebuild will at most use 50% of the CPU cycle to do the rebuild job. The default rebuild throttle for CPU cycle is 30. This parameter can be changed via the daos_mgmt_set_params() API call and will be eventually available through the management tools.","title":"Rebuild Throttling"},{"location":"QSG/admin/#software-upgrade","text":"Interoperability in DAOS is handled via protocol and schema versioning for persistent data structures. Further instructions on how to manage DAOS software upgrades will be provided in a future revision.","title":"Software Upgrade"},{"location":"QSG/admin/#protocol-interoperability","text":"Limited protocol interoperability is provided by the DAOS storage stack. Version compatibility checks will be performed to verify that: All targets in the same pool run the same protocol version. Client libraries linked with the application may be up to one protocol version older than the targets. If a protocol version mismatch is detected among storage targets in the same pool, the entire DAOS system will fail to start up and will report failure to the control API. Similarly, the connection from clients running a protocol version incompatible with the targets will return an error.","title":"Protocol Interoperability"},{"location":"QSG/admin/#persistent-layout","text":"The schema of persistent data structures may evolve from time to time to fix bugs, add new optimizations, or support new features. To that end, the persistent data structures support schema versioning. Upgrading the schema version will not be performed automatically and must be initiated by the administrator. A dedicated upgrade tool will be provided to upgrade the schema version to the latest one. All targets in the same pool must have the same schema version. Version checks are performed at system initialization time to enforce this constraint. To limit the validation matrix, each new DAOS release will be published with a list of supported schema versions. To run with the new DAOS release, administrators will then need to upgrade the DAOS system to one of the supported schema versions. New pool shards will always be formatted with the latest version. This versioning schema only applies to a data structure stored in persistent memory and not to block storage that only stores user data with no metadata. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028914 \u21a9","title":"Persistent Layout"},{"location":"QSG/autotest/","text":"Run DAO Autotest \u00b6 DAOS autotest tests the proper setup of the DAOS configuration, which is used to test the setup. DAOS autotest performs various activities like connecting to a pool, creating and opening a container, then reading and writing to the container. # create pool dmg pool create --size=50G # sample output Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 6.00% SCM/NVMe ratio --------------------------------------- UUID : 6af46954-f704-45f0-8b80-115ef855a065 Service Ranks : [1-3] Storage Ranks : [0-3] Total Size : 53 GB SCM : 3.0 GB (750 MB / rank) NVMe : 50 GB (12 GB / rank) # assign pool uuid to a variable export DAOS_POOL=<pool uuid> # run daos autotest daos pool autotest --pool $DAOS_POOL # Sample output Step Operation Status Time(sec) Comment 0 Initializing DAOS OK 0.000 1 Connecting to pool OK 0.070 2 Creating container OK 0.000 uuid = 3 Opening container OK 0.050 10 Generating 1M S1 layouts OK 4.620 11 Generating 10K SX layouts OK 0.140 20 Inserting 1M 128B values OK 75.130 21 Reading 128B values back OK 71.540 24 Inserting 1M 4KB values OK 109.190 25 Reading 4KB values back OK 103.620 28 Inserting 100K 1MB values OK 413.730 29 Reading 1MB values back OK 461.220 96 Closing container OK 0.040 97 Destroying container OK 0.070 98 Disconnecting from pool OK 0.000 99 Tearing down DAOS OK 0.000 Clean Up \u00b6 Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Run DAOS Autotest"},{"location":"QSG/autotest/#run-dao-autotest","text":"DAOS autotest tests the proper setup of the DAOS configuration, which is used to test the setup. DAOS autotest performs various activities like connecting to a pool, creating and opening a container, then reading and writing to the container. # create pool dmg pool create --size=50G # sample output Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 6.00% SCM/NVMe ratio --------------------------------------- UUID : 6af46954-f704-45f0-8b80-115ef855a065 Service Ranks : [1-3] Storage Ranks : [0-3] Total Size : 53 GB SCM : 3.0 GB (750 MB / rank) NVMe : 50 GB (12 GB / rank) # assign pool uuid to a variable export DAOS_POOL=<pool uuid> # run daos autotest daos pool autotest --pool $DAOS_POOL # Sample output Step Operation Status Time(sec) Comment 0 Initializing DAOS OK 0.000 1 Connecting to pool OK 0.070 2 Creating container OK 0.000 uuid = 3 Opening container OK 0.050 10 Generating 1M S1 layouts OK 4.620 11 Generating 10K SX layouts OK 0.140 20 Inserting 1M 128B values OK 75.130 21 Reading 128B values back OK 71.540 24 Inserting 1M 4KB values OK 109.190 25 Reading 4KB values back OK 103.620 28 Inserting 100K 1MB values OK 413.730 29 Reading 1MB values back OK 461.220 96 Closing container OK 0.040 97 Destroying container OK 0.070 98 Disconnecting from pool OK 0.000 99 Tearing down DAOS OK 0.000","title":"Run DAO Autotest"},{"location":"QSG/autotest/#clean-up","text":"Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Clean Up"},{"location":"QSG/cartselftest/","text":"Run CaRT Self_test \u00b6 On the client, run the cart self_test to verify basic network connectivity. # set env SHARED_DIR=<shared dir by all nodes> export FI_UNIVERSE_SIZE=2048 export OFI_INTERFACE=ib0 # generate the attach info file daos_agent -o /etc/daos/daos_agent.yml -l $SHARED_DIR/daos_agent.log dump-attachinfo -o $SHARED_DIR/daos_server.attach_info_tmp # selt_test --help for more details on params # for 4 servers --endpoint 0-3:0-1 ranks:tags. self_test --path $SHARED_DIR --group-name daos_server --endpoint 0-1:0-1 Adding endpoints: ranks: 0-1 (# ranks = 2) tags: 0-1 (# tags = 2) Warning: No --master-endpoint specified; using this command line application as the master endpoint Self Test Parameters: Group name to test against: daos_server # endpoints: 4 Message sizes: [(200000-BULK_GET 200000-BULK_PUT), (200000-BULK_GET 0-EMPTY), (0-EMPTY 200000-BULK_PUT), (200000-BULK_GET 1000-IOV), (1000-IOV 200000-BULK_PUT), (1000-IOV 1000-IOV), (1000-IOV 0-EMPTY), (0-EMPTY 1000-IOV), (0-EMPTY 0-EMPTY)] Buffer addresses end with: <Default> Repetitions per size: 40000 Max inflight RPCs: 1000 CLI [rank=0 pid=40050] Attached daos_server ################################################## Results for message size (200000-BULK_GET 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 197.56 RPC Throughput (RPCs/sec): 518 RPC Latencies (us): Min : 38791 25th %: 1695365 Median : 1916632 75th %: 2144087 Max : 2969361 Average: 1907415 Std Dev: 373832.81 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 1889518 0:1 - 1712934 1:0 - 1924995 1:1 - 2110649 ################################################## Results for message size (200000-BULK_GET 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.03 RPC Throughput (RPCs/sec): 587 RPC Latencies (us): Min : 4783 25th %: 1480053 Median : 1688064 75th %: 1897392 Max : 2276555 Average: 1681303 Std Dev: 314999.11 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2001222 0:1 - 1793990 1:0 - 1385306 1:1 - 1593675 ################################################## Results for message size (0-EMPTY 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.12 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 6302 25th %: 1063532 Median : 1654468 75th %: 2287784 Max : 3488227 Average: 1680617 Std Dev: 880402.68 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 1251585 0:1 - 1323953 1:0 - 2099173 1:1 - 2043352 ################################################## Results for message size (200000-BULK_GET 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.54 RPC Throughput (RPCs/sec): 587 RPC Latencies (us): Min : 5426 25th %: 1395359 Median : 1687404 75th %: 1983402 Max : 2426175 Average: 1681970 Std Dev: 393256.99 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2077476 0:1 - 1870102 1:0 - 1318136 1:1 - 1529193 ################################################## Results for message size (1000-IOV 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.66 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 5340 25th %: 442729 Median : 1221371 75th %: 2936906 Max : 3502405 Average: 1681142 Std Dev: 1308472.80 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 3006315 0:1 - 2913808 1:0 - 434763 1:1 - 465469 ################################################## Results for message size (1000-IOV 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 80.71 RPC Throughput (RPCs/sec): 42315 RPC Latencies (us): Min : 1187 25th %: 20187 Median : 23322 75th %: 26833 Max : 30246 Average: 23319 Std Dev: 4339.87 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 26828 0:1 - 26839 1:0 - 20275 1:1 - 20306 ################################################## Results for message size (1000-IOV 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 42.68 RPC Throughput (RPCs/sec): 44758 RPC Latencies (us): Min : 935 25th %: 15880 Median : 21444 75th %: 28434 Max : 34551 Average: 22035 Std Dev: 7234.26 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 28418 0:1 - 28449 1:0 - 16301 1:1 - 16318 ################################################## Results for message size (0-EMPTY 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 42.91 RPC Throughput (RPCs/sec): 44991 RPC Latencies (us): Min : 789 25th %: 20224 Median : 22195 75th %: 24001 Max : 26270 Average: 21943 Std Dev: 3039.50 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 24017 0:1 - 23987 1:0 - 20279 1:1 - 20309 ################################################## Results for message size (0-EMPTY 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 0.00 RPC Throughput (RPCs/sec): 47807 RPC Latencies (us): Min : 774 25th %: 16161 Median : 20419 75th %: 25102 Max : 29799 Average: 20633 Std Dev: 5401.96 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 25103 0:1 - 25099 1:0 - 16401 1:1 - 16421","title":"Run CaRT Self_test"},{"location":"QSG/cartselftest/#run-cart-self_test","text":"On the client, run the cart self_test to verify basic network connectivity. # set env SHARED_DIR=<shared dir by all nodes> export FI_UNIVERSE_SIZE=2048 export OFI_INTERFACE=ib0 # generate the attach info file daos_agent -o /etc/daos/daos_agent.yml -l $SHARED_DIR/daos_agent.log dump-attachinfo -o $SHARED_DIR/daos_server.attach_info_tmp # selt_test --help for more details on params # for 4 servers --endpoint 0-3:0-1 ranks:tags. self_test --path $SHARED_DIR --group-name daos_server --endpoint 0-1:0-1 Adding endpoints: ranks: 0-1 (# ranks = 2) tags: 0-1 (# tags = 2) Warning: No --master-endpoint specified; using this command line application as the master endpoint Self Test Parameters: Group name to test against: daos_server # endpoints: 4 Message sizes: [(200000-BULK_GET 200000-BULK_PUT), (200000-BULK_GET 0-EMPTY), (0-EMPTY 200000-BULK_PUT), (200000-BULK_GET 1000-IOV), (1000-IOV 200000-BULK_PUT), (1000-IOV 1000-IOV), (1000-IOV 0-EMPTY), (0-EMPTY 1000-IOV), (0-EMPTY 0-EMPTY)] Buffer addresses end with: <Default> Repetitions per size: 40000 Max inflight RPCs: 1000 CLI [rank=0 pid=40050] Attached daos_server ################################################## Results for message size (200000-BULK_GET 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 197.56 RPC Throughput (RPCs/sec): 518 RPC Latencies (us): Min : 38791 25th %: 1695365 Median : 1916632 75th %: 2144087 Max : 2969361 Average: 1907415 Std Dev: 373832.81 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 1889518 0:1 - 1712934 1:0 - 1924995 1:1 - 2110649 ################################################## Results for message size (200000-BULK_GET 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.03 RPC Throughput (RPCs/sec): 587 RPC Latencies (us): Min : 4783 25th %: 1480053 Median : 1688064 75th %: 1897392 Max : 2276555 Average: 1681303 Std Dev: 314999.11 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2001222 0:1 - 1793990 1:0 - 1385306 1:1 - 1593675 ################################################## Results for message size (0-EMPTY 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.12 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 6302 25th %: 1063532 Median : 1654468 75th %: 2287784 Max : 3488227 Average: 1680617 Std Dev: 880402.68 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 1251585 0:1 - 1323953 1:0 - 2099173 1:1 - 2043352 ################################################## Results for message size (200000-BULK_GET 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.54 RPC Throughput (RPCs/sec): 587 RPC Latencies (us): Min : 5426 25th %: 1395359 Median : 1687404 75th %: 1983402 Max : 2426175 Average: 1681970 Std Dev: 393256.99 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2077476 0:1 - 1870102 1:0 - 1318136 1:1 - 1529193 ################################################## Results for message size (1000-IOV 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.66 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 5340 25th %: 442729 Median : 1221371 75th %: 2936906 Max : 3502405 Average: 1681142 Std Dev: 1308472.80 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 3006315 0:1 - 2913808 1:0 - 434763 1:1 - 465469 ################################################## Results for message size (1000-IOV 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 80.71 RPC Throughput (RPCs/sec): 42315 RPC Latencies (us): Min : 1187 25th %: 20187 Median : 23322 75th %: 26833 Max : 30246 Average: 23319 Std Dev: 4339.87 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 26828 0:1 - 26839 1:0 - 20275 1:1 - 20306 ################################################## Results for message size (1000-IOV 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 42.68 RPC Throughput (RPCs/sec): 44758 RPC Latencies (us): Min : 935 25th %: 15880 Median : 21444 75th %: 28434 Max : 34551 Average: 22035 Std Dev: 7234.26 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 28418 0:1 - 28449 1:0 - 16301 1:1 - 16318 ################################################## Results for message size (0-EMPTY 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 42.91 RPC Throughput (RPCs/sec): 44991 RPC Latencies (us): Min : 789 25th %: 20224 Median : 22195 75th %: 24001 Max : 26270 Average: 21943 Std Dev: 3039.50 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 24017 0:1 - 23987 1:0 - 20279 1:1 - 20309 ################################################## Results for message size (0-EMPTY 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 0.00 RPC Throughput (RPCs/sec): 47807 RPC Latencies (us): Min : 774 25th %: 16161 Median : 20419 75th %: 25102 Max : 29799 Average: 20633 Std Dev: 5401.96 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 25103 0:1 - 25099 1:0 - 16401 1:1 - 16421","title":"Run CaRT Self_test"},{"location":"QSG/datamover/","text":"DataMover \u00b6 DataMover using 'daos' utility (Single process) \u00b6 Create Second container: # Create Second container daos container create --pool $DAOS_POOL --type POSIX Successfully created container 158469db-70d2-4a5d-aac9-3c06cbfa7459 export DAOS_CONT2=<cont uuid> Pool Query before copy: dmg -o /etc/daos/daos_control.yml pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 48 GB, min:743 MB, max:744 MB, mean:744 MB - NVMe: Total size: 800 GB Free: 499 GB, min:7.7 GB, max:7.9 GB, mean:7.8 GB Rebuild idle, 0 objs, 0 recs Move data from POSIX directory into a DAOS container: Daos 1.2 only supports directory copy if using daos filesystem copy # moving everything under /tmp/daos_dfuse to new cont $DAOS_CONT2 daos filesystem copy --src /tmp/daos_dfuse/ --dst daos://$DAOS_POOL/$DAOS_CONT2 Successfully copied to DAOS: / Pool Query to confirm data got copied (Free space has reduced from last pool query): dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:738 MB, max:739 MB, mean:739 MB - NVMe: Total size: 800 GB Free: 338 GB, min:5.1 GB, max:5.5 GB, mean:5.3 GB Rebuild idle, 0 objs, 0 recs Move data from DAOS container to POSIX directory: mkdir /tmp/daos_dfuse/daos_container_copy daos filesystem copy --src daos://$DAOS_POOL/$DAOS_CONT2 --dst /tmp/daos_dfuse/daos_container_copy mkdir /tmp/daos_dfuse/daos_cont_copy// failed, File exists Successfully copied to POSIX: /tmp/daos_dfuse/daos_cont_copy/ Pool Query to confirm data got copied: dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:732 MB, max:733 MB, mean:732 MB - NVMe: Total size: 800 GB Free: 128 GB, min:1.8 GB, max:2.3 GB, mean:2.0 GB Rebuild idle, 0 objs, 0 recs Check data inside the POSIX directories: ls -latr /tmp/daos_dfuse/ total 157286400 -rw-rw-r-- 1 standan standan 161061273600 Apr 29 23:23 testfile drwxrwxr-x 1 standan standan 64 Apr 29 23:28 test-dir.0-0 drwxrwxr-x 1 standan standan 64 Apr 29 23:30 clients drwxrwxr-x 1 standan standan 64 Apr 30 00:02 daos_container_copy ls -latr /tmp/daos_dfuse/daos_container_copy drwx------ 1 standan standan 64 Apr 30 00:02 daos_dfuse drwx------ 1 standan standan 64 Apr 30 00:11 testfile DataMover using mpifileutils (allows you to move data in multi-process mode) \u00b6 Build mpifileutils package: # load mpich module or set it's path in your environment module load mpi/mpich-x86_64 or export LD_LIBRARY_PATH=<mpich lib path>:$LD_LIBRARY_PATH export PATH=<mpich bin path>:$PATH # install daos-devel, if missing sudo yum install -y daos-devel # Build Dependencies mkdir install installdir=`pwd`/install mkdir deps cd deps wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz wget https://github.com/llnl/lwgrp/releases/download/v1.0.2/lwgrp-1.0.2.tar.gz wget https://github.com/llnl/dtcmp/releases/download/v1.1.0/dtcmp-1.1.0.tar.gz tar -zxf libcircle-0.3.0.tar.gz cd libcircle-0.3.0 ./configure --prefix=$installdir make install cd .. tar -zxf lwgrp-1.0.2.tar.gz cd lwgrp-1.0.2 ./configure --prefix=$installdir make install cd .. tar -zxf dtcmp-1.1.0.tar.gz cd dtcmp-1.1.0 ./configure --prefix=$installdir --with-lwgrp=$installdir make install cd .. cd .. # Build mpifileutils git clone https://github.com/hpc/mpifileutils mkdir build cd build cmake3 ../mpifileutils/ -DWITH_DTCMP_PREFIX=<path/to/dtcmp/install> -DWITH_LibCircle_PREFIX=<path/to/lib/circle/install> -DWITH_CART_PREFIX=/usr/ -DWITH_DAOS_PREFIX=/usr/ -DCMAKE_INSTALL_PREFIX=<path/where/mpifileutils/need/to/be/installed> -DENABLE_DAOS=ON -DENABLE_LIBARCHIVE=OFF make install # On launch node set mpifileutils LD_LIBRARY_PATH and PATH export LD_LIBRARY_PATH=<mpifileutils/lib/path>:$LD_LIBRARY_PATH export PATH=<mpifileutils/bin/path>:$PATH Create Second container: daos container create --pool $DAOS_POOL --type POSIX Successfully created container caf0135c-def8-45a5-bac3-d0b969e67c8b export DAOS_CONT2=<cont uuid> Move data from POSIX directory into a DAOS container: mpirun -hostfile /path/to/hostfile -np 16 /path/to/mpifileutils/install/bin/dcp --bufsize 64MB --chunksize 128MB /tmp/daos_dfuse daos://$DAOS_POOL/$DAOS_CONT2/ [2021-04-30T01:16:48] Walking /tmp/daos_dfuse [2021-04-30T01:16:58] Walked 24207 items in 10.030 secs (2413.415 items/sec) ... [2021-04-30T01:17:01] Walked 34245 items in 13.298 secs (2575.138 items/sec) ... [2021-04-30T01:17:01] Walked 34245 items in 13.300 seconds (2574.867 items/sec) [2021-04-30T01:17:01] Copying to / [2021-04-30T01:17:01] Items: 34245 [2021-04-30T01:17:01] Directories: 904 [2021-04-30T01:17:01] Files: 33341 [2021-04-30T01:17:01] Links: 0 [2021-04-30T01:17:01] Data: 150.127 GiB (4.611 MiB per file) [2021-04-30T01:17:01] Creating 904 directories [2021-04-30T01:17:01] Creating 33341 files. [2021-04-30T01:17:02] Copying data. [2021-04-30T01:17:12] Copied 4.049 GiB (3%) in 10.395 secs (398.867 MiB/s) 375 secs left ... [2021-04-30T01:22:37] Copied 8.561 GiB (6%) in 335.113 secs (26.160 MiB/s) 5541 secs left ... [2021-04-30T01:22:37] Copied 150.127 GiB (100%) in 335.113 secs (458.742 MiB/s) done [2021-04-30T01:22:37] Copy data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:22:37] Copy rate: 458.741 MiB/s (161197834240 bytes in 335.114 seconds) [2021-04-30T01:22:37] Syncing data to disk. [2021-04-30T01:22:37] Sync completed in 0.017 seconds. [2021-04-30T01:22:37] Fixing permissions. [2021-04-30T01:22:37] Updated 34245 items in 0.176 seconds (194912.821 items/sec) [2021-04-30T01:22:37] Syncing directory updates to disk. [2021-04-30T01:22:37] Sync completed in 0.012 seconds. [2021-04-30T01:22:37] Started: Apr-30-2021,01:17:01 [2021-04-30T01:22:37] Completed: Apr-30-2021,01:22:37 [2021-04-30T01:22:37] Seconds: 336.013 [2021-04-30T01:22:37] Items: 34245 [2021-04-30T01:22:37] Directories: 904 [2021-04-30T01:22:37] Files: 33341 [2021-04-30T01:22:37] Links: 0 [2021-04-30T01:22:37] Data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:22:37] Rate: 457.513 MiB/s (161197834240 bytes in 336.013 seconds) Pool Query to verify data was copied (free space should reduce): dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:734 MB, max:735 MB, mean:735 MB - NVMe: Total size: 800 GB Free: 338 GB, min:5.2 GB, max:5.4 GB, mean:5.3 GB Rebuild idle, 0 objs, 0 recs Move data from DAOS container to POSIX directory: mkdir /tmp/daos_dfuse/daos_container_copy mpirun -hostfile /path/to/hostfile -np 16 dcp --bufsize 64MB --chunksize 128MB daos://$DAOS_POOL/$DAOS_CONT2/ /tmp/daos_dfuse/daos_container_copy [2021-04-30T01:26:11] Walking / [2021-04-30T01:26:14] Walked 34246 items in 2.648 secs (12930.593 items/sec) ... [2021-04-30T01:26:14] Walked 34246 items in 2.650 seconds (12923.056 items/sec) [2021-04-30T01:26:14] Copying to /tmp/daos_dfuse/daos_container_copy [2021-04-30T01:26:14] Items: 34246 [2021-04-30T01:26:14] Directories: 905 [2021-04-30T01:26:14] Files: 33341 [2021-04-30T01:26:14] Links: 0 [2021-04-30T01:26:14] Data: 150.127 GiB (4.611 MiB per file) [2021-04-30T01:26:14] Creating 905 directories [2021-04-30T01:26:14] Original directory exists, skip the creation: `/tmp/daos_dfuse/daos_container_copy/' (errno=17 File exists) [2021-04-30T01:26:14] Creating 33341 files. [2021-04-30T01:26:19] Copying data. [2021-04-30T01:26:29] Copied 3.819 GiB (3%) in 10.213 secs (382.922 MiB/s) 391 secs left ... [2021-04-30T01:32:02] Copied 150.127 GiB (100%) in 343.861 secs (447.070 MiB/s) done [2021-04-30T01:32:02] Copy data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:32:02] Copy rate: 447.069 MiB/s (161197834240 bytes in 343.862 seconds) [2021-04-30T01:32:02] Syncing data to disk. [2021-04-30T01:32:02] Sync completed in 0.020 seconds. [2021-04-30T01:32:02] Fixing permissions. [2021-04-30T01:32:17] Updated 34162 items in 14.955 secs (2284.295 items/sec) ... [2021-04-30T01:32:17] Updated 34246 items in 14.955 secs (2289.890 items/sec) done [2021-04-30T01:32:17] Updated 34246 items in 14.956 seconds (2289.772 items/sec) [2021-04-30T01:32:17] Syncing directory updates to disk. [2021-04-30T01:32:17] Sync completed in 0.022 seconds. [2021-04-30T01:32:17] Started: Apr-30-2021,01:26:14 [2021-04-30T01:32:17] Completed: Apr-30-2021,01:32:17 [2021-04-30T01:32:17] Seconds: 363.327 [2021-04-30T01:32:17] Items: 34246 [2021-04-30T01:32:17] Directories: 905 [2021-04-30T01:32:17] Files: 33341 [2021-04-30T01:32:17] Links: 0 [2021-04-30T01:32:17] Data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:32:17] Rate: 423.118 MiB/s (161197834240 bytes in 363.327 seconds) Pool Query to very data was copied: dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:728 MB, max:730 MB, mean:729 MB - NVMe: Total size: 800 GB Free: 176 GB, min:2.6 GB, max:3.0 GB, mean:2.8 GB Rebuild idle, 0 objs, 0 recs Check data inside the POSIX directories: ls -latr /tmp/daos_dfuse/ total 157286400 -rw-rw-r-- 1 standan standan 161061273600 Apr 29 23:23 testfile drwxrwxr-x 1 standan standan 64 Apr 29 23:28 test-dir.0-0 drwxrwxr-x 1 standan standan 64 Apr 29 23:30 clients drwxr-xr-x 1 standan standan 64 Apr 30 01:25 daos_container_copy ls -latr /tmp/daos_dfuse/daos_container_copy drwxr-xr-x 1 standan standan 64 Apr 30 01:26 daos_dfuse * *For more details on datamover reference: https://github.com/hpc/mpifileutils/blob/master/DAOS-Support.md Clean Up \u00b6 Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Datamover test"},{"location":"QSG/datamover/#datamover","text":"","title":"DataMover"},{"location":"QSG/datamover/#datamover-using-daos-utility-single-process","text":"Create Second container: # Create Second container daos container create --pool $DAOS_POOL --type POSIX Successfully created container 158469db-70d2-4a5d-aac9-3c06cbfa7459 export DAOS_CONT2=<cont uuid> Pool Query before copy: dmg -o /etc/daos/daos_control.yml pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 48 GB, min:743 MB, max:744 MB, mean:744 MB - NVMe: Total size: 800 GB Free: 499 GB, min:7.7 GB, max:7.9 GB, mean:7.8 GB Rebuild idle, 0 objs, 0 recs Move data from POSIX directory into a DAOS container: Daos 1.2 only supports directory copy if using daos filesystem copy # moving everything under /tmp/daos_dfuse to new cont $DAOS_CONT2 daos filesystem copy --src /tmp/daos_dfuse/ --dst daos://$DAOS_POOL/$DAOS_CONT2 Successfully copied to DAOS: / Pool Query to confirm data got copied (Free space has reduced from last pool query): dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:738 MB, max:739 MB, mean:739 MB - NVMe: Total size: 800 GB Free: 338 GB, min:5.1 GB, max:5.5 GB, mean:5.3 GB Rebuild idle, 0 objs, 0 recs Move data from DAOS container to POSIX directory: mkdir /tmp/daos_dfuse/daos_container_copy daos filesystem copy --src daos://$DAOS_POOL/$DAOS_CONT2 --dst /tmp/daos_dfuse/daos_container_copy mkdir /tmp/daos_dfuse/daos_cont_copy// failed, File exists Successfully copied to POSIX: /tmp/daos_dfuse/daos_cont_copy/ Pool Query to confirm data got copied: dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:732 MB, max:733 MB, mean:732 MB - NVMe: Total size: 800 GB Free: 128 GB, min:1.8 GB, max:2.3 GB, mean:2.0 GB Rebuild idle, 0 objs, 0 recs Check data inside the POSIX directories: ls -latr /tmp/daos_dfuse/ total 157286400 -rw-rw-r-- 1 standan standan 161061273600 Apr 29 23:23 testfile drwxrwxr-x 1 standan standan 64 Apr 29 23:28 test-dir.0-0 drwxrwxr-x 1 standan standan 64 Apr 29 23:30 clients drwxrwxr-x 1 standan standan 64 Apr 30 00:02 daos_container_copy ls -latr /tmp/daos_dfuse/daos_container_copy drwx------ 1 standan standan 64 Apr 30 00:02 daos_dfuse drwx------ 1 standan standan 64 Apr 30 00:11 testfile","title":"DataMover using 'daos' utility (Single process)"},{"location":"QSG/datamover/#datamover-using-mpifileutils-allows-you-to-move-data-in-multi-process-mode","text":"Build mpifileutils package: # load mpich module or set it's path in your environment module load mpi/mpich-x86_64 or export LD_LIBRARY_PATH=<mpich lib path>:$LD_LIBRARY_PATH export PATH=<mpich bin path>:$PATH # install daos-devel, if missing sudo yum install -y daos-devel # Build Dependencies mkdir install installdir=`pwd`/install mkdir deps cd deps wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz wget https://github.com/llnl/lwgrp/releases/download/v1.0.2/lwgrp-1.0.2.tar.gz wget https://github.com/llnl/dtcmp/releases/download/v1.1.0/dtcmp-1.1.0.tar.gz tar -zxf libcircle-0.3.0.tar.gz cd libcircle-0.3.0 ./configure --prefix=$installdir make install cd .. tar -zxf lwgrp-1.0.2.tar.gz cd lwgrp-1.0.2 ./configure --prefix=$installdir make install cd .. tar -zxf dtcmp-1.1.0.tar.gz cd dtcmp-1.1.0 ./configure --prefix=$installdir --with-lwgrp=$installdir make install cd .. cd .. # Build mpifileutils git clone https://github.com/hpc/mpifileutils mkdir build cd build cmake3 ../mpifileutils/ -DWITH_DTCMP_PREFIX=<path/to/dtcmp/install> -DWITH_LibCircle_PREFIX=<path/to/lib/circle/install> -DWITH_CART_PREFIX=/usr/ -DWITH_DAOS_PREFIX=/usr/ -DCMAKE_INSTALL_PREFIX=<path/where/mpifileutils/need/to/be/installed> -DENABLE_DAOS=ON -DENABLE_LIBARCHIVE=OFF make install # On launch node set mpifileutils LD_LIBRARY_PATH and PATH export LD_LIBRARY_PATH=<mpifileutils/lib/path>:$LD_LIBRARY_PATH export PATH=<mpifileutils/bin/path>:$PATH Create Second container: daos container create --pool $DAOS_POOL --type POSIX Successfully created container caf0135c-def8-45a5-bac3-d0b969e67c8b export DAOS_CONT2=<cont uuid> Move data from POSIX directory into a DAOS container: mpirun -hostfile /path/to/hostfile -np 16 /path/to/mpifileutils/install/bin/dcp --bufsize 64MB --chunksize 128MB /tmp/daos_dfuse daos://$DAOS_POOL/$DAOS_CONT2/ [2021-04-30T01:16:48] Walking /tmp/daos_dfuse [2021-04-30T01:16:58] Walked 24207 items in 10.030 secs (2413.415 items/sec) ... [2021-04-30T01:17:01] Walked 34245 items in 13.298 secs (2575.138 items/sec) ... [2021-04-30T01:17:01] Walked 34245 items in 13.300 seconds (2574.867 items/sec) [2021-04-30T01:17:01] Copying to / [2021-04-30T01:17:01] Items: 34245 [2021-04-30T01:17:01] Directories: 904 [2021-04-30T01:17:01] Files: 33341 [2021-04-30T01:17:01] Links: 0 [2021-04-30T01:17:01] Data: 150.127 GiB (4.611 MiB per file) [2021-04-30T01:17:01] Creating 904 directories [2021-04-30T01:17:01] Creating 33341 files. [2021-04-30T01:17:02] Copying data. [2021-04-30T01:17:12] Copied 4.049 GiB (3%) in 10.395 secs (398.867 MiB/s) 375 secs left ... [2021-04-30T01:22:37] Copied 8.561 GiB (6%) in 335.113 secs (26.160 MiB/s) 5541 secs left ... [2021-04-30T01:22:37] Copied 150.127 GiB (100%) in 335.113 secs (458.742 MiB/s) done [2021-04-30T01:22:37] Copy data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:22:37] Copy rate: 458.741 MiB/s (161197834240 bytes in 335.114 seconds) [2021-04-30T01:22:37] Syncing data to disk. [2021-04-30T01:22:37] Sync completed in 0.017 seconds. [2021-04-30T01:22:37] Fixing permissions. [2021-04-30T01:22:37] Updated 34245 items in 0.176 seconds (194912.821 items/sec) [2021-04-30T01:22:37] Syncing directory updates to disk. [2021-04-30T01:22:37] Sync completed in 0.012 seconds. [2021-04-30T01:22:37] Started: Apr-30-2021,01:17:01 [2021-04-30T01:22:37] Completed: Apr-30-2021,01:22:37 [2021-04-30T01:22:37] Seconds: 336.013 [2021-04-30T01:22:37] Items: 34245 [2021-04-30T01:22:37] Directories: 904 [2021-04-30T01:22:37] Files: 33341 [2021-04-30T01:22:37] Links: 0 [2021-04-30T01:22:37] Data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:22:37] Rate: 457.513 MiB/s (161197834240 bytes in 336.013 seconds) Pool Query to verify data was copied (free space should reduce): dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:734 MB, max:735 MB, mean:735 MB - NVMe: Total size: 800 GB Free: 338 GB, min:5.2 GB, max:5.4 GB, mean:5.3 GB Rebuild idle, 0 objs, 0 recs Move data from DAOS container to POSIX directory: mkdir /tmp/daos_dfuse/daos_container_copy mpirun -hostfile /path/to/hostfile -np 16 dcp --bufsize 64MB --chunksize 128MB daos://$DAOS_POOL/$DAOS_CONT2/ /tmp/daos_dfuse/daos_container_copy [2021-04-30T01:26:11] Walking / [2021-04-30T01:26:14] Walked 34246 items in 2.648 secs (12930.593 items/sec) ... [2021-04-30T01:26:14] Walked 34246 items in 2.650 seconds (12923.056 items/sec) [2021-04-30T01:26:14] Copying to /tmp/daos_dfuse/daos_container_copy [2021-04-30T01:26:14] Items: 34246 [2021-04-30T01:26:14] Directories: 905 [2021-04-30T01:26:14] Files: 33341 [2021-04-30T01:26:14] Links: 0 [2021-04-30T01:26:14] Data: 150.127 GiB (4.611 MiB per file) [2021-04-30T01:26:14] Creating 905 directories [2021-04-30T01:26:14] Original directory exists, skip the creation: `/tmp/daos_dfuse/daos_container_copy/' (errno=17 File exists) [2021-04-30T01:26:14] Creating 33341 files. [2021-04-30T01:26:19] Copying data. [2021-04-30T01:26:29] Copied 3.819 GiB (3%) in 10.213 secs (382.922 MiB/s) 391 secs left ... [2021-04-30T01:32:02] Copied 150.127 GiB (100%) in 343.861 secs (447.070 MiB/s) done [2021-04-30T01:32:02] Copy data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:32:02] Copy rate: 447.069 MiB/s (161197834240 bytes in 343.862 seconds) [2021-04-30T01:32:02] Syncing data to disk. [2021-04-30T01:32:02] Sync completed in 0.020 seconds. [2021-04-30T01:32:02] Fixing permissions. [2021-04-30T01:32:17] Updated 34162 items in 14.955 secs (2284.295 items/sec) ... [2021-04-30T01:32:17] Updated 34246 items in 14.955 secs (2289.890 items/sec) done [2021-04-30T01:32:17] Updated 34246 items in 14.956 seconds (2289.772 items/sec) [2021-04-30T01:32:17] Syncing directory updates to disk. [2021-04-30T01:32:17] Sync completed in 0.022 seconds. [2021-04-30T01:32:17] Started: Apr-30-2021,01:26:14 [2021-04-30T01:32:17] Completed: Apr-30-2021,01:32:17 [2021-04-30T01:32:17] Seconds: 363.327 [2021-04-30T01:32:17] Items: 34246 [2021-04-30T01:32:17] Directories: 905 [2021-04-30T01:32:17] Files: 33341 [2021-04-30T01:32:17] Links: 0 [2021-04-30T01:32:17] Data: 150.127 GiB (161197834240 bytes) [2021-04-30T01:32:17] Rate: 423.118 MiB/s (161197834240 bytes in 363.327 seconds) Pool Query to very data was copied: dmg pool query --pool $DAOS_POOL Pool b22220ea-740d-46bc-84ad-35ed3a28aa31, ntarget=64, disabled=0, leader=1, version=1 Pool space info: - Target(VOS) count:64 - SCM: Total size: 48 GB Free: 47 GB, min:728 MB, max:730 MB, mean:729 MB - NVMe: Total size: 800 GB Free: 176 GB, min:2.6 GB, max:3.0 GB, mean:2.8 GB Rebuild idle, 0 objs, 0 recs Check data inside the POSIX directories: ls -latr /tmp/daos_dfuse/ total 157286400 -rw-rw-r-- 1 standan standan 161061273600 Apr 29 23:23 testfile drwxrwxr-x 1 standan standan 64 Apr 29 23:28 test-dir.0-0 drwxrwxr-x 1 standan standan 64 Apr 29 23:30 clients drwxr-xr-x 1 standan standan 64 Apr 30 01:25 daos_container_copy ls -latr /tmp/daos_dfuse/daos_container_copy drwxr-xr-x 1 standan standan 64 Apr 30 01:26 daos_dfuse * *For more details on datamover reference: https://github.com/hpc/mpifileutils/blob/master/DAOS-Support.md","title":"DataMover using mpifileutils (allows you to move data in multi-process mode)"},{"location":"QSG/datamover/#clean-up","text":"Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Clean Up"},{"location":"QSG/dbench/","text":"Run dbench \u00b6 Install dbench on all client nodes: \u00b6 sudo yum install dbench From one of the client node: dbench --clients-per-process 10 --directory /tmp/daos_dfuse/ --loadfile /usr/share/dbench/client.txt --timelimit 10 10 dbench version 4.00 - Copyright Andrew Tridgell 1999-2004 Running for 10 seconds with load '/usr/share/dbench/client.txt' and minimum warmup 2 secs failed to create barrier semaphore 9 of 10 processes prepared for launch 0 sec 10 of 10 processes prepared for launch 0 sec releasing clients 0 3 0.00 MB/sec warmup 1 sec latency 826.199 ms 0 7 0.00 MB/sec warmup 2 sec latency 269.284 ms 100 114 288.13 MB/sec execute 1 sec latency 230.428 ms 100 141 184.47 MB/sec execute 2 sec latency 246.159 ms 100 166 147.88 MB/sec execute 3 sec latency 266.298 ms 100 194 133.59 MB/sec execute 4 sec latency 255.767 ms 100 219 121.64 MB/sec execute 5 sec latency 257.980 ms 100 248 117.41 MB/sec execute 6 sec latency 278.191 ms 100 274 112.64 MB/sec execute 7 sec latency 283.694 ms 100 299 107.89 MB/sec execute 8 sec latency 274.483 ms 100 325 104.57 MB/sec execute 9 sec latency 285.639 ms 100 cleanup 10 sec 100 cleanup 11 sec 90 cleanup 12 sec 70 cleanup 13 sec 50 cleanup 14 sec 35 cleanup 15 sec 20 cleanup 16 sec 0 cleanup 17 sec Operation Count AvgLat MaxLat ---------------------------------------- NTCreateX 3877 24.215 170.761 Close 3800 0.004 0.022 Qfileinfo 3110 1.488 4.579 WriteX 18750 0.274 6.484 Throughput 104.568 MB/sec 100 clients 10 procs max_latency=285.639 ms List the dfuse mount point: # 'testfile' comes from ior run # 'test-dir.0-0' comes from mdtest run # 'clients' comes from dbench run ls /tmp/daos_dfuse clients test-dir.0-0 testfile Clean Up \u00b6 Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Run dbench"},{"location":"QSG/dbench/#run-dbench","text":"","title":"Run dbench"},{"location":"QSG/dbench/#install-dbench-on-all-client-nodes","text":"sudo yum install dbench From one of the client node: dbench --clients-per-process 10 --directory /tmp/daos_dfuse/ --loadfile /usr/share/dbench/client.txt --timelimit 10 10 dbench version 4.00 - Copyright Andrew Tridgell 1999-2004 Running for 10 seconds with load '/usr/share/dbench/client.txt' and minimum warmup 2 secs failed to create barrier semaphore 9 of 10 processes prepared for launch 0 sec 10 of 10 processes prepared for launch 0 sec releasing clients 0 3 0.00 MB/sec warmup 1 sec latency 826.199 ms 0 7 0.00 MB/sec warmup 2 sec latency 269.284 ms 100 114 288.13 MB/sec execute 1 sec latency 230.428 ms 100 141 184.47 MB/sec execute 2 sec latency 246.159 ms 100 166 147.88 MB/sec execute 3 sec latency 266.298 ms 100 194 133.59 MB/sec execute 4 sec latency 255.767 ms 100 219 121.64 MB/sec execute 5 sec latency 257.980 ms 100 248 117.41 MB/sec execute 6 sec latency 278.191 ms 100 274 112.64 MB/sec execute 7 sec latency 283.694 ms 100 299 107.89 MB/sec execute 8 sec latency 274.483 ms 100 325 104.57 MB/sec execute 9 sec latency 285.639 ms 100 cleanup 10 sec 100 cleanup 11 sec 90 cleanup 12 sec 70 cleanup 13 sec 50 cleanup 14 sec 35 cleanup 15 sec 20 cleanup 16 sec 0 cleanup 17 sec Operation Count AvgLat MaxLat ---------------------------------------- NTCreateX 3877 24.215 170.761 Close 3800 0.004 0.022 Qfileinfo 3110 1.488 4.579 WriteX 18750 0.274 6.484 Throughput 104.568 MB/sec 100 clients 10 procs max_latency=285.639 ms List the dfuse mount point: # 'testfile' comes from ior run # 'test-dir.0-0' comes from mdtest run # 'clients' comes from dbench run ls /tmp/daos_dfuse clients test-dir.0-0 testfile","title":"Install dbench on all client nodes:"},{"location":"QSG/dbench/#clean-up","text":"Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Clean Up"},{"location":"QSG/io500/","text":"IO500 Run \u00b6 Table of Contents \u00b6 Introduction Requirements Set-Up Environment Paths Build MpiFileUtils and Dependencies Clone and Build IO-500 Run io-500 Introduction \u00b6 The I500 benchmark measures the performance of various workloads, including ior, mdtest, and find. For general information about IO500, see IO500 - About . This document provides instructions to build the components necessary to run the IO500 benchmark with DAOS. Requirements \u00b6 Set environment variables for list of servers, client and admin node. \u00b6 export ADMIN_NODE=node-1 export SERVER_NODES=node-2,node-3 export CLIENT_NODES=node-4,node-5 export ALL_NODES=\\$ADMIN_NODE,\\$SERVER_NODES,\\$CLIENT_NODES Install pdsh on admin node \u00b6 sudo yum install -y pdsh MPI \u00b6 Any implementation or version of MPI should work, as long as the same MPI is used for all instructions. For example: # load openmpi module or set it\\'s path in your environment module load mpi/openmpi3-x86_64 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH Install dependent packages on client nodes \u00b6 For Centos: sudo yum install -y cmake3 libarchive-devel patch automake autoconf gcc-c++ libuuid-devel bzip2-devel openssl-devel Lmod For openSUSE/SLES 15.2: sudo zypper install -y cmake3 libarchive-devel patch automake autoconf gcc-c++ libuuid-devel libbz2-devel libopenssl-devel lua-lmod Set-Up \u00b6 Please refer to DAOS Set-Up for initial set up which consists of rpm installation, generate and set up certificates, setting up config files, starting servers and agents. For setting up on a specific system, a Quick Start guide can be followed: Quickstart Centos 7.9 with POSIX Quick Start openSUSE/SLES 15.2 with POSIX If installing from packages/RPMS daos-devel is also required on all \\$CLIENT_NODES: sudo yum install -y daos-devel Environment Paths \u00b6 These environment variables should be set to the actual locations where you have/want each installed. After setting these variables, most of the scripts can be \\\"copy-pasted\\\". # Where DAOS is installed. Use /usr if installed from RPMs. MY_DAOS_INSTALL_PATH=/usr # Where the mpifileutils repository will be cloned to MY_MFU_SOURCE_PATH=\\${HOME}/mpifileutils # Where to build dependencies for mpifileutils MY_MFU_DEPS_PATH=\\${MY_MFU_SOURCE_PATH}/deps # Where to build mpifileutils MY_MFU_BUILD_PATH=\\${MY_MFU_SOURCE_PATH}/build # Where to install the mpifileutils dependencies, libraries, and binaries MY_MFU_INSTALL_PATH=\\${MY_MFU_SOURCE_PATH}/install # Where the io500 repository will be cloned to MY_IO500_PATH=\\${HOME}/io500 Build MpiFileUtils and Dependencies \u00b6 These dependencies should be installed on all \\$CLIENT_NODES. Clone MpiFileUtils repository \u00b6 git clone https://github.com/mchaarawi/mpifileutils -b pfind_integration \\\"\\${MY_MFU_SOURCE_PATH}\\\" Create build and install directories \u00b6 mkdir -p \\${MY_MFU_DEPS_PATH} \\${MY_MFU_BUILD_PATH} \\${MY_MFU_INSTALL_PATH} Build MpiFileUtils dependencies \u00b6 cd \\${MY_MFU_DEPS_PATH} wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz wget https://github.com/llnl/lwgrp/releases/download/v1.0.3/lwgrp-1.0.3.tar.gz wget https://github.com/llnl/dtcmp/releases/download/v1.1.1/dtcmp-1.1.1.tar.gz tar -zxf libcircle-0.3.0.tar.gz cd libcircle-0.3.0 # OPTIONAL - This optional optimization can be applied to libcircle before building. # The \\\"512\\\" can be tuned depending on how many total MPI ranks are used to run the io-500. cat \\<\\< \\'EOF\\' > libcircle_opt.patch --- a/libcircle/token.c +++ b/libcircle/token.c @@ -1307,6 +1307,12 @@ LOG(CIRCLE_LOG_DBG, \\\"Sending work request to %d...\\\", source); + /* first always ask rank 0 for work */ + int temp; + MPI_Comm_rank(comm, &temp); + if (st->local_work_requested \\< 10 && temp != 0 && temp \\< 512) + source = 0; + /* increment number of work requests for profiling */ st->local_work_requested++; EOF patch -p1 \\< libcircle_opt.patch # END optional patch ./configure --prefix=\\${MY_MFU_INSTALL_PATH} make install cd .. tar -zxf lwgrp-1.0.3.tar.gz cd lwgrp-1.0.3 ./configure --prefix=\\${MY_MFU_INSTALL_PATH} make install cd .. tar -zxf dtcmp-1.1.1.tar.gz cd dtcmp-1.1.1 ./configure --prefix=\\${MY_MFU_INSTALL_PATH} --with-lwgrp=\\${MY_MFU_INSTALL_PATH} make install cd .. cd .. Build and install MpiFileUtils \u00b6 When building mpifileutils, the CFLAGS and LDFLAGS need to be set on the same line as the cmake3 command for the variables to be propagated to the cmake environment. cd \\\"\\${MY_MFU_BUILD_PATH}\\\" && CFLAGS=\\\"-I\\${MY_DAOS_INSTALL_PATH}/include\\\" \\ LDFLAGS=\\\"-L\\${MY_DAOS_INSTALL_PATH}/lib64/ -luuid -ldaos -ldfs -ldaos_common -lgurt -lpthread\\\" \\ cmake3 \\\"\\${MY_MFU_SOURCE_PATH}\\\" \\ -DENABLE_XATTRS=OFF \\ -DWITH_DTCMP_PREFIX=\\${MY_MFU_INSTALL_PATH} \\ -DWITH_LibCircle_PREFIX=\\${MY_MFU_INSTALL_PATH} \\ -DCMAKE_INSTALL_PREFIX=\\${MY_MFU_INSTALL_PATH} && make -j8 install Add MpiFileUtils libraries and binaries to your path \u00b6 export LD_LIBRARY_PATH=\\${MY_MFU_INSTALL_PATH}/lib:\\$LD_LIBRARY_PATH export LD_LIBRARY_PATH=\\${MY_MFU_INSTALL_PATH}/lib64:\\$LD_LIBRARY_PATH export PATH=\\${MY_MFU_INSTALL_PATH}/bin:\\$PATH Clone and Build IO-500 \u00b6 Clone the IO-500 repo \u00b6 git clone https://github.com/IO500/io500.git -b io500-sc20 \\\"\\${MY_IO500_PATH}\\\" && cd \\\"\\${MY_IO500_PATH}\\\" Edit prepare.sh to: \u00b6 Point to the pfind that works with our mpifileutils Use the latest master of IOR (you should use whatever the io-500 committee has for that submission though) Build ior with DFS support Assuming MY_DAOS_INSTALL_PATH is set, you can run: cat \\<\\< EOF > io500_prepare.patch diff --git a/prepare.sh b/prepare.sh index de354ee..a2964d7 100755 --- a/prepare.sh +++ b/prepare.sh @@ -7,8 +7,8 @@ echo It will also attempt to build the benchmarks echo It will output OK at the end if builds succeed echo -IOR_HASH=bd76b45ef9db -PFIND_HASH=9d77056adce6 +IOR_HASH=a90d414a304 +PFIND_HASH=mfu_integration INSTALL_DIR=\\\\$PWD BIN=\\\\$INSTALL_DIR/bin @@ -59,14 +59,14 @@ function get_ior { function get_pfind { echo \\\"Preparing parallel find\\\" - git_co https://github.com/VI4IO/pfind.git pfind \\\\$PFIND_HASH + git_co https://github.com/mchaarawi/pfind pfind \\\\$PFIND_HASH } ###### BUILD FUNCTIONS function build_ior { pushd \\\\$BUILD/ior ./bootstrap - ./configure --prefix=\\\\$INSTALL_DIR + ./configure --prefix=\\\\$INSTALL_DIR --with-daos=\\${MY_DAOS_INSTALL_PATH} cd src \\\\$MAKE clean \\\\$MAKE install EOF git apply io500_prepare.patch Update the Makefile with correct paths \u00b6 The Makefile needs to be updated to use the actual install location of DAOS and MFU. If you set MY_DAOS_INSTALL_PATH and MY_MFU_INSTALL_PATH, you can run: cat \\<\\< EOF > io500_Makefile.patch diff --git a/Makefile b/Makefile index 2975471..5dce307 100644 --- a/Makefile +++ b/Makefile @@ -1,10 +1,13 @@ CC = mpicc CFLAGS += -std=gnu99 -Wall -Wempty-body -Werror -Wstrict-prototypes -Werror=maybe-uninitialized -Warray-bounds +CFLAGS += -I\\${MY_DAOS_INSTALL_PATH}/include -I\\${MY_MFU_INSTALL_PATH}/include IORCFLAGS = \\\\$(shell grep CFLAGS ./build/ior/Makefile | cut -d \\\"=\\\" -f 2-) CFLAGS += -g3 -lefence -I./include/ -I./src/ -I./build/pfind/src/ -I./build/ior/src/ IORLIBS = \\\\$(shell grep LIBS ./build/ior/Makefile | cut -d \\\"=\\\" -f 2-) LDFLAGS += -lm \\\\$(IORCFLAGS) \\\\$(IORLIBS) # -lgpfs # may need some additional flags as provided to IOR +LDFLAGS += -L\\${MY_DAOS_INSTALL_PATH}/lib64 -ldaos -ldaos_common -ldfs -lgurt -luuid +LDFLAGS += -L\\${MY_MFU_INSTALL_PATH}/lib64 -lmfu_dfind -lmfu VERSION_GIT=\\\\$(shell git describe --always --abbrev=12) VERSION_TREE=\\\\$(shell git diff src | wc -l | sed -e \\'s/ *//g\\' -e \\'s/\\^0//\\' | sed \\\"s/\\([0-9]\\)/-\\1/\\\") EOF git apply io500_Makefile.patch Pfind stonewalling patch \u00b6 This is not required, but our version of pfind supports stonewalling, so you can add that: cat \\<\\< \\'EOF\\' > io500_stonewall.patch diff --git a/src/phase_find.c b/src/phase_find.c index e282b25..f2bb69c 100644 --- a/src/phase_find.c +++ b/src/phase_find.c @@ -61,6 +61,7 @@ static double run(void){ int rank; MPI_Comm_rank(of.pfind_com, & rank); + of.pfind_o->stonewall = 300; // pfind supports stonewalling timer -s, but ignore for now pfind_find_results_t * res = pfind_find(of.pfind_o); if(! res){ EOF git apply io500_stonewall.patch Run the prepare.sh script \u00b6 \\${MY_IO500_PATH}/prepare.sh Edit compile.sh \u00b6 It is expected here that IOR builds with the DFS driver, but pfind build will fail. build/pfind/compile.sh needs to be updated to point to the correct DAOS and MFU paths. DAOS=\\${MY_DAOS_INSTALL_PATH} MFU=\\${MY_MFU_INSTALL_PATH} To update, you can run: sed -i \\\"/\\^DAOS=/c\\DAOS=\\${MY_DAOS_INSTALL_PATH}\\\" \\${MY_IO500_PATH}/build/pfind/compile.sh sed -i \\\"/\\^MFU=/c\\MFU=\\${MY_MFU_INSTALL_PATH}\\\" \\${MY_IO500_PATH}/build/pfind/compile.sh Then run prepare.sh again: \\${MY_IO500_PATH}/prepare.sh Run io-500 \u00b6 Setup the config file \u00b6 A sample config-full.ini file for reference: https://github.com/mchaarawi/io500/blob/main/config-full.ini If you want to download this: wget https://raw.githubusercontent.com/mchaarawi/io500/main/config-full.ini In DAOS 1.1.3 and above, svcl is not required anymore and should be remove from the config. sed -i \\'s/ --dfs.svcl=\\$DAOS_SVCL//g\\' config-full.ini You should set a low stonewall at first to verify. For [find] the nprocs setting under that should be the same as the number of processes you want to run with the entire workflow. Create DAOS pool, container with type POSIX \u00b6 The pool size depends on the settings you use in the config file - larger datasets require more pool space. For documentation on creating pools, see https://daos-stack.github.io/admin/pool_operations/ . For documentation on creating containers, see https://daos-stack.github.io/user/container/ . For example: dmg pool create -z 100G daos container create --type POSIX --pool \\$DAOS_POOL Set the pool, cont, fuse environment variables \u00b6 This should be the pool, container, and dfuse path to use for the IO500 run. export DAOS_POOL=\\<uuid> export DAOS_CONT=\\<uuid> export DAOS_FUSE=\\<path> # Only for DAOS version \\< 1.1.3 DAOS_SVCL=\\<svcl> Create a dfuse mount on all client nodes \u00b6 pdsh -w \\$CLIENT_NODES -f 4 \\\"dfuse --pool=\\$DAOS_POOL --container=\\$DAOS_CONT -m \\$DAOS_FUSE\\\" Substitute variables in the config file \u00b6 This assumes you are in the directory \\${MY_IO500_PATH}. This will replace \\$DAOS_POOL, \\$DAOS_CONT, and \\$DAOS_FUSE with their actual values from the environment. envsubst \\< config-full.ini > temp.ini Either run the app manually: \u00b6 mpirun -hosts \\$CLIENT_NODES -np 2 ./io500 temp.ini OR use their provided io500.sh script (which generates the tarball): \u00b6 (note that here you must change the mpi procs / command to whatever you are using) ./io500.sh temp.ini Results \u00b6 After running, the results will be in the \\$DAOS_FUSE/results directory. result_summary.txt - Summary of IO rates and times for each test. result.txt - Each test command that was ran, along with rates and times. To view comparable results, you can download any of the official DAOS submissions here: https://io500.org/ .","title":"Io500"},{"location":"QSG/io500/#io500-run","text":"","title":"IO500 Run"},{"location":"QSG/io500/#table-of-contents","text":"Introduction Requirements Set-Up Environment Paths Build MpiFileUtils and Dependencies Clone and Build IO-500 Run io-500","title":"Table of Contents"},{"location":"QSG/io500/#introduction","text":"The I500 benchmark measures the performance of various workloads, including ior, mdtest, and find. For general information about IO500, see IO500 - About . This document provides instructions to build the components necessary to run the IO500 benchmark with DAOS.","title":"Introduction"},{"location":"QSG/io500/#requirements","text":"","title":"Requirements"},{"location":"QSG/io500/#set-environment-variables-for-list-of-servers-client-and-admin-node","text":"export ADMIN_NODE=node-1 export SERVER_NODES=node-2,node-3 export CLIENT_NODES=node-4,node-5 export ALL_NODES=\\$ADMIN_NODE,\\$SERVER_NODES,\\$CLIENT_NODES","title":"Set environment variables for list of servers, client and admin node."},{"location":"QSG/io500/#install-pdsh-on-admin-node","text":"sudo yum install -y pdsh","title":"Install pdsh on admin node"},{"location":"QSG/io500/#mpi","text":"Any implementation or version of MPI should work, as long as the same MPI is used for all instructions. For example: # load openmpi module or set it\\'s path in your environment module load mpi/openmpi3-x86_64 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH","title":"MPI"},{"location":"QSG/io500/#install-dependent-packages-on-client-nodes","text":"For Centos: sudo yum install -y cmake3 libarchive-devel patch automake autoconf gcc-c++ libuuid-devel bzip2-devel openssl-devel Lmod For openSUSE/SLES 15.2: sudo zypper install -y cmake3 libarchive-devel patch automake autoconf gcc-c++ libuuid-devel libbz2-devel libopenssl-devel lua-lmod","title":"Install dependent packages on client nodes"},{"location":"QSG/io500/#set-up","text":"Please refer to DAOS Set-Up for initial set up which consists of rpm installation, generate and set up certificates, setting up config files, starting servers and agents. For setting up on a specific system, a Quick Start guide can be followed: Quickstart Centos 7.9 with POSIX Quick Start openSUSE/SLES 15.2 with POSIX If installing from packages/RPMS daos-devel is also required on all \\$CLIENT_NODES: sudo yum install -y daos-devel","title":"Set-Up"},{"location":"QSG/io500/#environment-paths","text":"These environment variables should be set to the actual locations where you have/want each installed. After setting these variables, most of the scripts can be \\\"copy-pasted\\\". # Where DAOS is installed. Use /usr if installed from RPMs. MY_DAOS_INSTALL_PATH=/usr # Where the mpifileutils repository will be cloned to MY_MFU_SOURCE_PATH=\\${HOME}/mpifileutils # Where to build dependencies for mpifileutils MY_MFU_DEPS_PATH=\\${MY_MFU_SOURCE_PATH}/deps # Where to build mpifileutils MY_MFU_BUILD_PATH=\\${MY_MFU_SOURCE_PATH}/build # Where to install the mpifileutils dependencies, libraries, and binaries MY_MFU_INSTALL_PATH=\\${MY_MFU_SOURCE_PATH}/install # Where the io500 repository will be cloned to MY_IO500_PATH=\\${HOME}/io500","title":"Environment Paths"},{"location":"QSG/io500/#build-mpifileutils-and-dependencies","text":"These dependencies should be installed on all \\$CLIENT_NODES.","title":"Build MpiFileUtils and Dependencies"},{"location":"QSG/io500/#clone-mpifileutils-repository","text":"git clone https://github.com/mchaarawi/mpifileutils -b pfind_integration \\\"\\${MY_MFU_SOURCE_PATH}\\\"","title":"Clone MpiFileUtils repository"},{"location":"QSG/io500/#create-build-and-install-directories","text":"mkdir -p \\${MY_MFU_DEPS_PATH} \\${MY_MFU_BUILD_PATH} \\${MY_MFU_INSTALL_PATH}","title":"Create build and install directories"},{"location":"QSG/io500/#build-mpifileutils-dependencies","text":"cd \\${MY_MFU_DEPS_PATH} wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz wget https://github.com/llnl/lwgrp/releases/download/v1.0.3/lwgrp-1.0.3.tar.gz wget https://github.com/llnl/dtcmp/releases/download/v1.1.1/dtcmp-1.1.1.tar.gz tar -zxf libcircle-0.3.0.tar.gz cd libcircle-0.3.0 # OPTIONAL - This optional optimization can be applied to libcircle before building. # The \\\"512\\\" can be tuned depending on how many total MPI ranks are used to run the io-500. cat \\<\\< \\'EOF\\' > libcircle_opt.patch --- a/libcircle/token.c +++ b/libcircle/token.c @@ -1307,6 +1307,12 @@ LOG(CIRCLE_LOG_DBG, \\\"Sending work request to %d...\\\", source); + /* first always ask rank 0 for work */ + int temp; + MPI_Comm_rank(comm, &temp); + if (st->local_work_requested \\< 10 && temp != 0 && temp \\< 512) + source = 0; + /* increment number of work requests for profiling */ st->local_work_requested++; EOF patch -p1 \\< libcircle_opt.patch # END optional patch ./configure --prefix=\\${MY_MFU_INSTALL_PATH} make install cd .. tar -zxf lwgrp-1.0.3.tar.gz cd lwgrp-1.0.3 ./configure --prefix=\\${MY_MFU_INSTALL_PATH} make install cd .. tar -zxf dtcmp-1.1.1.tar.gz cd dtcmp-1.1.1 ./configure --prefix=\\${MY_MFU_INSTALL_PATH} --with-lwgrp=\\${MY_MFU_INSTALL_PATH} make install cd .. cd ..","title":"Build MpiFileUtils dependencies"},{"location":"QSG/io500/#build-and-install-mpifileutils","text":"When building mpifileutils, the CFLAGS and LDFLAGS need to be set on the same line as the cmake3 command for the variables to be propagated to the cmake environment. cd \\\"\\${MY_MFU_BUILD_PATH}\\\" && CFLAGS=\\\"-I\\${MY_DAOS_INSTALL_PATH}/include\\\" \\ LDFLAGS=\\\"-L\\${MY_DAOS_INSTALL_PATH}/lib64/ -luuid -ldaos -ldfs -ldaos_common -lgurt -lpthread\\\" \\ cmake3 \\\"\\${MY_MFU_SOURCE_PATH}\\\" \\ -DENABLE_XATTRS=OFF \\ -DWITH_DTCMP_PREFIX=\\${MY_MFU_INSTALL_PATH} \\ -DWITH_LibCircle_PREFIX=\\${MY_MFU_INSTALL_PATH} \\ -DCMAKE_INSTALL_PREFIX=\\${MY_MFU_INSTALL_PATH} && make -j8 install","title":"Build and install MpiFileUtils"},{"location":"QSG/io500/#add-mpifileutils-libraries-and-binaries-to-your-path","text":"export LD_LIBRARY_PATH=\\${MY_MFU_INSTALL_PATH}/lib:\\$LD_LIBRARY_PATH export LD_LIBRARY_PATH=\\${MY_MFU_INSTALL_PATH}/lib64:\\$LD_LIBRARY_PATH export PATH=\\${MY_MFU_INSTALL_PATH}/bin:\\$PATH","title":"Add MpiFileUtils libraries and binaries to your path"},{"location":"QSG/io500/#clone-and-build-io-500","text":"","title":"Clone and Build IO-500"},{"location":"QSG/io500/#clone-the-io-500-repo","text":"git clone https://github.com/IO500/io500.git -b io500-sc20 \\\"\\${MY_IO500_PATH}\\\" && cd \\\"\\${MY_IO500_PATH}\\\"","title":"Clone the IO-500 repo"},{"location":"QSG/io500/#edit-preparesh-to","text":"Point to the pfind that works with our mpifileutils Use the latest master of IOR (you should use whatever the io-500 committee has for that submission though) Build ior with DFS support Assuming MY_DAOS_INSTALL_PATH is set, you can run: cat \\<\\< EOF > io500_prepare.patch diff --git a/prepare.sh b/prepare.sh index de354ee..a2964d7 100755 --- a/prepare.sh +++ b/prepare.sh @@ -7,8 +7,8 @@ echo It will also attempt to build the benchmarks echo It will output OK at the end if builds succeed echo -IOR_HASH=bd76b45ef9db -PFIND_HASH=9d77056adce6 +IOR_HASH=a90d414a304 +PFIND_HASH=mfu_integration INSTALL_DIR=\\\\$PWD BIN=\\\\$INSTALL_DIR/bin @@ -59,14 +59,14 @@ function get_ior { function get_pfind { echo \\\"Preparing parallel find\\\" - git_co https://github.com/VI4IO/pfind.git pfind \\\\$PFIND_HASH + git_co https://github.com/mchaarawi/pfind pfind \\\\$PFIND_HASH } ###### BUILD FUNCTIONS function build_ior { pushd \\\\$BUILD/ior ./bootstrap - ./configure --prefix=\\\\$INSTALL_DIR + ./configure --prefix=\\\\$INSTALL_DIR --with-daos=\\${MY_DAOS_INSTALL_PATH} cd src \\\\$MAKE clean \\\\$MAKE install EOF git apply io500_prepare.patch","title":"Edit prepare.sh to:"},{"location":"QSG/io500/#update-the-makefile-with-correct-paths","text":"The Makefile needs to be updated to use the actual install location of DAOS and MFU. If you set MY_DAOS_INSTALL_PATH and MY_MFU_INSTALL_PATH, you can run: cat \\<\\< EOF > io500_Makefile.patch diff --git a/Makefile b/Makefile index 2975471..5dce307 100644 --- a/Makefile +++ b/Makefile @@ -1,10 +1,13 @@ CC = mpicc CFLAGS += -std=gnu99 -Wall -Wempty-body -Werror -Wstrict-prototypes -Werror=maybe-uninitialized -Warray-bounds +CFLAGS += -I\\${MY_DAOS_INSTALL_PATH}/include -I\\${MY_MFU_INSTALL_PATH}/include IORCFLAGS = \\\\$(shell grep CFLAGS ./build/ior/Makefile | cut -d \\\"=\\\" -f 2-) CFLAGS += -g3 -lefence -I./include/ -I./src/ -I./build/pfind/src/ -I./build/ior/src/ IORLIBS = \\\\$(shell grep LIBS ./build/ior/Makefile | cut -d \\\"=\\\" -f 2-) LDFLAGS += -lm \\\\$(IORCFLAGS) \\\\$(IORLIBS) # -lgpfs # may need some additional flags as provided to IOR +LDFLAGS += -L\\${MY_DAOS_INSTALL_PATH}/lib64 -ldaos -ldaos_common -ldfs -lgurt -luuid +LDFLAGS += -L\\${MY_MFU_INSTALL_PATH}/lib64 -lmfu_dfind -lmfu VERSION_GIT=\\\\$(shell git describe --always --abbrev=12) VERSION_TREE=\\\\$(shell git diff src | wc -l | sed -e \\'s/ *//g\\' -e \\'s/\\^0//\\' | sed \\\"s/\\([0-9]\\)/-\\1/\\\") EOF git apply io500_Makefile.patch","title":"Update the Makefile with correct paths"},{"location":"QSG/io500/#pfind-stonewalling-patch","text":"This is not required, but our version of pfind supports stonewalling, so you can add that: cat \\<\\< \\'EOF\\' > io500_stonewall.patch diff --git a/src/phase_find.c b/src/phase_find.c index e282b25..f2bb69c 100644 --- a/src/phase_find.c +++ b/src/phase_find.c @@ -61,6 +61,7 @@ static double run(void){ int rank; MPI_Comm_rank(of.pfind_com, & rank); + of.pfind_o->stonewall = 300; // pfind supports stonewalling timer -s, but ignore for now pfind_find_results_t * res = pfind_find(of.pfind_o); if(! res){ EOF git apply io500_stonewall.patch","title":"Pfind stonewalling patch"},{"location":"QSG/io500/#run-the-preparesh-script","text":"\\${MY_IO500_PATH}/prepare.sh","title":"Run the prepare.sh script"},{"location":"QSG/io500/#edit-compilesh","text":"It is expected here that IOR builds with the DFS driver, but pfind build will fail. build/pfind/compile.sh needs to be updated to point to the correct DAOS and MFU paths. DAOS=\\${MY_DAOS_INSTALL_PATH} MFU=\\${MY_MFU_INSTALL_PATH} To update, you can run: sed -i \\\"/\\^DAOS=/c\\DAOS=\\${MY_DAOS_INSTALL_PATH}\\\" \\${MY_IO500_PATH}/build/pfind/compile.sh sed -i \\\"/\\^MFU=/c\\MFU=\\${MY_MFU_INSTALL_PATH}\\\" \\${MY_IO500_PATH}/build/pfind/compile.sh Then run prepare.sh again: \\${MY_IO500_PATH}/prepare.sh","title":"Edit compile.sh"},{"location":"QSG/io500/#run-io-500","text":"","title":"Run io-500"},{"location":"QSG/io500/#setup-the-config-file","text":"A sample config-full.ini file for reference: https://github.com/mchaarawi/io500/blob/main/config-full.ini If you want to download this: wget https://raw.githubusercontent.com/mchaarawi/io500/main/config-full.ini In DAOS 1.1.3 and above, svcl is not required anymore and should be remove from the config. sed -i \\'s/ --dfs.svcl=\\$DAOS_SVCL//g\\' config-full.ini You should set a low stonewall at first to verify. For [find] the nprocs setting under that should be the same as the number of processes you want to run with the entire workflow.","title":"Setup the config file"},{"location":"QSG/io500/#create-daos-pool-container-with-type-posix","text":"The pool size depends on the settings you use in the config file - larger datasets require more pool space. For documentation on creating pools, see https://daos-stack.github.io/admin/pool_operations/ . For documentation on creating containers, see https://daos-stack.github.io/user/container/ . For example: dmg pool create -z 100G daos container create --type POSIX --pool \\$DAOS_POOL","title":"Create DAOS pool, container with type POSIX"},{"location":"QSG/io500/#set-the-pool-cont-fuse-environment-variables","text":"This should be the pool, container, and dfuse path to use for the IO500 run. export DAOS_POOL=\\<uuid> export DAOS_CONT=\\<uuid> export DAOS_FUSE=\\<path> # Only for DAOS version \\< 1.1.3 DAOS_SVCL=\\<svcl>","title":"Set the pool, cont, fuse environment variables"},{"location":"QSG/io500/#create-a-dfuse-mount-on-all-client-nodes","text":"pdsh -w \\$CLIENT_NODES -f 4 \\\"dfuse --pool=\\$DAOS_POOL --container=\\$DAOS_CONT -m \\$DAOS_FUSE\\\"","title":"Create a dfuse mount on all client nodes"},{"location":"QSG/io500/#substitute-variables-in-the-config-file","text":"This assumes you are in the directory \\${MY_IO500_PATH}. This will replace \\$DAOS_POOL, \\$DAOS_CONT, and \\$DAOS_FUSE with their actual values from the environment. envsubst \\< config-full.ini > temp.ini","title":"Substitute variables in the config file"},{"location":"QSG/io500/#either-run-the-app-manually","text":"mpirun -hosts \\$CLIENT_NODES -np 2 ./io500 temp.ini","title":"Either run the app manually:"},{"location":"QSG/io500/#or-use-their-provided-io500sh-script-which-generates-the-tarball","text":"(note that here you must change the mpi procs / command to whatever you are using) ./io500.sh temp.ini","title":"OR use their provided io500.sh script (which generates the tarball):"},{"location":"QSG/io500/#results","text":"After running, the results will be in the \\$DAOS_FUSE/results directory. result_summary.txt - Summary of IO rates and times for each test. result.txt - Each test command that was ran, along with rates and times. To view comparable results, you can download any of the official DAOS submissions here: https://io500.org/ .","title":"Results"},{"location":"QSG/mdtest/","text":"Run mdtest \u00b6 Create a substantial directory structure. Use mdtest to create 30K files: mpirun -hostfile /path/to/hostfile_clients -np 10 mdtest -a POSIX -z 0 -F -C -i 1 -n 3334 -e 4096 -d /tmp/daos_dfuse/ -w 4096 -- started at 04/29/2021 23:28:11 -- mdtest-3.4.0+dev was launched with 10 total task(s) on 3 node(s) Command line used: mdtest '-a' 'POSIX' '-z' '0' '-F' '-C' '-i' '1' '-n' '3334' '-e' '4096' '-d' '/tmp/daos_dfuse/' '-w' '4096' Path: /tmp/daos_dfuse FS: 36.5 GiB Used FS: 18.8% Inodes: 2.3 Mi Used Inodes: 5.9% Nodemap: 1001001001 10 tasks, 33340 files SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- --- --- ---- ------- File creation : 2943.697 2943.674 2943.686 0.006 File stat : 0.000 0.000 0.000 0.000 File read : 0.000 0.000 0.000 0.000 File removal : 0.000 0.000 0.000 0.000 Tree creation : 1079.858 1079.858 1079.858 0.000 Tree removal : 0.000 0.000 0.000 0.000 -- finished at 04/29/2021 23:28:22 -- Clean Up \u00b6 Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w $SERVER_NODES \"sudo systemctl stop daos_server\"","title":"Run mdtest"},{"location":"QSG/mdtest/#run-mdtest","text":"Create a substantial directory structure. Use mdtest to create 30K files: mpirun -hostfile /path/to/hostfile_clients -np 10 mdtest -a POSIX -z 0 -F -C -i 1 -n 3334 -e 4096 -d /tmp/daos_dfuse/ -w 4096 -- started at 04/29/2021 23:28:11 -- mdtest-3.4.0+dev was launched with 10 total task(s) on 3 node(s) Command line used: mdtest '-a' 'POSIX' '-z' '0' '-F' '-C' '-i' '1' '-n' '3334' '-e' '4096' '-d' '/tmp/daos_dfuse/' '-w' '4096' Path: /tmp/daos_dfuse FS: 36.5 GiB Used FS: 18.8% Inodes: 2.3 Mi Used Inodes: 5.9% Nodemap: 1001001001 10 tasks, 33340 files SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- --- --- ---- ------- File creation : 2943.697 2943.674 2943.686 0.006 File stat : 0.000 0.000 0.000 0.000 File read : 0.000 0.000 0.000 0.000 File removal : 0.000 0.000 0.000 0.000 Tree creation : 1079.858 1079.858 1079.858 0.000 Tree removal : 0.000 0.000 0.000 0.000 -- finished at 04/29/2021 23:28:22 --","title":"Run mdtest"},{"location":"QSG/mdtest/#clean-up","text":"Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w $SERVER_NODES \"sudo systemctl stop daos_server\"","title":"Clean Up"},{"location":"QSG/runior/","text":"Run IOR \u00b6 Use IOR to generate an HPC type POSIX I/O load to the POSIX container. Build ior \u00b6 git clone https://github.com/hpc/ior.git cd ior ./bootstrap mkdir build;cd build ../configure --with-daos=/usr --prefix=<your dir> make make install Add <your dir\\>/lib to LD_LIBRARY_PATh and <your dir\\>/bin to PATH Or On all client nodes: *sudo yum install ior* Use ior to write and read around 150G of data # load mpich module or set it's path in your environment module load mpi/mpich-x86_64 or export LD_LIBRARY_PATH=<mpich lib path>:$LD_LIBRARY_PATH export PATH=<mpich bin path>:$PATH mpirun -hostfile /path/to/hostfile_clients -np 30 ior -a POSIX -b 5G -t 1M -v -W -w -r -R -i 1 -k -o /tmp/daos_dfuse/testfile IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O Began : Thu Apr 29 23:23:09 2021 Command line : ior -a POSIX -b 5G -t 1M -v -W -w -r -R -i 1 -k -o /tmp/daos_dfuse/testfile Machine : Linux wolf-86.wolf.hpdd.intel.com Start time skew across all tasks: 0.00 sec TestID : 0 StartTime : Thu Apr 29 23:23:09 2021 Path : /tmp/daos_dfuse/testfile FS : 789.8 GiB Used FS: 16.5% Inodes: -0.0 Mi Used Inodes: 0.0% Participating tasks : 30 Options: api : POSIX apiVersion : test filename : /tmp/daos_dfuse/testfile access : single-shared-file type : independent segments : 1 ordering in a file : sequential ordering inter file : no tasks offsets nodes : 3 tasks : 30 clients per node : 10 repetitions : 1 xfersize : 1 MiB blocksize : 5 GiB aggregate filesize : 150 GiB verbose : 1 Results: access bw(MiB/s) IOPS Latency(s) block(KiB) xfer(KiB) open(s) wr/rd(s) close(s) total(s) iter ------ --------- ---- ---------- ---------- --------- -------- -------- -------- -------- ---- Commencing write performance test: Thu Apr 29 23:23:09 2021 write 1299.23 1299.84 0.022917 5242880 1024.00 10.79 118.17 0.000377 118.22 0 Verifying contents of the file(s) just written. Thu Apr 29 23:25:07 2021 Commencing read performance test: Thu Apr 29 23:25:35 2021 read 5429 5431 0.005523 5242880 1024.00 0.012188 28.28 0.000251 28.29 0 Max Write: 1299.23 MiB/sec (1362.35 MB/sec) Max Read: 5429.38 MiB/sec (5693.11 MB/sec) Summary of all tests: Operation Max(MiB) Min(MiB) Mean(MiB) StdDev Max(OPs) Min(OPs) Mean(OPs) StdDev Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggs(MiB) API RefNum write 1299.23 1299.23 1299.23 0.00 1299.23 1299.23 1299.23 0.00 118.22343 NA NA 0 30 10 1 0 0 1 0 0 1 5368709120 1048576 153600.0 POSIX 0 read 5429.38 5429.38 5429.38 0.00 5429.38 5429.38 5429.38 0.00 28.29054 NA NA 0 30 10 1 0 0 1 0 0 1 5368709120 1048576 153600.0 POSIX 0 Finished : Thu Apr 29 23:26:03 2021 Clean Up \u00b6 Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w $SERVER_NODES \"sudo systemctl stop daos_server\"","title":"Run IOR"},{"location":"QSG/runior/#run-ior","text":"Use IOR to generate an HPC type POSIX I/O load to the POSIX container.","title":"Run IOR"},{"location":"QSG/runior/#build-ior","text":"git clone https://github.com/hpc/ior.git cd ior ./bootstrap mkdir build;cd build ../configure --with-daos=/usr --prefix=<your dir> make make install Add <your dir\\>/lib to LD_LIBRARY_PATh and <your dir\\>/bin to PATH Or On all client nodes: *sudo yum install ior* Use ior to write and read around 150G of data # load mpich module or set it's path in your environment module load mpi/mpich-x86_64 or export LD_LIBRARY_PATH=<mpich lib path>:$LD_LIBRARY_PATH export PATH=<mpich bin path>:$PATH mpirun -hostfile /path/to/hostfile_clients -np 30 ior -a POSIX -b 5G -t 1M -v -W -w -r -R -i 1 -k -o /tmp/daos_dfuse/testfile IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O Began : Thu Apr 29 23:23:09 2021 Command line : ior -a POSIX -b 5G -t 1M -v -W -w -r -R -i 1 -k -o /tmp/daos_dfuse/testfile Machine : Linux wolf-86.wolf.hpdd.intel.com Start time skew across all tasks: 0.00 sec TestID : 0 StartTime : Thu Apr 29 23:23:09 2021 Path : /tmp/daos_dfuse/testfile FS : 789.8 GiB Used FS: 16.5% Inodes: -0.0 Mi Used Inodes: 0.0% Participating tasks : 30 Options: api : POSIX apiVersion : test filename : /tmp/daos_dfuse/testfile access : single-shared-file type : independent segments : 1 ordering in a file : sequential ordering inter file : no tasks offsets nodes : 3 tasks : 30 clients per node : 10 repetitions : 1 xfersize : 1 MiB blocksize : 5 GiB aggregate filesize : 150 GiB verbose : 1 Results: access bw(MiB/s) IOPS Latency(s) block(KiB) xfer(KiB) open(s) wr/rd(s) close(s) total(s) iter ------ --------- ---- ---------- ---------- --------- -------- -------- -------- -------- ---- Commencing write performance test: Thu Apr 29 23:23:09 2021 write 1299.23 1299.84 0.022917 5242880 1024.00 10.79 118.17 0.000377 118.22 0 Verifying contents of the file(s) just written. Thu Apr 29 23:25:07 2021 Commencing read performance test: Thu Apr 29 23:25:35 2021 read 5429 5431 0.005523 5242880 1024.00 0.012188 28.28 0.000251 28.29 0 Max Write: 1299.23 MiB/sec (1362.35 MB/sec) Max Read: 5429.38 MiB/sec (5693.11 MB/sec) Summary of all tests: Operation Max(MiB) Min(MiB) Mean(MiB) StdDev Max(OPs) Min(OPs) Mean(OPs) StdDev Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggs(MiB) API RefNum write 1299.23 1299.23 1299.23 0.00 1299.23 1299.23 1299.23 0.00 118.22343 NA NA 0 30 10 1 0 0 1 0 0 1 5368709120 1048576 153600.0 POSIX 0 read 5429.38 5429.38 5429.38 0.00 5429.38 5429.38 5429.38 0.00 28.29054 NA NA 0 30 10 1 0 0 1 0 0 1 5368709120 1048576 153600.0 POSIX 0 Finished : Thu Apr 29 23:26:03 2021","title":"Build ior"},{"location":"QSG/runior/#clean-up","text":"Remove one of the copy created using datamover rm -rf /tmp/daos_dfuse/daos_container_copy Remove dfuse mountpoint: # unmount dfuse pdsh -w $CLIENT_NODES 'fusermount3 -uz /tmp/daos_dfuse' # remove mount dir pdsh -w $CLIENT_NODES rm -rf /tmp/daos_dfuse List containers to be destroyed: # list containers daos pool list-containers --pool $DAOS_POOL # sample output # sample output cd46cf6e-f886-4682-8077-e3cbcd09b43a caf0135c-def8-45a5-bac3-d0b969e67c8b Destroy Containers: # destroy container1 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT # destroy container2 daos container destroy --pool $DAOS_POOL --cont $DAOS_CONT2 List Pools to be destroyed: # list pool dmg pool list # sample output Pool UUID Svc Replicas --------- ------------ b22220ea-740d-46bc-84ad-35ed3a28aa31 [1-3] Destroy Pool: # destroy pool dmg pool destroy --pool $DAOS_POOL Stop Agents: # stop agents pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent\" Stop Servers: # stop servers pdsh -S -w $SERVER_NODES \"sudo systemctl stop daos_server\"","title":"Clean Up"},{"location":"QSG/scratch/","text":"Build from scratch \u00b6 Install pre-requisites \u00b6 CentOS git clone # Run as root yum clean all && \\ yum -y install epel-release && \\ yum -y upgrade && \\ yum -y install \\ boost-python36-devel \\ clang-analyzer \\ cmake \\ CUnit-devel \\ doxygen \\ e2fsprogs \\ file \\ flex \\ fuse3-devel \\ gcc \\ gcc-c++ \\ git \\ golang \\ graphviz \\ hwloc-devel \\ ipmctl \\ java-1.8.0-openjdk \\ json-c-devel \\ lcov \\ libaio-devel \\ libcmocka-devel \\ libevent-devel \\ libipmctl-devel \\ libiscsi-devel \\ libtool \\ libtool-ltdl-devel \\ libunwind-devel \\ libuuid-devel \\ libyaml-devel \\ Lmod \\ lz4-devel \\ make \\ man \\ maven \\ nasm \\ ndctl \\ numactl-devel \\ openssl-devel \\ pandoc \\ patch \\ patchelf \\ pciutils \\ python3-pip \\ python36-Cython \\ python36-devel \\ python36-distro \\ python36-jira \\ python36-numpy \\ python36-paramiko \\ python36-pylint \\ python36-requests \\ python36-requests \\ python36-tabulate \\ python36-pyxattr \\ python36-scons \\ sg3_utils \\ valgrind-devel \\ yasm && \\ yum clean all yum -y install openmpi3-devel && yum clean all SuSE git clone # Run as root zypper --non-interactive update && \\ zypper --non-interactive install \\ boost-devel \\ clang \\ cmake \\ cunit-devel \\ curl \\ doxygen \\ flex \\ fuse3-devel \\ gcc \\ gcc-c++ \\ git \\ graphviz \\ gzip \\ hwloc-devel \\ java-1_8_0-openjdk-devel \\ libaio-devel \\ libcmocka-devel \\ libevent-devel \\ libiscsi-devel \\ libjson-c-devel \\ libltdl7 \\ liblz4-devel \\ libnuma-devel \\ libopenssl-devel \\ libtool \\ libunwind-devel \\ libuuid-devel \\ libyaml-devel \\ lua-lmod \\ make \\ man \\ maven \\ nasm \\ openmpi3-devel \\ pandoc \\ patch \\ patchelf \\ python3-devel \\ python3-distro \\ python3-pip \\ python3-pyxattr \\ python3-PyYAML \\ python3-tabulate \\ scons \\ sg3_utils \\ valgrind-devel \\ which \\ yasm && \\ zypper clean --all update-ca-certificates zypper --non-interactive --no-gpg-checks install --allow-unsigned-rpm lua-lmod zypper --non-interactive install -y ipmctl-devel go1.14 go1.14-race Daos source code \u00b6 Download daos source code using the following command: git clone git clone --recurse-submodules -b v1.2-rc1 https://github.com/daos-stack/daos.git Building DAOS & Dependencies \u00b6 Once the sources are downloaded, the pre-requisites to build the source code can be installed by: Note: CentOS git clone cd daos scons-3 --config=force --build-deps=yes install SuSE git clone cd daos scons --config=force --build-deps=yes install","title":"Scratch"},{"location":"QSG/scratch/#build-from-scratch","text":"","title":"Build from scratch"},{"location":"QSG/scratch/#install-pre-requisites","text":"CentOS git clone # Run as root yum clean all && \\ yum -y install epel-release && \\ yum -y upgrade && \\ yum -y install \\ boost-python36-devel \\ clang-analyzer \\ cmake \\ CUnit-devel \\ doxygen \\ e2fsprogs \\ file \\ flex \\ fuse3-devel \\ gcc \\ gcc-c++ \\ git \\ golang \\ graphviz \\ hwloc-devel \\ ipmctl \\ java-1.8.0-openjdk \\ json-c-devel \\ lcov \\ libaio-devel \\ libcmocka-devel \\ libevent-devel \\ libipmctl-devel \\ libiscsi-devel \\ libtool \\ libtool-ltdl-devel \\ libunwind-devel \\ libuuid-devel \\ libyaml-devel \\ Lmod \\ lz4-devel \\ make \\ man \\ maven \\ nasm \\ ndctl \\ numactl-devel \\ openssl-devel \\ pandoc \\ patch \\ patchelf \\ pciutils \\ python3-pip \\ python36-Cython \\ python36-devel \\ python36-distro \\ python36-jira \\ python36-numpy \\ python36-paramiko \\ python36-pylint \\ python36-requests \\ python36-requests \\ python36-tabulate \\ python36-pyxattr \\ python36-scons \\ sg3_utils \\ valgrind-devel \\ yasm && \\ yum clean all yum -y install openmpi3-devel && yum clean all SuSE git clone # Run as root zypper --non-interactive update && \\ zypper --non-interactive install \\ boost-devel \\ clang \\ cmake \\ cunit-devel \\ curl \\ doxygen \\ flex \\ fuse3-devel \\ gcc \\ gcc-c++ \\ git \\ graphviz \\ gzip \\ hwloc-devel \\ java-1_8_0-openjdk-devel \\ libaio-devel \\ libcmocka-devel \\ libevent-devel \\ libiscsi-devel \\ libjson-c-devel \\ libltdl7 \\ liblz4-devel \\ libnuma-devel \\ libopenssl-devel \\ libtool \\ libunwind-devel \\ libuuid-devel \\ libyaml-devel \\ lua-lmod \\ make \\ man \\ maven \\ nasm \\ openmpi3-devel \\ pandoc \\ patch \\ patchelf \\ python3-devel \\ python3-distro \\ python3-pip \\ python3-pyxattr \\ python3-PyYAML \\ python3-tabulate \\ scons \\ sg3_utils \\ valgrind-devel \\ which \\ yasm && \\ zypper clean --all update-ca-certificates zypper --non-interactive --no-gpg-checks install --allow-unsigned-rpm lua-lmod zypper --non-interactive install -y ipmctl-devel go1.14 go1.14-race","title":"Install pre-requisites"},{"location":"QSG/scratch/#daos-source-code","text":"Download daos source code using the following command: git clone git clone --recurse-submodules -b v1.2-rc1 https://github.com/daos-stack/daos.git","title":"Daos source code"},{"location":"QSG/scratch/#building-daos-dependencies","text":"Once the sources are downloaded, the pre-requisites to build the source code can be installed by: Note: CentOS git clone cd daos scons-3 --config=force --build-deps=yes install SuSE git clone cd daos scons --config=force --build-deps=yes install","title":"Building DAOS &amp; Dependencies"},{"location":"QSG/setup/","text":"DAOS Set-Up on CentOS \u00b6 The following instructions detail how to install, set up and start DAOS servers and clients on two or more nodes. This document includes instructions for CentOS. For setup instructions on OpenSuse, refer to the OpenSuse setup . For more details reference the DAOS administration guide: https://daos-stack.github.io/admin/hardware/ Requirements \u00b6 The following steps require two or more hosts which will be divided up into admin, client, and server roles. One node can be used for both the admin and client node. All nodes must have: sudo access configured password-less ssh configured pdsh installed (or some other means of running multiple remote commands in parallel) In addition the server nodes should also have: an InfiniBand network adapter configured one or more NVMe devices IOMMU is enabled: https://daos-stack.github.io/admin/predeployment_check/#enable-iommu-optional For the use of the commands outlined on this page the following shell variables will need to be defined: ADMIN_NODE CLIENT_NODES SERVER_NODES ALL_NODES For example, if you want to use node-1 as the admin node, node-2 and node-3 as client nodes, and node-[4-6] as server nodes, these variables would be defined as: ADMIN\\_NODE=node-1 CLIENT\\_NODES=node-2,node-3 SERVER\\_NODES=node-4,node-5,node-6 ALL\\_NODES=\\$ADMIN\\_NODE,\\$CLIENT\\_NODES,\\$SERVER\\_NODES Note If a client node is also serving as an admin node, exclude \\$ADMIN\\_NODE from the ALL\\_NODES assignment to prevent duplication. For example: ALL\\_NODES=\\$CLIENT\\_NODES,\\$SERVER\\_NODES RPM Installation \u00b6 In this section the required RPMs will be installed on each of the nodes based upon their role. Admin and client nodes require the installation of the daos-client RPM and the server nodes require the installation of the daos-server RPM. Configure access to the DAOS package repository at https://packages.daos.io/v1.2 . pdsh -w \\$ALL\\_NODES \\'sudo wget -O /etc/yum.repos.d/daos-packages.repo https://packages.daos.io/v1.2/CentOS7/packages/x86\\_64/daos\\_packages.repo\\' Import GPG key on all nodes: pdsh -w \\$ALL\\_NODES \\'sudo rpm \\--import https://packages.daos.io/RPM-GPG-KEY\\ Perform the additional steps: pdsh -w \\$ALL\\_NODES \\'sudo yum install -y epel-release\\' Install the DAOS server RPMs on the server nodes: pdsh -w \\$SERVER\\_NODES \\'sudo yum install -y daos-server\\' Install the DAOS client RPMs on the client and admin nodes: pdsh -w \\$ALL\\_NODES -x \\$SERVER\\_NODES \\'sudo yum install -y daos-client\\' (Optionally) Install the DAOS test RPMs on the client nodes - typically not required pdsh -w \\$ALL\\_NODES -x \\$SERVER\\_NODES \\'sudo yum install -y daos-tests\\' Hardware Provisioning \u00b6 In this section, PMem (Intel(R) Optane(TM) persistent memory) and NVME SSDs will be prepared and configured to be used by DAOS. Note PMem preparation is required once per DAOS installation. Prepare the pmem devices on Server nodes: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. Please ensure namespaces are unmounted and locally attached SCM & NVMe devices are not in use. Please be patient as it may take several minutes and subsequent reboot maybe required. Are you sure you want to continue? (yes/no) yes A reboot is required to process new SCM memory allocation goals. Reboot the server node. Run the prepare cmdline again: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... SCM namespaces: SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 0 3.2 TB Prepare the NVME devices on Server nodes: daos\\_server storage prepare \\--nvme-only -u root Preparing locally-attached NVMe storage\\... Scan the available storage on the Server nodes: daos\\_server storage scan Scanning locally-attached storage\\... NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:5e:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:5f:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:81:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB 0000:da:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 1 3.2 TB Generate certificates \u00b6 In this section certificates will be generated and installed for encrypting the DAOS control plane communications. Administrative nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the current user Admin certificate (admin.crt) owned by the current user Admin key (admin.key) owned by the current user Client nodes require the following certificate files: CA root certificate (daosCa.crt) owned by the current user Agent certificate (agent.crt) owned by the daos_agent user Agent key (agent.key) owned by the daos_agent user Server nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the daos_server user Server certificate (server.crt) owned by the daos_server user Server key (server.key) owned by the daos_server user A copy of the Client certificate (client.crt) owned by the daos_server user See https://daos-stack.github.io/admin/deployment/#certificate-configuration for more informaation. Note The following commands are run from the \\$ADMIN\\_NODE . Generate a new set of certificates: cd /tmp /usr/lib64/daos/certgen/gen\\_certificates.sh Note These files should be protected from unauthorized access and preserved for future use. Copy the certificates to a common location on each node in order to move them to the final location: pdsh -S -w \\$ALL\\_NODES -x \\$(hostname -s) scp -r \\$(hostname -s):/tmp/daosCA /tmp Copy the certificates to their default location (/etc/daos) on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the admin nodes then use the following command to create it: pdsh -S -w \\$ADMIN\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the client nodes, use the following command to create it: pdsh -S -w \\$CLIENT\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each server node: pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.key /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/clients/agent.crt Set the ownership of the admin certificates on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/admin.\\* Set the ownership of the client certificates on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$CLIENT\\_NODES sudo chown daos\\_agent:daos\\_agent /etc/daos/certs/agent.\\* Set the ownership of the server certificates on each server node: pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/daosCA.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/server.\\* pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients/agent.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients Create Configuration Files \u00b6 In this section the daos\\_server , daos\\_agent , and dmg command configuration files will be defined. Examples are available at https://github.com/daos-stack/daos/tree/release/1.2/utils/config/examples Determine the addresses for the NVMe devices on the server nodes: pdsh -S -w \\$SERVER\\_NODES sudo lspci \\| grep -i nvme Note Save the addresses of the NVMe devices to use with each DAOS server, e.g. \\\"81:00.0\\\", from each server node. This information will be used to populate the \\\"bdev_list\\\" server configuration parameter below. Create a server configuration file by modifying the default /etc/daos/daos\\_server.yml file on the server nodes. An example of the daos_server.yml is presented below. Copy the modified server yaml file to all the server nodes at `/etc/daos/daos_server.yml. More details on configuring the daos_server.yml file are available at Server configuration file details . name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false client_cert_dir: /etc/daos/certs/clients ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/server.crt key: /etc/daos/certs/server.key provider: ofi+verbs;ofi_rxm socket_dir: /var/run/daos_server nr_hugepages: 4096 control_log_mask: DEBUG control_log_file: /tmp/daos_server.log helper_log_file: /tmp/daos_admin.log engines: - targets: 8 nr_xs_helpers: 0 fabric_iface: ib0 fabric_iface_port: 31316 log_mask: INFO log_file: /tmp/daos_engine_0.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos0 scm_class: dcpm scm_list: [/dev/pmem0] bdev_class: nvme bdev_list: [\"0000:81:00.0\"] # generate regular nvme.conf - targets: 8 nr_xs_helpers: 0 fabric_iface: ib1 fabric_iface_port: 31416 log_mask: INFO log_file: /tmp/daos_engine_1.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos1 scm_class: dcpm scm_list: [/dev/pmem1] bdev_class: nvme bdev_list: [\"0000:83:00.0\"] # generate regular nvme.conf Copy the modified server yaml file to all the server nodes at /etc/daos/daos_server.yml . Create an agent configuration file by modifying the default /etc/daos/daos_agent.yml file on the client nodes. The following is an example daos_agent.yml. Copy the modified agent yaml file to all the client nodes at /etc/daos/daos_agent.yml . More details on configuring the daos_agent.yml file are available at Agent configuration file details name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/agent.crt key: /etc/daos/certs/agent.key runtime_dir: /var/run/daos_agent log_file: /tmp/daos_agent.log Create a dmg configuration file by modifying the default /etc/daos/daos_control.yml file on the admin node. The following is an example of the daos_control.yml . More details on configuring the daos_control.yml file are available in the DMG configuration file details . name: daos_server port: 10001 hostlist: ['node-4', 'node-5', 'node-6'] transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/admin.crt key: /etc/daos/certs/admin.key Start the DAOS Servers \u00b6 Start daos engines on server nodes: pdsh -S -w $SERVER_NODES \"sudo systemctl daemon-reload\" pdsh -S -w $SERVER_NODES \"sudo systemctl start daos_server\" Check status and format storage: # check status pdsh -S -w $SERVER_NODES \"sudo systemctl status daos_server\" # if you see following format messages (depending on number of servers), proceed to storage format wolf-179: May 05 22:21:03 wolf-179.wolf.hpdd.intel.com daos_server[37431]: Metadata format required on instance 0 # format storage dmg storage format -l $SERVER_NODES --reformat Verify that all servers have started: # system query from ADMIN_NODE dmg system query -v # all the server ranks should show 'Joined' STATE Rank UUID Control Address Fault Domain State Reason ---- ---- --------------- ------------ ----- ------ 0 604c4ffa-563a-49dc-b702-3c87293dbcf3 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 1 f0791f98-4379-4ace-a083-6ca3ffa65756 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 2 745d2a5b-46dd-42c5-b90a-d2e46e178b3e 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined 3 ba6a7800-3952-46ce-af92-bba9daa35048 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined Start the DAOS Agents \u00b6 Start the daos agents on the client nodes: # start agents pdsh -S -w $CLIENT_NODES \"sudo systemctl start daos_agent\" (Optional) Check daos_agent status: # check status pdsh -S -w $CLIENT_NODES \"cat /tmp/daos_agent.log\" # Sample output depending on number of client nodes node-2: agent INFO 2021/05/05 22:38:46 DAOS Agent v1.2 (pid 47580) listening on /var/run/daos_agent/daos_agent.sock node-3: agent INFO 2021/05/05 22:38:53 DAOS Agent v1.2 (pid 39135) listening on /var/run/daos_agent/daos_agent.sock","title":"DAOS CentOS Setup"},{"location":"QSG/setup/#daos-set-up-on-centos","text":"The following instructions detail how to install, set up and start DAOS servers and clients on two or more nodes. This document includes instructions for CentOS. For setup instructions on OpenSuse, refer to the OpenSuse setup . For more details reference the DAOS administration guide: https://daos-stack.github.io/admin/hardware/","title":"DAOS Set-Up on CentOS"},{"location":"QSG/setup/#requirements","text":"The following steps require two or more hosts which will be divided up into admin, client, and server roles. One node can be used for both the admin and client node. All nodes must have: sudo access configured password-less ssh configured pdsh installed (or some other means of running multiple remote commands in parallel) In addition the server nodes should also have: an InfiniBand network adapter configured one or more NVMe devices IOMMU is enabled: https://daos-stack.github.io/admin/predeployment_check/#enable-iommu-optional For the use of the commands outlined on this page the following shell variables will need to be defined: ADMIN_NODE CLIENT_NODES SERVER_NODES ALL_NODES For example, if you want to use node-1 as the admin node, node-2 and node-3 as client nodes, and node-[4-6] as server nodes, these variables would be defined as: ADMIN\\_NODE=node-1 CLIENT\\_NODES=node-2,node-3 SERVER\\_NODES=node-4,node-5,node-6 ALL\\_NODES=\\$ADMIN\\_NODE,\\$CLIENT\\_NODES,\\$SERVER\\_NODES Note If a client node is also serving as an admin node, exclude \\$ADMIN\\_NODE from the ALL\\_NODES assignment to prevent duplication. For example: ALL\\_NODES=\\$CLIENT\\_NODES,\\$SERVER\\_NODES","title":"Requirements"},{"location":"QSG/setup/#rpm-installation","text":"In this section the required RPMs will be installed on each of the nodes based upon their role. Admin and client nodes require the installation of the daos-client RPM and the server nodes require the installation of the daos-server RPM. Configure access to the DAOS package repository at https://packages.daos.io/v1.2 . pdsh -w \\$ALL\\_NODES \\'sudo wget -O /etc/yum.repos.d/daos-packages.repo https://packages.daos.io/v1.2/CentOS7/packages/x86\\_64/daos\\_packages.repo\\' Import GPG key on all nodes: pdsh -w \\$ALL\\_NODES \\'sudo rpm \\--import https://packages.daos.io/RPM-GPG-KEY\\ Perform the additional steps: pdsh -w \\$ALL\\_NODES \\'sudo yum install -y epel-release\\' Install the DAOS server RPMs on the server nodes: pdsh -w \\$SERVER\\_NODES \\'sudo yum install -y daos-server\\' Install the DAOS client RPMs on the client and admin nodes: pdsh -w \\$ALL\\_NODES -x \\$SERVER\\_NODES \\'sudo yum install -y daos-client\\' (Optionally) Install the DAOS test RPMs on the client nodes - typically not required pdsh -w \\$ALL\\_NODES -x \\$SERVER\\_NODES \\'sudo yum install -y daos-tests\\'","title":"RPM Installation"},{"location":"QSG/setup/#hardware-provisioning","text":"In this section, PMem (Intel(R) Optane(TM) persistent memory) and NVME SSDs will be prepared and configured to be used by DAOS. Note PMem preparation is required once per DAOS installation. Prepare the pmem devices on Server nodes: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. Please ensure namespaces are unmounted and locally attached SCM & NVMe devices are not in use. Please be patient as it may take several minutes and subsequent reboot maybe required. Are you sure you want to continue? (yes/no) yes A reboot is required to process new SCM memory allocation goals. Reboot the server node. Run the prepare cmdline again: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... SCM namespaces: SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 0 3.2 TB Prepare the NVME devices on Server nodes: daos\\_server storage prepare \\--nvme-only -u root Preparing locally-attached NVMe storage\\... Scan the available storage on the Server nodes: daos\\_server storage scan Scanning locally-attached storage\\... NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:5e:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:5f:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:81:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB 0000:da:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 1 3.2 TB","title":"Hardware Provisioning"},{"location":"QSG/setup/#generate-certificates","text":"In this section certificates will be generated and installed for encrypting the DAOS control plane communications. Administrative nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the current user Admin certificate (admin.crt) owned by the current user Admin key (admin.key) owned by the current user Client nodes require the following certificate files: CA root certificate (daosCa.crt) owned by the current user Agent certificate (agent.crt) owned by the daos_agent user Agent key (agent.key) owned by the daos_agent user Server nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the daos_server user Server certificate (server.crt) owned by the daos_server user Server key (server.key) owned by the daos_server user A copy of the Client certificate (client.crt) owned by the daos_server user See https://daos-stack.github.io/admin/deployment/#certificate-configuration for more informaation. Note The following commands are run from the \\$ADMIN\\_NODE . Generate a new set of certificates: cd /tmp /usr/lib64/daos/certgen/gen\\_certificates.sh Note These files should be protected from unauthorized access and preserved for future use. Copy the certificates to a common location on each node in order to move them to the final location: pdsh -S -w \\$ALL\\_NODES -x \\$(hostname -s) scp -r \\$(hostname -s):/tmp/daosCA /tmp Copy the certificates to their default location (/etc/daos) on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the admin nodes then use the following command to create it: pdsh -S -w \\$ADMIN\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the client nodes, use the following command to create it: pdsh -S -w \\$CLIENT\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each server node: pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.key /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/clients/agent.crt Set the ownership of the admin certificates on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/admin.\\* Set the ownership of the client certificates on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$CLIENT\\_NODES sudo chown daos\\_agent:daos\\_agent /etc/daos/certs/agent.\\* Set the ownership of the server certificates on each server node: pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/daosCA.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/server.\\* pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients/agent.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients","title":"Generate certificates"},{"location":"QSG/setup/#create-configuration-files","text":"In this section the daos\\_server , daos\\_agent , and dmg command configuration files will be defined. Examples are available at https://github.com/daos-stack/daos/tree/release/1.2/utils/config/examples Determine the addresses for the NVMe devices on the server nodes: pdsh -S -w \\$SERVER\\_NODES sudo lspci \\| grep -i nvme Note Save the addresses of the NVMe devices to use with each DAOS server, e.g. \\\"81:00.0\\\", from each server node. This information will be used to populate the \\\"bdev_list\\\" server configuration parameter below. Create a server configuration file by modifying the default /etc/daos/daos\\_server.yml file on the server nodes. An example of the daos_server.yml is presented below. Copy the modified server yaml file to all the server nodes at `/etc/daos/daos_server.yml. More details on configuring the daos_server.yml file are available at Server configuration file details . name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false client_cert_dir: /etc/daos/certs/clients ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/server.crt key: /etc/daos/certs/server.key provider: ofi+verbs;ofi_rxm socket_dir: /var/run/daos_server nr_hugepages: 4096 control_log_mask: DEBUG control_log_file: /tmp/daos_server.log helper_log_file: /tmp/daos_admin.log engines: - targets: 8 nr_xs_helpers: 0 fabric_iface: ib0 fabric_iface_port: 31316 log_mask: INFO log_file: /tmp/daos_engine_0.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos0 scm_class: dcpm scm_list: [/dev/pmem0] bdev_class: nvme bdev_list: [\"0000:81:00.0\"] # generate regular nvme.conf - targets: 8 nr_xs_helpers: 0 fabric_iface: ib1 fabric_iface_port: 31416 log_mask: INFO log_file: /tmp/daos_engine_1.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos1 scm_class: dcpm scm_list: [/dev/pmem1] bdev_class: nvme bdev_list: [\"0000:83:00.0\"] # generate regular nvme.conf Copy the modified server yaml file to all the server nodes at /etc/daos/daos_server.yml . Create an agent configuration file by modifying the default /etc/daos/daos_agent.yml file on the client nodes. The following is an example daos_agent.yml. Copy the modified agent yaml file to all the client nodes at /etc/daos/daos_agent.yml . More details on configuring the daos_agent.yml file are available at Agent configuration file details name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/agent.crt key: /etc/daos/certs/agent.key runtime_dir: /var/run/daos_agent log_file: /tmp/daos_agent.log Create a dmg configuration file by modifying the default /etc/daos/daos_control.yml file on the admin node. The following is an example of the daos_control.yml . More details on configuring the daos_control.yml file are available in the DMG configuration file details . name: daos_server port: 10001 hostlist: ['node-4', 'node-5', 'node-6'] transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/admin.crt key: /etc/daos/certs/admin.key","title":"Create Configuration Files"},{"location":"QSG/setup/#start-the-daos-servers","text":"Start daos engines on server nodes: pdsh -S -w $SERVER_NODES \"sudo systemctl daemon-reload\" pdsh -S -w $SERVER_NODES \"sudo systemctl start daos_server\" Check status and format storage: # check status pdsh -S -w $SERVER_NODES \"sudo systemctl status daos_server\" # if you see following format messages (depending on number of servers), proceed to storage format wolf-179: May 05 22:21:03 wolf-179.wolf.hpdd.intel.com daos_server[37431]: Metadata format required on instance 0 # format storage dmg storage format -l $SERVER_NODES --reformat Verify that all servers have started: # system query from ADMIN_NODE dmg system query -v # all the server ranks should show 'Joined' STATE Rank UUID Control Address Fault Domain State Reason ---- ---- --------------- ------------ ----- ------ 0 604c4ffa-563a-49dc-b702-3c87293dbcf3 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 1 f0791f98-4379-4ace-a083-6ca3ffa65756 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 2 745d2a5b-46dd-42c5-b90a-d2e46e178b3e 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined 3 ba6a7800-3952-46ce-af92-bba9daa35048 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined","title":"Start the DAOS Servers"},{"location":"QSG/setup/#start-the-daos-agents","text":"Start the daos agents on the client nodes: # start agents pdsh -S -w $CLIENT_NODES \"sudo systemctl start daos_agent\" (Optional) Check daos_agent status: # check status pdsh -S -w $CLIENT_NODES \"cat /tmp/daos_agent.log\" # Sample output depending on number of client nodes node-2: agent INFO 2021/05/05 22:38:46 DAOS Agent v1.2 (pid 47580) listening on /var/run/daos_agent/daos_agent.sock node-3: agent INFO 2021/05/05 22:38:53 DAOS Agent v1.2 (pid 39135) listening on /var/run/daos_agent/daos_agent.sock","title":"Start the DAOS Agents"},{"location":"QSG/suseQSG/","text":"Quick Start openSUSE/SLES 15.2 with POSIX \u00b6 Introduction Requirements Set-Up Run CART selftest Run DAOS PERF* Run DAOS_RACER* Run DAOS_TEST* Create pool and dfuse mountpoint for IOR, MDTEST and DATAMOVER Create pool Create container Set up dfuse mount point: Run IOR Build IOR Run MDTEST Run DBENCH Run DATAMOVER Run DATAMOVER using mpifileutils Build mpifileutils Clean Up Introduction \u00b6 The purpose of this Quick Start is to provider a user with a set of cmdlines to quickly setup and use DAOS with POSIX on openSUSE/SLES 15.2. This document covers installation of daos rpms on openSUSE/SLES 15.2 and updating the DAOS configuration files needed by daos servers The quick start describes how to use dfuse in order to take advantage of daos support for POSIX. The quick start steps users through running benchmarking tools like ior and mdtest along with some examples of how to move data between a POSIX file system and daos containers (and vise versa) and finally cleaning up your daos setup. Requirements \u00b6 The quick start requires a minimum of 1 server with PMEM and SSDs connected via infiniband storage network and 1 client node and 1 admin node without pmem/ssd but on the infiniband storage network. All nodes have a base openSUSE or SLES 15.2 installed. Install pdsh on the admin node sudo zypper install pdsh For example, if one wanted to use node-1 as their admin node, node-2 and node-3 as client nodes, and node-4 and node-5 as their server nodes then these variables would be defined as: ADMIN_NODE=node-1 CLIENT_NODES=node-2, node-3 SERVER_NODES=node-4, node-5 ALL_NODES=\\$ADMIN_NODE,\\$CLIENT_NODES,\\$SERVER_NODES If a client node is also serving as an admin node then exclude \\$ADMIN_NODE from the ALL_NODES assignment to prevent duplication, e.g. ALL_NODES=\\$CLIENT_NODES,\\$SERVER_NODES Set-Up \u00b6 Please refer here for initial set up which consists of rpm installation, generate and set up certificates, setting up config files, starting servers and agents. For this quick start, the daos-tests package will need to be installed on the client nodes The following applications will be run from a client node: \\\"*\\\" Indicates that the cmdline will run internally only; cmdline should be removed for external customers Run CART selftest \u00b6 SHARED_DIR=\\<shared dir by all nodes> export FI_UNIVERSE_SIZE=2048 export OFI_INTERFACE=eth0 # selt_test --help for more details on params #Generate the attach info file (enable SHARED_DIR with perms for sudo to write ) sudo daos_agent -o /etc/daos/daos_agent.yml -l \\$SHARED_DIR/daos_agent.log dump-attachinfo -o \\$SHARED_DIR/daos_server.attach_info_tmp # Run: self_test --path \\$SHARED_DIR --group-name daos_server --endpoint 0-1:0 (for 4 servers --endpoint 0-3:0 ranks:tags) # Sample output: Adding endpoints: ranks: 0-1 (# ranks = 2) tags: 0 (# tags = 1) Warning: No --master-endpoint specified; using this command line application as the master endpoint Self Test Parameters: Group name to test against: daos_server # endpoints: 2 Message sizes: [(200000-BULK_GET 200000-BULK_PUT), (200000-BULK_GET 0-EMPTY), (0-EMPTY 200000-BULK_PUT), (200000-BULK_GET 1000-IOV), (1000-IOV 200000-BULK_PUT), (1000-IOV 1000-IOV), (1000-IOV 0-EMPTY), (0-EMPTY 1000-IOV), (0-EMPTY 0-EMPTY)] Buffer addresses end with: \\<Default> Repetitions per size: 20000 Max inflight RPCs: 1000 CLI [rank=0 pid=3255] Attached daos_server ################################################## Results for message size (200000-BULK_GET 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 222.67 RPC Throughput (RPCs/sec): 584 RPC Latencies (us): Min : 27191 25th %: 940293 Median : 1678137 75th %: 2416765 Max : 3148987 Average: 1671626 Std Dev: 821872.40 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2416764 1:0 - 969063 ################################################## Results for message size (200000-BULK_GET 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.08 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 2880 25th %: 1156162 Median : 1617356 75th %: 2185604 Max : 2730569 Average: 1659133 Std Dev: 605053.68 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2185589 1:0 - 1181363 ################################################## Results for message size (0-EMPTY 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.11 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 4956 25th %: 747786 Median : 1558111 75th %: 2583834 Max : 3437395 Average: 1659959 Std Dev: 1078975.59 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2583826 1:0 - 776862 ################################################## Results for message size (200000-BULK_GET 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.57 RPC Throughput (RPCs/sec): 587 RPC Latencies (us): Min : 2755 25th %: 12341 Median : 1385716 75th %: 3393178 Max : 3399349 Average: 1660125 Std Dev: 1446054.82 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 12343 1:0 - 3393174 ################################################## Results for message size (1000-IOV 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.68 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 4557 25th %: 522380 Median : 1640322 75th %: 2725419 Max : 3441963 Average: 1661254 Std Dev: 1147206.09 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 600190 1:0 - 2725402 ################################################## Results for message size (1000-IOV 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 88.87 RPC Throughput (RPCs/sec): 46595 RPC Latencies (us): Min : 1165 25th %: 21374 Median : 21473 75th %: 21572 Max : 21961 Average: 20923 Std Dev: 2786.99 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 21430 1:0 - 21516 ################################################## Results for message size (1000-IOV 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 59.03 RPC Throughput (RPCs/sec): 61902 RPC Latencies (us): Min : 1164 25th %: 15544 Median : 16104 75th %: 16575 Max : 17237 Average: 15696 Std Dev: 2126.37 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 15579 1:0 - 16571 ################################################## Results for message size (0-EMPTY 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 46.93 RPC Throughput (RPCs/sec): 49209 RPC Latencies (us): Min : 945 25th %: 20327 Median : 20393 75th %: 20434 Max : 20576 Average: 19821 Std Dev: 2699.27 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 20393 1:0 - 20393 ################################################## Results for message size (0-EMPTY 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 0.00 RPC Throughput (RPCs/sec): 65839 RPC Latencies (us): Min : 879 25th %: 14529 Median : 15108 75th %: 15650 Max : 16528 Average: 14765 Std Dev: 2087.87 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 14569 1:0 - 15649 Run DAOS PERF* \u00b6 (requires openmpi3 - libmpi.so .40 ) module load gnu-openmpi/3.1.6 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH export D_LOG_FILE=/tmp/daos_perf.log # Single process daos_perf -a 64 -d 256 -c R2S -P 20G -T daos -s 1k -R \\\"U;pV\\\" -g /etc/daos/daos_control.yml # MPI orterun --enable-recovery -x D_LOG_FILE=/tmp/daos_perf_daos.log --host \\<host name>:4 --map-by node --mca btl_openib_warn_default_gid_prefix \\\"0\\\" --mca btl \\\"tcp,self\\\" --mca oob \\\"tcp\\\" --mca pml \\\"ob1\\\" --mca btl_tcp_if_include \\\"eth0\\\" --np 4 --tag-output /usr/bin/daos_perf -a 64 -d 256 -c R2S -P 20G -T daos -s 1k -R \\\"U;pV\\\" -g /etc/daos/daos_control.yml # Sample Output: Test : DAOS R2S (full stack, 2 replica) Pool : 9c88849b-b0d6-4444-bb39-42769a7a1ef5 Parameters : pool size : SCM: 20480 MB, NVMe: 0 MB credits : -1 (sync I/O for -ve) obj_per_cont : 1 x 1 (procs) dkey_per_obj : 256 akey_per_dkey : 64 recx_per_akey : 16 value type : single stride size : 1024 zero copy : no VOS file : \\<NULL> Running test=UPDATE Running UPDATE test (iteration=1) UPDATE successfully completed: duration : 91.385233 sec bandwith : 2.801 MB/sec rate : 2868.56 IO/sec latency : 348.607 us (nonsense if credits > 1) Duration across processes: MAX duration : 91.385233 sec MIN duration : 91.385233 sec Average duration : 91.385233 sec Completed test=UPDATE Run DAOS_RACER* \u00b6 (requires openmpi3 - libmpi.so .40 ) module load gnu-openmpi/3.1.6 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH # RUN: export D_LOG_FILE=/tmp/daos_racer.log export D_LOG_MASK=ERR /usr/bin/daos_racer -n /etc/daos/daos_control.yml NOTE: daos_racer is currently disabled due to DAOS-7359 Run DAOS_TEST* \u00b6 (requires openmpi3 - libmpi.so .40 ) module load gnu-openmpi/3.1.6 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH # RUN: export OFI_INTERFACE=eth0 export POOL_SCM_SIZE=8G export POOL_NVME_SIZE=16G daos_test -pctVAKCoRb -n /etc/daos/daos_control.yml # Sample output from -p (pool test) ================= DAOS pool tests.. ===================== [==========] Running 14 test(s). setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool b91606bb-87dd-4a57-8eee-5d8747a37b31 [ RUN ] POOL1: connect to non-existing pool [ OK ] POOL1: connect to non-existing pool [ RUN ] POOL2: connect/disconnect to pool rank 0 connecting to pool synchronously ... success rank 0 querying pool info... success rank 0 disconnecting from pool synchronously ... success rank 0 success [ OK ] POOL2: connect/disconnect to pool [ RUN ] POOL3: connect/disconnect to pool (async) rank 0 connecting to pool asynchronously ... success rank 0 querying pool info... success rank 0 disconnecting from pool asynchronously ... success rank 0 success [ OK ] POOL3: connect/disconnect to pool (async) [ RUN ] POOL4: pool handle local2global and global2local rank 0 connecting to pool synchronously ... success rank 0 querying pool info... success rank 0 call local2global on pool handlesuccess rank 0 broadcast global pool handle ...success rank 0 disconnecting from pool synchronously ... success rank 0 success [ OK ] POOL4: pool handle local2global and global2local [ RUN ] POOL5: exclusive connection SUBTEST 1: other connections already exist; shall get -1012 establishing a non-exclusive connection trying to establish an exclusive connection disconnecting the non-exclusive connection SUBTEST 2: no other connections; shall succeed establishing an exclusive connection SUBTEST 3: shall prevent other connections (-1012) trying to establish a non-exclusive connection disconnecting the exclusive connection [ OK ] POOL5: exclusive connection [ RUN ] POOL6: exclude targets and query pool info Skip it for now, because CaRT can\\'t support subgroup membership, excluding a node w/o killing it will cause IV issue. [ OK ] POOL6: exclude targets and query pool info [ RUN ] POOL7: set/get/list user-defined pool attributes (sync) setup: connecting to pool connected to pool, ntarget=16 setting pool attributes synchronously ... listing pool attributes synchronously ... Verifying Total Name Length.. Verifying Small Name.. Verifying All Names.. getting pool attributes synchronously ... Verifying Name-Value (A).. Verifying Name-Value (B).. Verifying with NULL buffer.. Deleting all attributes Verifying all attributes deletion [ OK ] POOL7: set/get/list user-defined pool attributes (sync) [ RUN ] POOL8: set/get/list user-defined pool attributes (async) setting pool attributes asynchronously ... listing pool attributes asynchronously ... Verifying Total Name Length.. Verifying Small Name.. Verifying All Names.. getting pool attributes asynchronously ... Verifying Name-Value (A).. Verifying Name-Value (B).. Verifying with NULL buffer.. Deleting all attributes Verifying all attributes deletion [ OK ] POOL8: set/get/list user-defined pool attributes (async) [ RUN ] POOL9: pool reconnect after daos re-init connected to pool, ntarget=16 [ OK ] POOL9: pool reconnect after daos re-init [ RUN ] POOL10: pool create with properties and query create pool with properties, and query it to verify. setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 6b686dce-8cc3-40e6-b706-4c5aedfee4a9 setup: connecting to pool connected to pool, ntarget=16 ACL prop matches expected defaults teardown: destroyed pool 6b686dce-8cc3-40e6-b706-4c5aedfee4a9 [ OK ] POOL10: pool create with properties and query [ RUN ] POOL11: pool list containers (zero) setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool f3bc2fb8-17aa-45cc-9b1a-87ffebddb048 setup: connected to pool: f3bc2fb8-17aa-45cc-9b1a-87ffebddb048 daos_pool_list_cont returned rc=0 success t0: output nconts=0 verifying conts[0..10], nfilled=0 success t1: conts[] over-sized success t2: nconts=0, non-NULL conts[] rc=0 success t3: in &nconts NULL, -DER_INVAL success teardown: destroyed pool f3bc2fb8-17aa-45cc-9b1a-87ffebddb048 [ OK ] POOL11: pool list containers (zero) [ RUN ] POOL12: pool list containers (many) setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool d0fbd1a8-2061-428c-9cfb-376f201689fc setup: connected to pool: d0fbd1a8-2061-428c-9cfb-376f201689fc setup: alloc lcarg->conts len 16 setup: creating container: b52e36e6-26db-4e51-9bbe-df776624ca59 setup: creating container: 645212d3-3fad-446d-be2a-c362b0884555 setup: creating container: 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 setup: creating container: 07cbf32b-beaf-444a-af64-5602aa41485f setup: creating container: 427824a9-a6c7-4417-83c8-1fc2f5e49c83 setup: creating container: 898ffab6-9472-47cc-a6ca-9a6fe54704c0 setup: creating container: d4d3181c-7376-41fa-8f81-fb363b7b8e2e setup: creating container: c1253a14-991d-4e2a-a3cf-ff2e4b160240 setup: creating container: 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 setup: creating container: 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 setup: creating container: 480f2232-58c9-444e-bff5-36e2efa3609d setup: creating container: d33ae63c-eeb4-411f-8ca4-c70eb86371dc setup: creating container: d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb setup: creating container: f789b108-96f1-4052-ad22-c6fb5b02cd19 setup: creating container: 2c83b9d1-a723-49da-912b-63ffe3e43930 setup: creating container: 31e5df3b-809f-47d8-b170-68329b687361 daos_pool_list_cont returned rc=0 success t0: output nconts=16 verifying conts[0..26], nfilled=16 container 645212d3-3fad-446d-be2a-c362b0884555 found in list result container 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 found in list result container d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb found in list result container b52e36e6-26db-4e51-9bbe-df776624ca59 found in list result container 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 found in list result container 2c83b9d1-a723-49da-912b-63ffe3e43930 found in list result container d33ae63c-eeb4-411f-8ca4-c70eb86371dc found in list result container 427824a9-a6c7-4417-83c8-1fc2f5e49c83 found in list result container 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 found in list result container f789b108-96f1-4052-ad22-c6fb5b02cd19 found in list result container 480f2232-58c9-444e-bff5-36e2efa3609d found in list result container 07cbf32b-beaf-444a-af64-5602aa41485f found in list result container 31e5df3b-809f-47d8-b170-68329b687361 found in list result container 898ffab6-9472-47cc-a6ca-9a6fe54704c0 found in list result container c1253a14-991d-4e2a-a3cf-ff2e4b160240 found in list result container d4d3181c-7376-41fa-8f81-fb363b7b8e2e found in list result success t1: conts[] over-sized success t2: nconts=0, non-NULL conts[] rc=0 success t3: in &nconts NULL, -DER_INVAL verifying conts[0..16], nfilled=16 container 645212d3-3fad-446d-be2a-c362b0884555 found in list result container 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 found in list result container d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb found in list result container b52e36e6-26db-4e51-9bbe-df776624ca59 found in list result container 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 found in list result container 2c83b9d1-a723-49da-912b-63ffe3e43930 found in list result container d33ae63c-eeb4-411f-8ca4-c70eb86371dc found in list result container 427824a9-a6c7-4417-83c8-1fc2f5e49c83 found in list result container 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 found in list result container f789b108-96f1-4052-ad22-c6fb5b02cd19 found in list result container 480f2232-58c9-444e-bff5-36e2efa3609d found in list result container 07cbf32b-beaf-444a-af64-5602aa41485f found in list result container 31e5df3b-809f-47d8-b170-68329b687361 found in list result container 898ffab6-9472-47cc-a6ca-9a6fe54704c0 found in list result container c1253a14-991d-4e2a-a3cf-ff2e4b160240 found in list result container d4d3181c-7376-41fa-8f81-fb363b7b8e2e found in list result success t4: conts[] exact length verifying conts[0..15], nfilled=0 success t5: conts[] under-sized success teardown: destroy container: b52e36e6-26db-4e51-9bbe-df776624ca59 teardown: destroy container: 645212d3-3fad-446d-be2a-c362b0884555 teardown: destroy container: 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 teardown: destroy container: 07cbf32b-beaf-444a-af64-5602aa41485f teardown: destroy container: 427824a9-a6c7-4417-83c8-1fc2f5e49c83 teardown: destroy container: 898ffab6-9472-47cc-a6ca-9a6fe54704c0 teardown: destroy container: d4d3181c-7376-41fa-8f81-fb363b7b8e2e teardown: destroy container: c1253a14-991d-4e2a-a3cf-ff2e4b160240 teardown: destroy container: 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 teardown: destroy container: 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 teardown: destroy container: 480f2232-58c9-444e-bff5-36e2efa3609d teardown: destroy container: d33ae63c-eeb4-411f-8ca4-c70eb86371dc teardown: destroy container: d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb teardown: destroy container: f789b108-96f1-4052-ad22-c6fb5b02cd19 teardown: destroy container: 2c83b9d1-a723-49da-912b-63ffe3e43930 teardown: destroy container: 31e5df3b-809f-47d8-b170-68329b687361 teardown: destroyed pool d0fbd1a8-2061-428c-9cfb-376f201689fc [ OK ] POOL12: pool list containers (many) [ RUN ] POOL13: retry POOL_{CONNECT,DISCONNECT,QUERY} setting DAOS_POOL_CONNECT_FAIL_CORPC ... success connecting to pool ... success setting DAOS_POOL_QUERY_FAIL_CORPC ... success querying pool info... success setting DAOS_POOL_DISCONNECT_FAIL_CORPC ... success disconnecting from pool ... success [ OK ] POOL13: retry POOL_{CONNECT,DISCONNECT,QUERY} [ RUN ] POOL14: pool connect access based on ACL pool ACL gives the owner no permissions setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 86a9951c-08f2-4ad0-a4ad-037f8b85a656 setup: connecting to pool daos_pool_connect failed, rc: -1001 failed to connect pool: -1001 pool disconnect failed: -1002 teardown: destroyed pool 86a9951c-08f2-4ad0-a4ad-037f8b85a656 pool ACL gives the owner RO, they want RW setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 15d9c7f9-140e-436d-88d0-2d5924348725 setup: connecting to pool daos_pool_connect failed, rc: -1001 failed to connect pool: -1001 pool disconnect failed: -1002 teardown: destroyed pool 15d9c7f9-140e-436d-88d0-2d5924348725 pool ACL gives the owner RO, they want RO setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool b6ce2f69-94a8-4fa7-8c43-b35766a0fb6f setup: connecting to pool connected to pool, ntarget=16 teardown: destroyed pool b6ce2f69-94a8-4fa7-8c43-b35766a0fb6f pool ACL gives the owner RW, they want RO setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 992015b0-7c70-458e-afa5-eecae4ad5313 setup: connecting to pool connected to pool, ntarget=16 teardown: destroyed pool 992015b0-7c70-458e-afa5-eecae4ad5313 pool ACL gives the owner RW, they want RW setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 464bfccf-2ec9-4105-b791-f1163960dc0d setup: connecting to pool connected to pool, ntarget=16 teardown: destroyed pool 464bfccf-2ec9-4105-b791-f1163960dc0d [ OK ] POOL14: pool connect access based on ACL teardown: destroyed pool b91606bb-87dd-4a57-8eee-5d8747a37b31 [==========] 14 test(s) run. [ PASSED ] 14 test(s). Create pool and dfuse mountpoint for IOR, MDTEST and DATAMOVER \u00b6 Create pool \u00b6 dmg pool create --name=daos_test_pool --size=500G # Sample output Creating DAOS pool with automatic storage allocation: 500 GB NVMe + 6.00% SCM Pool created with 6.00% SCM/NVMe ratio --------------------------------------- UUID : acf889b6-f290-4d7b-823a-5fae0014a64d Service Ranks : 0 Storage Ranks : 0 Total Size : 530 GB SCM : 30 GB (30 GB / rank) NVMe : 500 GB (500 GB / rank) dmg pool list # Sample output Pool UUID Svc Replicas -------------- ---------------- acf889b6-f290-4d7b-823a-5fae0014a64d 0 DAOS_POOL=\\<pool uuid> (define on all clients) Create container \u00b6 daos cont create --type=POSIX --oclass=SX --pool=\\$DAOS_POOL DAOS_CONT=\\<cont uuid> (define on all clients) Set up dfuse mount point: \u00b6 ( Run dfuse on all client nodes ) # Create directory mkdir -p /tmp/daos_dfuse/daos_test # Use dfuse to mount the daos container to the above directory dfuse --container \\$DAOS_CONT --disable-direct-io --mountpoint /tmp/daos_dfuse/daos_test --pool \\$DAOS_POOL # verfiy that the file type is dfuse df -h # Sample output dfuse 500G 17G 34G 34% /tmp/daos_dfuse/daos_test Run IOR \u00b6 (uses mpich in the examples) Build IOR \u00b6 ( Requires mpich added to PATH and LD_LIBRARY_PATH ) module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Download ior source git clone https://github.com/hpc/ior.git # Build IOR cd ior ./bootstrap mkdir build;cd build MPICC=mpicc ../configure --with-daos=/usr --prefix=\\<your dir> make make install # Add IOR to paths add \\<your dir>/lib to LD_LIBRARY_PATh and \\<your dir>/bin to PATH module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Run: mpirun -np 20 -ppn 10 -hosts host1,host2 ior -a POSIX -b 1G -v -w -W -r -R -k -F -T 10 -i 1 -s 1 -o /tmp/daos_dfuse/daos_test/testfile -t 1G # Sample output IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O Began : Thu Apr 29 16:31:55 2021 Command line : ior -a POSIX -b 1G -v -w -W -r -R -k -F -T 10 -i 1 -s 1 -o /tmp/daos_dfuse/daos_test/testfile -t 1G Machine : Linux wolf-184 Start time skew across all tasks: 0.00 sec TestID : 0 StartTime : Thu Apr 29 16:31:55 2021 Path : /tmp/daos_dfuse/daos_test/testfile.00000000 FS : 493.6 GiB Used FS: 6.6% Inodes: -0.0 Mi Used Inodes: 0.0% Participating tasks : 20 Options: api : POSIX apiVersion : test filename : /tmp/daos_dfuse/daos_test/testfile access : file-per-process type : independent segments : 1 ordering in a file : sequential ordering inter file : no tasks offsets nodes : 2 tasks : 20 clients per node : 10 repetitions : 1 xfersize : 1 GiB blocksize : 1 GiB aggregate filesize : 20 GiB verbose : 1 Results: access bw(MiB/s) IOPS Latency(s) block(KiB) xfer(KiB) open(s) wr/rd(s) close(s) total(s) iter ------ --------- ---- ---------- ---------- --------- -------- -------- -------- -------- ---- Commencing write performance test: Thu Apr 29 16:31:56 2021 write 224.18 0.218924 91.34 1048576 1048576 0.024130 91.36 0.000243 91.36 0 Verifying contents of the file(s) just written. Thu Apr 29 16:33:27 2021 Commencing read performance test: Thu Apr 29 16:34:59 2021 read 223.26 0.218024 91.60 1048576 1048576 0.137707 91.73 0.000087 91.73 0 Max Write: 224.18 MiB/sec (235.07 MB/sec) Max Read: 223.26 MiB/sec (234.10 MB/sec) Summary of all tests: Operation Max(MiB) Min(MiB) Mean(MiB) StdDev Max(OPs) Min(OPs) Mean(OPs) StdDev Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggs(MiB) API RefNum write 224.18 224.18 224.18 0.00 0.22 0.22 0.22 0.00 91.35618 NA NA 0 20 10 1 1 0 1 0 0 1 1073741824 1073741824 20480.0 POSIX 0 read 223.26 223.26 223.26 0.00 0.22 0.22 0.22 0.00 91.73292 NA NA 0 20 10 1 1 0 1 0 0 1 1073741824 1073741824 20480.0 POSIX 0 Finished : Thu Apr 29 16:36:31 2021 Run MDTEST \u00b6 module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Create 10000 files # Run: mpirun -np 20 -ppn 10 -hosts host1,host2 mdtest -a POSIX -z 0 -N 1 -P -i 1 -n 500 -e 4096 -d /tmp/daos_dfuse/daos_test -w 4096 #Sample output -- started at 04/29/2021 17:09:02 -- mdtest-3.4.0+dev was launched with 20 total task(s) on 2 node(s) Command line used: mdtest \\'-a\\' \\'POSIX\\' \\'-z\\' \\'0\\' \\'-N\\' \\'1\\' \\'-P\\' \\'-i\\' \\'1\\' \\'-n\\' \\'500\\' \\'-e\\' \\'4096\\' \\'-d\\' \\'/tmp/daos_dfuse/daos_test\\' \\'-w\\' \\'4096\\' Path: /tmp/daos_dfuse FS: 36.5 GiB Used FS: 86.5% Inodes: 2.3 Mi Used Inodes: 9.7% Nodemap: 11111111110000000000 V-0: Rank 0 Line 2216 Shifting ranks by 10 for each phase. 20 tasks, 10000 files/directories SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- --- --- ---- ------- Directory creation : 2249.104 2249.085 2249.094 0.009 Directory stat : 4089.121 4089.116 4089.118 0.001 Directory removal : 318.313 318.311 318.312 0.000 File creation : 1080.348 1080.334 1080.341 0.007 File stat : 1676.635 1676.619 1676.632 0.004 File read : 1486.296 1486.291 1486.295 0.002 File removal : 611.135 611.132 611.133 0.001 Tree creation : 667.967 667.967 667.967 0.000 Tree removal : 18.063 18.063 18.063 0.000 SUMMARY time: (of 1 iterations) Operation Max Min Mean Std Dev --------- --- --- ---- ------- Directory creation : 4.446 4.446 4.446 0.000 Directory stat : 2.446 2.446 2.446 0.000 Directory removal : 31.416 31.416 31.416 0.000 File creation : 9.256 9.256 9.256 0.000 File stat : 5.964 5.964 5.964 0.000 File read : 6.728 6.728 6.728 0.000 File removal : 16.363 16.363 16.363 0.000 Tree creation : 0.001 0.001 0.001 0.000 Tree removal : 0.055 0.055 0.055 0.000 -- finished at 04/29/2021 17:10:19 -- # Create 1000000 files #Run: mpirun -np 20 -ppn 10 -hosts host1,host2 mdtest -a POSIX -z 0 -N 1 -P -i 1 -n 50000 -e 4096 -d /tmp/daos_dfuse/daos_test -w 4096 Run DBENCH \u00b6 module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Run: dbench -c /usr/share/dbench/client.txt -t 10 -D /tmp/daos_dfuse/daos_test 10 #Sample output dbench version 3.04 - Copyright Andrew Tridgell 1999-2004 Running for 10 seconds with load \\'/usr/share/dbench/client.txt\\' and minimum warmup 2 secs 10 clients started 10 131 58.09 MB/sec warmup 1 sec 10 401 42.01 MB/sec execute 1 sec 10 538 43.39 MB/sec execute 2 sec 10 682 37.23 MB/sec execute 3 sec 10 803 28.65 MB/sec execute 4 sec 10 898 23.36 MB/sec execute 5 sec 10 980 19.87 MB/sec execute 6 sec 10 1074 18.43 MB/sec execute 7 sec 10 1161 16.25 MB/sec execute 8 sec 10 1240 14.52 MB/sec execute 9 sec 10 1367 15.67 MB/sec cleanup 10 sec 10 1367 14.25 MB/sec cleanup 11 sec 10 1367 14.08 MB/sec cleanup 11 sec Throughput 15.6801 MB/sec 10 procs Run DATAMOVER \u00b6 For more details on datamover reference: https://github.com/hpc/mpifileutils/blob/master/DAOS-Support.md # Create a POSIX container daos container create --pool \\$DAOS_POOL --type POSIX DAOS_CONT2=\\<cont uuid> # Copy POSIX directory to POSIX container (only directory copies are supported in 1.2) daos filesystem copy --src /tmp/daos_dfuse/daos_test --dst daos://\\$DAOS_POOL/\\$DAOS_CONT2 # Copy the same POSIX container to a different POSIX directory daos filesystem copy --src daos://\\$DAOS_POOL/\\$DAOS_CONT2 --dst /tmp/datamover2 ls -latr /tmp/datamover2/daos_test/ # Sample output ls -la /tmp/daos_dfuse/daos_test/ total 10485760 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000009 ls -la /tmp/datamover2/daos_test/ total 10485808 drwx------ 2 mjean mjean 4096 Apr 29 17:45 . drwx------ 3 mjean mjean 4096 Apr 29 17:43 .. -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:43 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:45 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:43 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:45 testfile.00000009 Run DATAMOVER using mpifileutils \u00b6 Build mpifileutils \u00b6 Mpifileutils can be built using dependency packages or dependencies built from source For more details on mpifileutils reference: https://github.com/hpc/mpifileutils/blob/master/DAOS-Support.md # Install the following packages: zypper install mpich-devel libbz2-devel # Setup environment (on launch node) #Setup mpich env module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH export MPI_HOME=\\<mpich path> Build mpifileutils with dependencies installed from packages # Install build dependencies (on all client nodes) sudo zypper install dtcmp-mpich-devel libcircle-mpich-devel libcap-devel # Build mpifileutils from installed packages (on all client nodes) git clone --depth 1 https://github.com/hpc/mpifileutils mkdir build install cd build cmake ../mpifileutils -DENABLE_DAOS=ON \\ -DENABLE_LIBARCHIVE=OFF \\ -DDTCMP_INCLUDE_DIRS=/usr/lib64/mpi/gcc/mpich/include \\ -DDTCMP_LIBRARIES=/usr/lib64/mpi/gcc/mpich/lib64/libdtcmp.so \\ -DLibCircle_INCLUDE_DIRS=/usr/lib64/mpi/gcc/mpich/include \\ -DLibCircle_LIBRARIES=/usr/lib64/mpi/gcc/mpich/lib64/libcircle.so \\ -DWITH_CART_PREFIX=/usr \\ -DWITH_DAOS_PREFIX=/usr \\ -DCMAKE_INSTALL_INCLUDEDIR=/usr/lib64/mpi/gcc/mpich/include \\ -DCMAKE_INSTALL_PREFIX=/usr/lib64/mpi/gcc/mpich/ \\ -DCMAKE_INSTALL_LIBDIR=/usr/lib64/mpi/gcc/mpich/lib64 sudo make install Build mpifileutils with dependencies that are built from source mkdir install installdir=`pwd`/install export CC=mpicc # download dependencies and build mkdir deps cd deps wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz wget https://github.com/llnl/lwgrp/releases/download/v1.0.3/lwgrp-1.0.3.tar.gz wget https://github.com/llnl/dtcmp/releases/download/v1.1.1/dtcmp-1.1.1.tar.gz wget https://github.com/libarchive/libarchive/releases/download/3.5.1/libarchive-3.5.1.tar.gz tar -zxf libcircle-0.3.0.tar.gz cd libcircle-0.3.0 ./configure --prefix=\\$installdir make install cd .. tar -zxf lwgrp-1.0.3.tar.gz cd lwgrp-1.0.3 ./configure --prefix=\\$installdir make install cd .. tar -zxf dtcmp-1.1.1.tar.gz cd dtcmp-1.1.1 ./configure --prefix=\\$installdir --with-lwgrp=\\$installdir make install cd .. tar -zxf libarchive-3.5.1.tar.gz cd libarchive-3.5.1 ./configure --prefix=\\$installdir make install cd .. cd .. # Download mpifileutils and build git clone --depth 1 https://github.com/hpc/mpifileutils mkdir build install cd build cmake ../mpifileutils \\ -DWITH_DTCMP_PREFIX=../install \\ -DWITH_LibCircle_PREFIX=../install \\ -DDTCMP_INCLUDE_DIRS=./install/include \\ -DDTCMP_LIBRARIES=../install/lib64/libdtcmp.so \\ -DLibCircle_INCLUDE_DIRS=../install/include \\ -DLibCircle_LIBRARIES=../install/lib64/libcircle.so \\ -DCMAKE_INSTALL_PREFIX=../install \\ -DWITH_CART_PREFIX=/usr \\ -DWITH_DAOS_PREFIX=/usr \\ -DENABLE_DAOS=ON make install Create two POSIX containers for the mpifilutils test cases daos container create --pool \\$DAOS_POOL --type POSIX DAOS_CONT3=\\<cont uuid> daos container create --pool \\$DAOS_POOL --type POSIX DAOS_CONT4=\\<cont uuid> Run doas copy (dcp) mpirun -hosts \\<hosts> -np 16 --ppn 16 dcp --bufsize 64MB --chunksize 128MB /tmp/daos_dfuse/daos_test daos://\\$DAOS_POOL/\\$DAOS_CONT3 #Sample output [2021-04-29T23:55:52] Walking /tmp/daos_dfuse/daos_test [2021-04-29T23:55:52] Walked 11 items in 0.026 secs (417.452 items/sec) ... [2021-04-29T23:55:52] Walked 11 items in 0.026 seconds (415.641 items/sec) [2021-04-29T23:55:52] Copying to / [2021-04-29T23:55:52] Items: 11 [2021-04-29T23:55:52] Directories: 1 [2021-04-29T23:55:52] Files: 10 [2021-04-29T23:55:52] Links: 0 [2021-04-29T23:55:52] Data: 10.000 GiB (1.000 GiB per file) [2021-04-29T23:55:52] Creating 1 directories [2021-04-29T23:55:52] Creating 10 files. [2021-04-29T23:55:52] Copying data. [2021-04-29T23:56:53] Copied 1.312 GiB (13%) in 61.194 secs (21.963 MiB/s) 405 secs left ... [2021-04-29T23:58:11] Copied 6.000 GiB (60%) in 139.322 secs (44.099 MiB/s) 93 secs left ... [2021-04-29T23:58:11] Copied 10.000 GiB (100%) in 139.322 secs (73.499 MiB/s) done [2021-04-29T23:58:11] Copy data: 10.000 GiB (10737418240 bytes) [2021-04-29T23:58:11] Copy rate: 73.499 MiB/s (10737418240 bytes in 139.322 seconds) [2021-04-29T23:58:11] Syncing data to disk. [2021-04-29T23:58:11] Sync completed in 0.006 seconds. [2021-04-29T23:58:11] Fixing permissions. [2021-04-29T23:58:11] Updated 11 items in 0.002 seconds (4822.579 items/sec) [2021-04-29T23:58:11] Syncing directory updates to disk. [2021-04-29T23:58:11] Sync completed in 0.001 seconds. [2021-04-29T23:58:11] Started: Apr-29-2021,23:55:52 [2021-04-29T23:58:11] Completed: Apr-29-2021,23:58:11 [2021-04-29T23:58:11] Seconds: 139.335 [2021-04-29T23:58:11] Items: 11 [2021-04-29T23:58:11] Directories: 1 [2021-04-29T23:58:11] Files: 10 [2021-04-29T23:58:11] Links: 0 [2021-04-29T23:58:11] Data: 10.000 GiB (10737418240 bytes) [2021-04-29T23:58:11] Rate: 73.492 MiB/s (10737418240 bytes in 139.335 seconds) # Create directory mkdir /tmp/datamover3 #RUN mpirun -hosts wolf-184 --ppn 16 -np 16 dcp --bufsize 64MB --chunksize 128MB daos://\\$DAOS_POOL/\\$DAOS_CONT3 /tmp/datamover3/ # Sample output [2021-04-30T00:02:14] Walking / [2021-04-30T00:02:15] Walked 12 items in 0.112 secs (107.354 items/sec) ... [2021-04-30T00:02:15] Walked 12 items in 0.112 seconds (107.236 items/sec) [2021-04-30T00:02:15] Copying to /tmp/datamover3 [2021-04-30T00:02:15] Items: 12 [2021-04-30T00:02:15] Directories: 2 [2021-04-30T00:02:15] Files: 10 [2021-04-30T00:02:15] Links: 0 [2021-04-30T00:02:15] Data: 10.000 GiB (1.000 GiB per file) [2021-04-30T00:02:15] Creating 2 directories [2021-04-30T00:02:15] Original directory exists, skip the creation: `/tmp/datamover3/\\' (errno=17 File exists) [2021-04-30T00:02:15] Creating 10 files. [2021-04-30T00:02:15] Copying data. [2021-04-30T00:03:15] Copied 1.938 GiB (19%) in 60.341 secs (32.880 MiB/s) 251 secs left ... [2021-04-30T00:03:46] Copied 8.750 GiB (88%) in 91.953 secs (97.441 MiB/s) 13 secs left ... [2021-04-30T00:03:46] Copied 10.000 GiB (100%) in 91.953 secs (111.361 MiB/s) done [2021-04-30T00:03:46] Copy data: 10.000 GiB (10737418240 bytes) [2021-04-30T00:03:46] Copy rate: 111.361 MiB/s (10737418240 bytes in 91.954 seconds) [2021-04-30T00:03:46] Syncing data to disk. [2021-04-30T00:03:47] Sync completed in 0.135 seconds. [2021-04-30T00:03:47] Fixing permissions. [2021-04-30T00:03:47] Updated 12 items in 0.000 seconds (71195.069 items/sec) [2021-04-30T00:03:47] Syncing directory updates to disk. [2021-04-30T00:03:47] Sync completed in 0.001 seconds. [2021-04-30T00:03:47] Started: Apr-30-2021,00:02:15 [2021-04-30T00:03:47] Completed: Apr-30-2021,00:03:47 [2021-04-30T00:03:47] Seconds: 92.091 [2021-04-30T00:03:47] Items: 12 [2021-04-30T00:03:47] Directories: 2 [2021-04-30T00:03:47] Files: 10 [2021-04-30T00:03:47] Links: 0 [2021-04-30T00:03:47] Data: 10.000 GiB (10737418240 bytes) [2021-04-30T00:03:47] Rate: 111.194 MiB/s (10737418240 bytes in 92.091 seconds) # Verify the two directories have the same content mjean\\@wolf-184:\\~/build> ls -la /tmp/datamover3/daos_test/ total 10485808 drwxr-xr-x 2 mjean mjean 4096 Apr 30 00:02 . drwxr-xr-x 3 mjean mjean 4096 Apr 30 00:02 .. -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000009 mjean\\@wolf-184:\\~/build> ls -la /tmp/daos_dfuse/daos_test/ total 10485760 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000009 Clean Up \u00b6 Remove datamover tmp directories rm -rf /tmp/datamover2 rm -rf /tmp/datamover3 Remove dfuse mountpoint: # unmount dfuse pdsh -w \\$CLIENT_NODES \\'fusermount3 -uz /tmp/daos_dfuse/daos_test\\' # remove mount dir pdsh -w \\$CLIENT_NODES rm -rf /tmp/daos_dfuse Destroy Containers : # destroy container1 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT # destroy container2 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT2 # destroy container3 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT3 # destroy container4 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT4 Destroy Pool: # destroy pool dmg pool destroy --pool \\$DAOS_POOL Stop Agents: # stop agents pdsh -S -w \\$CLIENT_NODES \\\"sudo systemctl stop daos_agent\\\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"suseQSG"},{"location":"QSG/suseQSG/#quick-start-opensusesles-152-with-posix","text":"Introduction Requirements Set-Up Run CART selftest Run DAOS PERF* Run DAOS_RACER* Run DAOS_TEST* Create pool and dfuse mountpoint for IOR, MDTEST and DATAMOVER Create pool Create container Set up dfuse mount point: Run IOR Build IOR Run MDTEST Run DBENCH Run DATAMOVER Run DATAMOVER using mpifileutils Build mpifileutils Clean Up","title":"Quick Start openSUSE/SLES 15.2 with POSIX"},{"location":"QSG/suseQSG/#introduction","text":"The purpose of this Quick Start is to provider a user with a set of cmdlines to quickly setup and use DAOS with POSIX on openSUSE/SLES 15.2. This document covers installation of daos rpms on openSUSE/SLES 15.2 and updating the DAOS configuration files needed by daos servers The quick start describes how to use dfuse in order to take advantage of daos support for POSIX. The quick start steps users through running benchmarking tools like ior and mdtest along with some examples of how to move data between a POSIX file system and daos containers (and vise versa) and finally cleaning up your daos setup.","title":"Introduction"},{"location":"QSG/suseQSG/#requirements","text":"The quick start requires a minimum of 1 server with PMEM and SSDs connected via infiniband storage network and 1 client node and 1 admin node without pmem/ssd but on the infiniband storage network. All nodes have a base openSUSE or SLES 15.2 installed. Install pdsh on the admin node sudo zypper install pdsh For example, if one wanted to use node-1 as their admin node, node-2 and node-3 as client nodes, and node-4 and node-5 as their server nodes then these variables would be defined as: ADMIN_NODE=node-1 CLIENT_NODES=node-2, node-3 SERVER_NODES=node-4, node-5 ALL_NODES=\\$ADMIN_NODE,\\$CLIENT_NODES,\\$SERVER_NODES If a client node is also serving as an admin node then exclude \\$ADMIN_NODE from the ALL_NODES assignment to prevent duplication, e.g. ALL_NODES=\\$CLIENT_NODES,\\$SERVER_NODES","title":"Requirements"},{"location":"QSG/suseQSG/#set-up","text":"Please refer here for initial set up which consists of rpm installation, generate and set up certificates, setting up config files, starting servers and agents. For this quick start, the daos-tests package will need to be installed on the client nodes The following applications will be run from a client node: \\\"*\\\" Indicates that the cmdline will run internally only; cmdline should be removed for external customers","title":"Set-Up"},{"location":"QSG/suseQSG/#run-cart-selftest","text":"SHARED_DIR=\\<shared dir by all nodes> export FI_UNIVERSE_SIZE=2048 export OFI_INTERFACE=eth0 # selt_test --help for more details on params #Generate the attach info file (enable SHARED_DIR with perms for sudo to write ) sudo daos_agent -o /etc/daos/daos_agent.yml -l \\$SHARED_DIR/daos_agent.log dump-attachinfo -o \\$SHARED_DIR/daos_server.attach_info_tmp # Run: self_test --path \\$SHARED_DIR --group-name daos_server --endpoint 0-1:0 (for 4 servers --endpoint 0-3:0 ranks:tags) # Sample output: Adding endpoints: ranks: 0-1 (# ranks = 2) tags: 0 (# tags = 1) Warning: No --master-endpoint specified; using this command line application as the master endpoint Self Test Parameters: Group name to test against: daos_server # endpoints: 2 Message sizes: [(200000-BULK_GET 200000-BULK_PUT), (200000-BULK_GET 0-EMPTY), (0-EMPTY 200000-BULK_PUT), (200000-BULK_GET 1000-IOV), (1000-IOV 200000-BULK_PUT), (1000-IOV 1000-IOV), (1000-IOV 0-EMPTY), (0-EMPTY 1000-IOV), (0-EMPTY 0-EMPTY)] Buffer addresses end with: \\<Default> Repetitions per size: 20000 Max inflight RPCs: 1000 CLI [rank=0 pid=3255] Attached daos_server ################################################## Results for message size (200000-BULK_GET 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 222.67 RPC Throughput (RPCs/sec): 584 RPC Latencies (us): Min : 27191 25th %: 940293 Median : 1678137 75th %: 2416765 Max : 3148987 Average: 1671626 Std Dev: 821872.40 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2416764 1:0 - 969063 ################################################## Results for message size (200000-BULK_GET 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.08 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 2880 25th %: 1156162 Median : 1617356 75th %: 2185604 Max : 2730569 Average: 1659133 Std Dev: 605053.68 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2185589 1:0 - 1181363 ################################################## Results for message size (0-EMPTY 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.11 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 4956 25th %: 747786 Median : 1558111 75th %: 2583834 Max : 3437395 Average: 1659959 Std Dev: 1078975.59 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 2583826 1:0 - 776862 ################################################## Results for message size (200000-BULK_GET 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.57 RPC Throughput (RPCs/sec): 587 RPC Latencies (us): Min : 2755 25th %: 12341 Median : 1385716 75th %: 3393178 Max : 3399349 Average: 1660125 Std Dev: 1446054.82 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 12343 1:0 - 3393174 ################################################## Results for message size (1000-IOV 200000-BULK_PUT) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 112.68 RPC Throughput (RPCs/sec): 588 RPC Latencies (us): Min : 4557 25th %: 522380 Median : 1640322 75th %: 2725419 Max : 3441963 Average: 1661254 Std Dev: 1147206.09 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 600190 1:0 - 2725402 ################################################## Results for message size (1000-IOV 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 88.87 RPC Throughput (RPCs/sec): 46595 RPC Latencies (us): Min : 1165 25th %: 21374 Median : 21473 75th %: 21572 Max : 21961 Average: 20923 Std Dev: 2786.99 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 21430 1:0 - 21516 ################################################## Results for message size (1000-IOV 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 59.03 RPC Throughput (RPCs/sec): 61902 RPC Latencies (us): Min : 1164 25th %: 15544 Median : 16104 75th %: 16575 Max : 17237 Average: 15696 Std Dev: 2126.37 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 15579 1:0 - 16571 ################################################## Results for message size (0-EMPTY 1000-IOV) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 46.93 RPC Throughput (RPCs/sec): 49209 RPC Latencies (us): Min : 945 25th %: 20327 Median : 20393 75th %: 20434 Max : 20576 Average: 19821 Std Dev: 2699.27 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 20393 1:0 - 20393 ################################################## Results for message size (0-EMPTY 0-EMPTY) (max_inflight_rpcs = 1000): Master Endpoint 2:0 ------------------- RPC Bandwidth (MB/sec): 0.00 RPC Throughput (RPCs/sec): 65839 RPC Latencies (us): Min : 879 25th %: 14529 Median : 15108 75th %: 15650 Max : 16528 Average: 14765 Std Dev: 2087.87 RPC Failures: 0 Endpoint results (rank:tag - Median Latency (us)): 0:0 - 14569 1:0 - 15649","title":"Run CART selftest"},{"location":"QSG/suseQSG/#run-daos-perf","text":"(requires openmpi3 - libmpi.so .40 ) module load gnu-openmpi/3.1.6 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH export D_LOG_FILE=/tmp/daos_perf.log # Single process daos_perf -a 64 -d 256 -c R2S -P 20G -T daos -s 1k -R \\\"U;pV\\\" -g /etc/daos/daos_control.yml # MPI orterun --enable-recovery -x D_LOG_FILE=/tmp/daos_perf_daos.log --host \\<host name>:4 --map-by node --mca btl_openib_warn_default_gid_prefix \\\"0\\\" --mca btl \\\"tcp,self\\\" --mca oob \\\"tcp\\\" --mca pml \\\"ob1\\\" --mca btl_tcp_if_include \\\"eth0\\\" --np 4 --tag-output /usr/bin/daos_perf -a 64 -d 256 -c R2S -P 20G -T daos -s 1k -R \\\"U;pV\\\" -g /etc/daos/daos_control.yml # Sample Output: Test : DAOS R2S (full stack, 2 replica) Pool : 9c88849b-b0d6-4444-bb39-42769a7a1ef5 Parameters : pool size : SCM: 20480 MB, NVMe: 0 MB credits : -1 (sync I/O for -ve) obj_per_cont : 1 x 1 (procs) dkey_per_obj : 256 akey_per_dkey : 64 recx_per_akey : 16 value type : single stride size : 1024 zero copy : no VOS file : \\<NULL> Running test=UPDATE Running UPDATE test (iteration=1) UPDATE successfully completed: duration : 91.385233 sec bandwith : 2.801 MB/sec rate : 2868.56 IO/sec latency : 348.607 us (nonsense if credits > 1) Duration across processes: MAX duration : 91.385233 sec MIN duration : 91.385233 sec Average duration : 91.385233 sec Completed test=UPDATE","title":"Run DAOS PERF*"},{"location":"QSG/suseQSG/#run-daos_racer","text":"(requires openmpi3 - libmpi.so .40 ) module load gnu-openmpi/3.1.6 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH # RUN: export D_LOG_FILE=/tmp/daos_racer.log export D_LOG_MASK=ERR /usr/bin/daos_racer -n /etc/daos/daos_control.yml NOTE: daos_racer is currently disabled due to DAOS-7359","title":"Run DAOS_RACER*"},{"location":"QSG/suseQSG/#run-daos_test","text":"(requires openmpi3 - libmpi.so .40 ) module load gnu-openmpi/3.1.6 or export LD_LIBRARY_PATH=\\<openmpi lib path>:\\$LD_LIBRARY_PATH export PATH=\\<openmpi bin path>:\\$PATH # RUN: export OFI_INTERFACE=eth0 export POOL_SCM_SIZE=8G export POOL_NVME_SIZE=16G daos_test -pctVAKCoRb -n /etc/daos/daos_control.yml # Sample output from -p (pool test) ================= DAOS pool tests.. ===================== [==========] Running 14 test(s). setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool b91606bb-87dd-4a57-8eee-5d8747a37b31 [ RUN ] POOL1: connect to non-existing pool [ OK ] POOL1: connect to non-existing pool [ RUN ] POOL2: connect/disconnect to pool rank 0 connecting to pool synchronously ... success rank 0 querying pool info... success rank 0 disconnecting from pool synchronously ... success rank 0 success [ OK ] POOL2: connect/disconnect to pool [ RUN ] POOL3: connect/disconnect to pool (async) rank 0 connecting to pool asynchronously ... success rank 0 querying pool info... success rank 0 disconnecting from pool asynchronously ... success rank 0 success [ OK ] POOL3: connect/disconnect to pool (async) [ RUN ] POOL4: pool handle local2global and global2local rank 0 connecting to pool synchronously ... success rank 0 querying pool info... success rank 0 call local2global on pool handlesuccess rank 0 broadcast global pool handle ...success rank 0 disconnecting from pool synchronously ... success rank 0 success [ OK ] POOL4: pool handle local2global and global2local [ RUN ] POOL5: exclusive connection SUBTEST 1: other connections already exist; shall get -1012 establishing a non-exclusive connection trying to establish an exclusive connection disconnecting the non-exclusive connection SUBTEST 2: no other connections; shall succeed establishing an exclusive connection SUBTEST 3: shall prevent other connections (-1012) trying to establish a non-exclusive connection disconnecting the exclusive connection [ OK ] POOL5: exclusive connection [ RUN ] POOL6: exclude targets and query pool info Skip it for now, because CaRT can\\'t support subgroup membership, excluding a node w/o killing it will cause IV issue. [ OK ] POOL6: exclude targets and query pool info [ RUN ] POOL7: set/get/list user-defined pool attributes (sync) setup: connecting to pool connected to pool, ntarget=16 setting pool attributes synchronously ... listing pool attributes synchronously ... Verifying Total Name Length.. Verifying Small Name.. Verifying All Names.. getting pool attributes synchronously ... Verifying Name-Value (A).. Verifying Name-Value (B).. Verifying with NULL buffer.. Deleting all attributes Verifying all attributes deletion [ OK ] POOL7: set/get/list user-defined pool attributes (sync) [ RUN ] POOL8: set/get/list user-defined pool attributes (async) setting pool attributes asynchronously ... listing pool attributes asynchronously ... Verifying Total Name Length.. Verifying Small Name.. Verifying All Names.. getting pool attributes asynchronously ... Verifying Name-Value (A).. Verifying Name-Value (B).. Verifying with NULL buffer.. Deleting all attributes Verifying all attributes deletion [ OK ] POOL8: set/get/list user-defined pool attributes (async) [ RUN ] POOL9: pool reconnect after daos re-init connected to pool, ntarget=16 [ OK ] POOL9: pool reconnect after daos re-init [ RUN ] POOL10: pool create with properties and query create pool with properties, and query it to verify. setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 6b686dce-8cc3-40e6-b706-4c5aedfee4a9 setup: connecting to pool connected to pool, ntarget=16 ACL prop matches expected defaults teardown: destroyed pool 6b686dce-8cc3-40e6-b706-4c5aedfee4a9 [ OK ] POOL10: pool create with properties and query [ RUN ] POOL11: pool list containers (zero) setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool f3bc2fb8-17aa-45cc-9b1a-87ffebddb048 setup: connected to pool: f3bc2fb8-17aa-45cc-9b1a-87ffebddb048 daos_pool_list_cont returned rc=0 success t0: output nconts=0 verifying conts[0..10], nfilled=0 success t1: conts[] over-sized success t2: nconts=0, non-NULL conts[] rc=0 success t3: in &nconts NULL, -DER_INVAL success teardown: destroyed pool f3bc2fb8-17aa-45cc-9b1a-87ffebddb048 [ OK ] POOL11: pool list containers (zero) [ RUN ] POOL12: pool list containers (many) setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool d0fbd1a8-2061-428c-9cfb-376f201689fc setup: connected to pool: d0fbd1a8-2061-428c-9cfb-376f201689fc setup: alloc lcarg->conts len 16 setup: creating container: b52e36e6-26db-4e51-9bbe-df776624ca59 setup: creating container: 645212d3-3fad-446d-be2a-c362b0884555 setup: creating container: 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 setup: creating container: 07cbf32b-beaf-444a-af64-5602aa41485f setup: creating container: 427824a9-a6c7-4417-83c8-1fc2f5e49c83 setup: creating container: 898ffab6-9472-47cc-a6ca-9a6fe54704c0 setup: creating container: d4d3181c-7376-41fa-8f81-fb363b7b8e2e setup: creating container: c1253a14-991d-4e2a-a3cf-ff2e4b160240 setup: creating container: 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 setup: creating container: 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 setup: creating container: 480f2232-58c9-444e-bff5-36e2efa3609d setup: creating container: d33ae63c-eeb4-411f-8ca4-c70eb86371dc setup: creating container: d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb setup: creating container: f789b108-96f1-4052-ad22-c6fb5b02cd19 setup: creating container: 2c83b9d1-a723-49da-912b-63ffe3e43930 setup: creating container: 31e5df3b-809f-47d8-b170-68329b687361 daos_pool_list_cont returned rc=0 success t0: output nconts=16 verifying conts[0..26], nfilled=16 container 645212d3-3fad-446d-be2a-c362b0884555 found in list result container 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 found in list result container d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb found in list result container b52e36e6-26db-4e51-9bbe-df776624ca59 found in list result container 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 found in list result container 2c83b9d1-a723-49da-912b-63ffe3e43930 found in list result container d33ae63c-eeb4-411f-8ca4-c70eb86371dc found in list result container 427824a9-a6c7-4417-83c8-1fc2f5e49c83 found in list result container 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 found in list result container f789b108-96f1-4052-ad22-c6fb5b02cd19 found in list result container 480f2232-58c9-444e-bff5-36e2efa3609d found in list result container 07cbf32b-beaf-444a-af64-5602aa41485f found in list result container 31e5df3b-809f-47d8-b170-68329b687361 found in list result container 898ffab6-9472-47cc-a6ca-9a6fe54704c0 found in list result container c1253a14-991d-4e2a-a3cf-ff2e4b160240 found in list result container d4d3181c-7376-41fa-8f81-fb363b7b8e2e found in list result success t1: conts[] over-sized success t2: nconts=0, non-NULL conts[] rc=0 success t3: in &nconts NULL, -DER_INVAL verifying conts[0..16], nfilled=16 container 645212d3-3fad-446d-be2a-c362b0884555 found in list result container 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 found in list result container d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb found in list result container b52e36e6-26db-4e51-9bbe-df776624ca59 found in list result container 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 found in list result container 2c83b9d1-a723-49da-912b-63ffe3e43930 found in list result container d33ae63c-eeb4-411f-8ca4-c70eb86371dc found in list result container 427824a9-a6c7-4417-83c8-1fc2f5e49c83 found in list result container 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 found in list result container f789b108-96f1-4052-ad22-c6fb5b02cd19 found in list result container 480f2232-58c9-444e-bff5-36e2efa3609d found in list result container 07cbf32b-beaf-444a-af64-5602aa41485f found in list result container 31e5df3b-809f-47d8-b170-68329b687361 found in list result container 898ffab6-9472-47cc-a6ca-9a6fe54704c0 found in list result container c1253a14-991d-4e2a-a3cf-ff2e4b160240 found in list result container d4d3181c-7376-41fa-8f81-fb363b7b8e2e found in list result success t4: conts[] exact length verifying conts[0..15], nfilled=0 success t5: conts[] under-sized success teardown: destroy container: b52e36e6-26db-4e51-9bbe-df776624ca59 teardown: destroy container: 645212d3-3fad-446d-be2a-c362b0884555 teardown: destroy container: 4a2ab45a-2adb-4c1e-8d03-2c7f06ad0721 teardown: destroy container: 07cbf32b-beaf-444a-af64-5602aa41485f teardown: destroy container: 427824a9-a6c7-4417-83c8-1fc2f5e49c83 teardown: destroy container: 898ffab6-9472-47cc-a6ca-9a6fe54704c0 teardown: destroy container: d4d3181c-7376-41fa-8f81-fb363b7b8e2e teardown: destroy container: c1253a14-991d-4e2a-a3cf-ff2e4b160240 teardown: destroy container: 66439972-fd1e-4330-a4b4-e5ee6f6c2b06 teardown: destroy container: 09a4cdca-b4d0-4d3c-9898-75a7a5adbd45 teardown: destroy container: 480f2232-58c9-444e-bff5-36e2efa3609d teardown: destroy container: d33ae63c-eeb4-411f-8ca4-c70eb86371dc teardown: destroy container: d9b3eb0e-7ef3-4e8b-8bd6-8e757a39d0cb teardown: destroy container: f789b108-96f1-4052-ad22-c6fb5b02cd19 teardown: destroy container: 2c83b9d1-a723-49da-912b-63ffe3e43930 teardown: destroy container: 31e5df3b-809f-47d8-b170-68329b687361 teardown: destroyed pool d0fbd1a8-2061-428c-9cfb-376f201689fc [ OK ] POOL12: pool list containers (many) [ RUN ] POOL13: retry POOL_{CONNECT,DISCONNECT,QUERY} setting DAOS_POOL_CONNECT_FAIL_CORPC ... success connecting to pool ... success setting DAOS_POOL_QUERY_FAIL_CORPC ... success querying pool info... success setting DAOS_POOL_DISCONNECT_FAIL_CORPC ... success disconnecting from pool ... success [ OK ] POOL13: retry POOL_{CONNECT,DISCONNECT,QUERY} [ RUN ] POOL14: pool connect access based on ACL pool ACL gives the owner no permissions setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 86a9951c-08f2-4ad0-a4ad-037f8b85a656 setup: connecting to pool daos_pool_connect failed, rc: -1001 failed to connect pool: -1001 pool disconnect failed: -1002 teardown: destroyed pool 86a9951c-08f2-4ad0-a4ad-037f8b85a656 pool ACL gives the owner RO, they want RW setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 15d9c7f9-140e-436d-88d0-2d5924348725 setup: connecting to pool daos_pool_connect failed, rc: -1001 failed to connect pool: -1001 pool disconnect failed: -1002 teardown: destroyed pool 15d9c7f9-140e-436d-88d0-2d5924348725 pool ACL gives the owner RO, they want RO setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool b6ce2f69-94a8-4fa7-8c43-b35766a0fb6f setup: connecting to pool connected to pool, ntarget=16 teardown: destroyed pool b6ce2f69-94a8-4fa7-8c43-b35766a0fb6f pool ACL gives the owner RW, they want RO setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 992015b0-7c70-458e-afa5-eecae4ad5313 setup: connecting to pool connected to pool, ntarget=16 teardown: destroyed pool 992015b0-7c70-458e-afa5-eecae4ad5313 pool ACL gives the owner RW, they want RW setup: creating pool, SCM size=8 GB, NVMe size=16 GB setup: created pool 464bfccf-2ec9-4105-b791-f1163960dc0d setup: connecting to pool connected to pool, ntarget=16 teardown: destroyed pool 464bfccf-2ec9-4105-b791-f1163960dc0d [ OK ] POOL14: pool connect access based on ACL teardown: destroyed pool b91606bb-87dd-4a57-8eee-5d8747a37b31 [==========] 14 test(s) run. [ PASSED ] 14 test(s).","title":"Run DAOS_TEST*"},{"location":"QSG/suseQSG/#create-pool-and-dfuse-mountpoint-for-ior-mdtest-and-datamover","text":"","title":"Create pool and dfuse mountpoint\u00a0 for IOR, MDTEST and DATAMOVER"},{"location":"QSG/suseQSG/#create-pool","text":"dmg pool create --name=daos_test_pool --size=500G # Sample output Creating DAOS pool with automatic storage allocation: 500 GB NVMe + 6.00% SCM Pool created with 6.00% SCM/NVMe ratio --------------------------------------- UUID : acf889b6-f290-4d7b-823a-5fae0014a64d Service Ranks : 0 Storage Ranks : 0 Total Size : 530 GB SCM : 30 GB (30 GB / rank) NVMe : 500 GB (500 GB / rank) dmg pool list # Sample output Pool UUID Svc Replicas -------------- ---------------- acf889b6-f290-4d7b-823a-5fae0014a64d 0 DAOS_POOL=\\<pool uuid> (define on all clients)","title":"Create pool"},{"location":"QSG/suseQSG/#create-container","text":"daos cont create --type=POSIX --oclass=SX --pool=\\$DAOS_POOL DAOS_CONT=\\<cont uuid> (define on all clients)","title":"Create container"},{"location":"QSG/suseQSG/#set-up-dfuse-mount-point","text":"( Run dfuse on all client nodes ) # Create directory mkdir -p /tmp/daos_dfuse/daos_test # Use dfuse to mount the daos container to the above directory dfuse --container \\$DAOS_CONT --disable-direct-io --mountpoint /tmp/daos_dfuse/daos_test --pool \\$DAOS_POOL # verfiy that the file type is dfuse df -h # Sample output dfuse 500G 17G 34G 34% /tmp/daos_dfuse/daos_test","title":"Set up dfuse mount point:"},{"location":"QSG/suseQSG/#run-ior","text":"(uses mpich in the examples)","title":"Run IOR"},{"location":"QSG/suseQSG/#build-ior","text":"( Requires mpich added to PATH and LD_LIBRARY_PATH ) module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Download ior source git clone https://github.com/hpc/ior.git # Build IOR cd ior ./bootstrap mkdir build;cd build MPICC=mpicc ../configure --with-daos=/usr --prefix=\\<your dir> make make install # Add IOR to paths add \\<your dir>/lib to LD_LIBRARY_PATh and \\<your dir>/bin to PATH module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Run: mpirun -np 20 -ppn 10 -hosts host1,host2 ior -a POSIX -b 1G -v -w -W -r -R -k -F -T 10 -i 1 -s 1 -o /tmp/daos_dfuse/daos_test/testfile -t 1G # Sample output IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O Began : Thu Apr 29 16:31:55 2021 Command line : ior -a POSIX -b 1G -v -w -W -r -R -k -F -T 10 -i 1 -s 1 -o /tmp/daos_dfuse/daos_test/testfile -t 1G Machine : Linux wolf-184 Start time skew across all tasks: 0.00 sec TestID : 0 StartTime : Thu Apr 29 16:31:55 2021 Path : /tmp/daos_dfuse/daos_test/testfile.00000000 FS : 493.6 GiB Used FS: 6.6% Inodes: -0.0 Mi Used Inodes: 0.0% Participating tasks : 20 Options: api : POSIX apiVersion : test filename : /tmp/daos_dfuse/daos_test/testfile access : file-per-process type : independent segments : 1 ordering in a file : sequential ordering inter file : no tasks offsets nodes : 2 tasks : 20 clients per node : 10 repetitions : 1 xfersize : 1 GiB blocksize : 1 GiB aggregate filesize : 20 GiB verbose : 1 Results: access bw(MiB/s) IOPS Latency(s) block(KiB) xfer(KiB) open(s) wr/rd(s) close(s) total(s) iter ------ --------- ---- ---------- ---------- --------- -------- -------- -------- -------- ---- Commencing write performance test: Thu Apr 29 16:31:56 2021 write 224.18 0.218924 91.34 1048576 1048576 0.024130 91.36 0.000243 91.36 0 Verifying contents of the file(s) just written. Thu Apr 29 16:33:27 2021 Commencing read performance test: Thu Apr 29 16:34:59 2021 read 223.26 0.218024 91.60 1048576 1048576 0.137707 91.73 0.000087 91.73 0 Max Write: 224.18 MiB/sec (235.07 MB/sec) Max Read: 223.26 MiB/sec (234.10 MB/sec) Summary of all tests: Operation Max(MiB) Min(MiB) Mean(MiB) StdDev Max(OPs) Min(OPs) Mean(OPs) StdDev Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggs(MiB) API RefNum write 224.18 224.18 224.18 0.00 0.22 0.22 0.22 0.00 91.35618 NA NA 0 20 10 1 1 0 1 0 0 1 1073741824 1073741824 20480.0 POSIX 0 read 223.26 223.26 223.26 0.00 0.22 0.22 0.22 0.00 91.73292 NA NA 0 20 10 1 1 0 1 0 0 1 1073741824 1073741824 20480.0 POSIX 0 Finished : Thu Apr 29 16:36:31 2021","title":"Build IOR"},{"location":"QSG/suseQSG/#run-mdtest","text":"module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Create 10000 files # Run: mpirun -np 20 -ppn 10 -hosts host1,host2 mdtest -a POSIX -z 0 -N 1 -P -i 1 -n 500 -e 4096 -d /tmp/daos_dfuse/daos_test -w 4096 #Sample output -- started at 04/29/2021 17:09:02 -- mdtest-3.4.0+dev was launched with 20 total task(s) on 2 node(s) Command line used: mdtest \\'-a\\' \\'POSIX\\' \\'-z\\' \\'0\\' \\'-N\\' \\'1\\' \\'-P\\' \\'-i\\' \\'1\\' \\'-n\\' \\'500\\' \\'-e\\' \\'4096\\' \\'-d\\' \\'/tmp/daos_dfuse/daos_test\\' \\'-w\\' \\'4096\\' Path: /tmp/daos_dfuse FS: 36.5 GiB Used FS: 86.5% Inodes: 2.3 Mi Used Inodes: 9.7% Nodemap: 11111111110000000000 V-0: Rank 0 Line 2216 Shifting ranks by 10 for each phase. 20 tasks, 10000 files/directories SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- --- --- ---- ------- Directory creation : 2249.104 2249.085 2249.094 0.009 Directory stat : 4089.121 4089.116 4089.118 0.001 Directory removal : 318.313 318.311 318.312 0.000 File creation : 1080.348 1080.334 1080.341 0.007 File stat : 1676.635 1676.619 1676.632 0.004 File read : 1486.296 1486.291 1486.295 0.002 File removal : 611.135 611.132 611.133 0.001 Tree creation : 667.967 667.967 667.967 0.000 Tree removal : 18.063 18.063 18.063 0.000 SUMMARY time: (of 1 iterations) Operation Max Min Mean Std Dev --------- --- --- ---- ------- Directory creation : 4.446 4.446 4.446 0.000 Directory stat : 2.446 2.446 2.446 0.000 Directory removal : 31.416 31.416 31.416 0.000 File creation : 9.256 9.256 9.256 0.000 File stat : 5.964 5.964 5.964 0.000 File read : 6.728 6.728 6.728 0.000 File removal : 16.363 16.363 16.363 0.000 Tree creation : 0.001 0.001 0.001 0.000 Tree removal : 0.055 0.055 0.055 0.000 -- finished at 04/29/2021 17:10:19 -- # Create 1000000 files #Run: mpirun -np 20 -ppn 10 -hosts host1,host2 mdtest -a POSIX -z 0 -N 1 -P -i 1 -n 50000 -e 4096 -d /tmp/daos_dfuse/daos_test -w 4096","title":"Run MDTEST"},{"location":"QSG/suseQSG/#run-dbench","text":"module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH # Run: dbench -c /usr/share/dbench/client.txt -t 10 -D /tmp/daos_dfuse/daos_test 10 #Sample output dbench version 3.04 - Copyright Andrew Tridgell 1999-2004 Running for 10 seconds with load \\'/usr/share/dbench/client.txt\\' and minimum warmup 2 secs 10 clients started 10 131 58.09 MB/sec warmup 1 sec 10 401 42.01 MB/sec execute 1 sec 10 538 43.39 MB/sec execute 2 sec 10 682 37.23 MB/sec execute 3 sec 10 803 28.65 MB/sec execute 4 sec 10 898 23.36 MB/sec execute 5 sec 10 980 19.87 MB/sec execute 6 sec 10 1074 18.43 MB/sec execute 7 sec 10 1161 16.25 MB/sec execute 8 sec 10 1240 14.52 MB/sec execute 9 sec 10 1367 15.67 MB/sec cleanup 10 sec 10 1367 14.25 MB/sec cleanup 11 sec 10 1367 14.08 MB/sec cleanup 11 sec Throughput 15.6801 MB/sec 10 procs","title":"Run DBENCH"},{"location":"QSG/suseQSG/#run-datamover","text":"For more details on datamover reference: https://github.com/hpc/mpifileutils/blob/master/DAOS-Support.md # Create a POSIX container daos container create --pool \\$DAOS_POOL --type POSIX DAOS_CONT2=\\<cont uuid> # Copy POSIX directory to POSIX container (only directory copies are supported in 1.2) daos filesystem copy --src /tmp/daos_dfuse/daos_test --dst daos://\\$DAOS_POOL/\\$DAOS_CONT2 # Copy the same POSIX container to a different POSIX directory daos filesystem copy --src daos://\\$DAOS_POOL/\\$DAOS_CONT2 --dst /tmp/datamover2 ls -latr /tmp/datamover2/daos_test/ # Sample output ls -la /tmp/daos_dfuse/daos_test/ total 10485760 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000009 ls -la /tmp/datamover2/daos_test/ total 10485808 drwx------ 2 mjean mjean 4096 Apr 29 17:45 . drwx------ 3 mjean mjean 4096 Apr 29 17:43 .. -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:43 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:45 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:43 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:44 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 17:45 testfile.00000009","title":"Run DATAMOVER"},{"location":"QSG/suseQSG/#run-datamover-using-mpifileutils","text":"","title":"Run DATAMOVER using mpifileutils"},{"location":"QSG/suseQSG/#build-mpifileutils","text":"Mpifileutils can be built using dependency packages or dependencies built from source For more details on mpifileutils reference: https://github.com/hpc/mpifileutils/blob/master/DAOS-Support.md # Install the following packages: zypper install mpich-devel libbz2-devel # Setup environment (on launch node) #Setup mpich env module load gnu-mpich/3.4\\~a2 or export LD_LIBRARY_PATH=\\<mpich lib path>:\\$LD_LIBRARY_PATH export PATH=\\<mpich bin path>:\\$PATH export MPI_HOME=\\<mpich path> Build mpifileutils with dependencies installed from packages # Install build dependencies (on all client nodes) sudo zypper install dtcmp-mpich-devel libcircle-mpich-devel libcap-devel # Build mpifileutils from installed packages (on all client nodes) git clone --depth 1 https://github.com/hpc/mpifileutils mkdir build install cd build cmake ../mpifileutils -DENABLE_DAOS=ON \\ -DENABLE_LIBARCHIVE=OFF \\ -DDTCMP_INCLUDE_DIRS=/usr/lib64/mpi/gcc/mpich/include \\ -DDTCMP_LIBRARIES=/usr/lib64/mpi/gcc/mpich/lib64/libdtcmp.so \\ -DLibCircle_INCLUDE_DIRS=/usr/lib64/mpi/gcc/mpich/include \\ -DLibCircle_LIBRARIES=/usr/lib64/mpi/gcc/mpich/lib64/libcircle.so \\ -DWITH_CART_PREFIX=/usr \\ -DWITH_DAOS_PREFIX=/usr \\ -DCMAKE_INSTALL_INCLUDEDIR=/usr/lib64/mpi/gcc/mpich/include \\ -DCMAKE_INSTALL_PREFIX=/usr/lib64/mpi/gcc/mpich/ \\ -DCMAKE_INSTALL_LIBDIR=/usr/lib64/mpi/gcc/mpich/lib64 sudo make install Build mpifileutils with dependencies that are built from source mkdir install installdir=`pwd`/install export CC=mpicc # download dependencies and build mkdir deps cd deps wget https://github.com/hpc/libcircle/releases/download/v0.3/libcircle-0.3.0.tar.gz wget https://github.com/llnl/lwgrp/releases/download/v1.0.3/lwgrp-1.0.3.tar.gz wget https://github.com/llnl/dtcmp/releases/download/v1.1.1/dtcmp-1.1.1.tar.gz wget https://github.com/libarchive/libarchive/releases/download/3.5.1/libarchive-3.5.1.tar.gz tar -zxf libcircle-0.3.0.tar.gz cd libcircle-0.3.0 ./configure --prefix=\\$installdir make install cd .. tar -zxf lwgrp-1.0.3.tar.gz cd lwgrp-1.0.3 ./configure --prefix=\\$installdir make install cd .. tar -zxf dtcmp-1.1.1.tar.gz cd dtcmp-1.1.1 ./configure --prefix=\\$installdir --with-lwgrp=\\$installdir make install cd .. tar -zxf libarchive-3.5.1.tar.gz cd libarchive-3.5.1 ./configure --prefix=\\$installdir make install cd .. cd .. # Download mpifileutils and build git clone --depth 1 https://github.com/hpc/mpifileutils mkdir build install cd build cmake ../mpifileutils \\ -DWITH_DTCMP_PREFIX=../install \\ -DWITH_LibCircle_PREFIX=../install \\ -DDTCMP_INCLUDE_DIRS=./install/include \\ -DDTCMP_LIBRARIES=../install/lib64/libdtcmp.so \\ -DLibCircle_INCLUDE_DIRS=../install/include \\ -DLibCircle_LIBRARIES=../install/lib64/libcircle.so \\ -DCMAKE_INSTALL_PREFIX=../install \\ -DWITH_CART_PREFIX=/usr \\ -DWITH_DAOS_PREFIX=/usr \\ -DENABLE_DAOS=ON make install Create two POSIX containers for the mpifilutils test cases daos container create --pool \\$DAOS_POOL --type POSIX DAOS_CONT3=\\<cont uuid> daos container create --pool \\$DAOS_POOL --type POSIX DAOS_CONT4=\\<cont uuid> Run doas copy (dcp) mpirun -hosts \\<hosts> -np 16 --ppn 16 dcp --bufsize 64MB --chunksize 128MB /tmp/daos_dfuse/daos_test daos://\\$DAOS_POOL/\\$DAOS_CONT3 #Sample output [2021-04-29T23:55:52] Walking /tmp/daos_dfuse/daos_test [2021-04-29T23:55:52] Walked 11 items in 0.026 secs (417.452 items/sec) ... [2021-04-29T23:55:52] Walked 11 items in 0.026 seconds (415.641 items/sec) [2021-04-29T23:55:52] Copying to / [2021-04-29T23:55:52] Items: 11 [2021-04-29T23:55:52] Directories: 1 [2021-04-29T23:55:52] Files: 10 [2021-04-29T23:55:52] Links: 0 [2021-04-29T23:55:52] Data: 10.000 GiB (1.000 GiB per file) [2021-04-29T23:55:52] Creating 1 directories [2021-04-29T23:55:52] Creating 10 files. [2021-04-29T23:55:52] Copying data. [2021-04-29T23:56:53] Copied 1.312 GiB (13%) in 61.194 secs (21.963 MiB/s) 405 secs left ... [2021-04-29T23:58:11] Copied 6.000 GiB (60%) in 139.322 secs (44.099 MiB/s) 93 secs left ... [2021-04-29T23:58:11] Copied 10.000 GiB (100%) in 139.322 secs (73.499 MiB/s) done [2021-04-29T23:58:11] Copy data: 10.000 GiB (10737418240 bytes) [2021-04-29T23:58:11] Copy rate: 73.499 MiB/s (10737418240 bytes in 139.322 seconds) [2021-04-29T23:58:11] Syncing data to disk. [2021-04-29T23:58:11] Sync completed in 0.006 seconds. [2021-04-29T23:58:11] Fixing permissions. [2021-04-29T23:58:11] Updated 11 items in 0.002 seconds (4822.579 items/sec) [2021-04-29T23:58:11] Syncing directory updates to disk. [2021-04-29T23:58:11] Sync completed in 0.001 seconds. [2021-04-29T23:58:11] Started: Apr-29-2021,23:55:52 [2021-04-29T23:58:11] Completed: Apr-29-2021,23:58:11 [2021-04-29T23:58:11] Seconds: 139.335 [2021-04-29T23:58:11] Items: 11 [2021-04-29T23:58:11] Directories: 1 [2021-04-29T23:58:11] Files: 10 [2021-04-29T23:58:11] Links: 0 [2021-04-29T23:58:11] Data: 10.000 GiB (10737418240 bytes) [2021-04-29T23:58:11] Rate: 73.492 MiB/s (10737418240 bytes in 139.335 seconds) # Create directory mkdir /tmp/datamover3 #RUN mpirun -hosts wolf-184 --ppn 16 -np 16 dcp --bufsize 64MB --chunksize 128MB daos://\\$DAOS_POOL/\\$DAOS_CONT3 /tmp/datamover3/ # Sample output [2021-04-30T00:02:14] Walking / [2021-04-30T00:02:15] Walked 12 items in 0.112 secs (107.354 items/sec) ... [2021-04-30T00:02:15] Walked 12 items in 0.112 seconds (107.236 items/sec) [2021-04-30T00:02:15] Copying to /tmp/datamover3 [2021-04-30T00:02:15] Items: 12 [2021-04-30T00:02:15] Directories: 2 [2021-04-30T00:02:15] Files: 10 [2021-04-30T00:02:15] Links: 0 [2021-04-30T00:02:15] Data: 10.000 GiB (1.000 GiB per file) [2021-04-30T00:02:15] Creating 2 directories [2021-04-30T00:02:15] Original directory exists, skip the creation: `/tmp/datamover3/\\' (errno=17 File exists) [2021-04-30T00:02:15] Creating 10 files. [2021-04-30T00:02:15] Copying data. [2021-04-30T00:03:15] Copied 1.938 GiB (19%) in 60.341 secs (32.880 MiB/s) 251 secs left ... [2021-04-30T00:03:46] Copied 8.750 GiB (88%) in 91.953 secs (97.441 MiB/s) 13 secs left ... [2021-04-30T00:03:46] Copied 10.000 GiB (100%) in 91.953 secs (111.361 MiB/s) done [2021-04-30T00:03:46] Copy data: 10.000 GiB (10737418240 bytes) [2021-04-30T00:03:46] Copy rate: 111.361 MiB/s (10737418240 bytes in 91.954 seconds) [2021-04-30T00:03:46] Syncing data to disk. [2021-04-30T00:03:47] Sync completed in 0.135 seconds. [2021-04-30T00:03:47] Fixing permissions. [2021-04-30T00:03:47] Updated 12 items in 0.000 seconds (71195.069 items/sec) [2021-04-30T00:03:47] Syncing directory updates to disk. [2021-04-30T00:03:47] Sync completed in 0.001 seconds. [2021-04-30T00:03:47] Started: Apr-30-2021,00:02:15 [2021-04-30T00:03:47] Completed: Apr-30-2021,00:03:47 [2021-04-30T00:03:47] Seconds: 92.091 [2021-04-30T00:03:47] Items: 12 [2021-04-30T00:03:47] Directories: 2 [2021-04-30T00:03:47] Files: 10 [2021-04-30T00:03:47] Links: 0 [2021-04-30T00:03:47] Data: 10.000 GiB (10737418240 bytes) [2021-04-30T00:03:47] Rate: 111.194 MiB/s (10737418240 bytes in 92.091 seconds) # Verify the two directories have the same content mjean\\@wolf-184:\\~/build> ls -la /tmp/datamover3/daos_test/ total 10485808 drwxr-xr-x 2 mjean mjean 4096 Apr 30 00:02 . drwxr-xr-x 3 mjean mjean 4096 Apr 30 00:02 .. -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 30 00:03 testfile.00000009 mjean\\@wolf-184:\\~/build> ls -la /tmp/daos_dfuse/daos_test/ total 10485760 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000000 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000001 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000002 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000003 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000004 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000005 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000006 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000007 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000008 -rw-r--r-- 1 mjean mjean 1073741824 Apr 29 16:31 testfile.00000009","title":"Build mpifileutils"},{"location":"QSG/suseQSG/#clean-up","text":"Remove datamover tmp directories rm -rf /tmp/datamover2 rm -rf /tmp/datamover3 Remove dfuse mountpoint: # unmount dfuse pdsh -w \\$CLIENT_NODES \\'fusermount3 -uz /tmp/daos_dfuse/daos_test\\' # remove mount dir pdsh -w \\$CLIENT_NODES rm -rf /tmp/daos_dfuse Destroy Containers : # destroy container1 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT # destroy container2 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT2 # destroy container3 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT3 # destroy container4 daos container destroy --pool \\$DAOS_POOL --cont \\$DAOS_CONT4 Destroy Pool: # destroy pool dmg pool destroy --pool \\$DAOS_POOL Stop Agents: # stop agents pdsh -S -w \\$CLIENT_NODES \\\"sudo systemctl stop daos_agent\\\" Stop Servers: # stop servers pdsh -S -w \\$SERVER_NODES \\\"sudo systemctl stop daos_server\\\"","title":"Clean Up"},{"location":"QSG/suse_setup/","text":"DAOS Set-Up on OpenSUSE \u00b6 Introduction \u00b6 The following instructions detail how to install, set up and start DAOS servers and clients on two or more nodes. This document includes instructions for OpenSUSE. For setup instructions on CentOS, refer to the CentOS setup . For more details reference the DAOS administration guide: https://daos-stack.github.io/admin/hardware/ Requirements \u00b6 The following steps require two or more hosts which will be divided up into admin, client, and server roles. One node can be used for both the admin and client node. All nodes must have: sudo access configured password-less ssh configured pdsh installed (or some other means of running multiple remote commands in parallel) In addition the server nodes should also have: an InfiniBand network adapter configured one or more NVMe devices IOMMU is enabled https://daos-stack.github.io/admin/predeployment_check/#enable-iommu-optional For the use of the commands outlined on this page the following shell variables will need to be defined: ADMIN_NODE CLIENT_NODES SERVER_NODES ALL_NODES For example, if one wanted to use node-1 as their admin node, node-2 and node-3 as client nodes, and node-[4-6] as their server nodes then these variables would be defined as: ADMIN\\_NODE=node-1 CLIENT\\_NODES=node-2,node-3 SERVER\\_NODES=node-4,node-5,node-6 ALL\\_NODES=\\$ADMIN\\_NODE,\\$CLIENT\\_NODES,\\$SERVER\\_NODES Note If a client node is also serving as an admin node, exclude \\$ADMIN\\_NODE from the ALL\\_NODES assignment to prevent duplication. For example: ALL\\_NODES=\\$CLIENT\\_NODES,\\$SERVER\\_NODES RPM Installation \u00b6 In this section the required RPMs will be installed on each of the nodes based upon their role. Admin and client nodes require the installation of the daos-client RPM and the server nodes require the installation of the daos-server RPM. Configure access to the DAOS package repository at https://packages.daos.io/v1.2 . pdsh -w $ALL_NODES 'sudo zypper ar https://packages.daos.io/v1.2/Leap15/packages/x86_64/ daos_packages' Import GPG key on all nodes: pdsh -w \\$ALL\\_NODES \\'sudo rpm \\--import https://packages.daos.io/RPM-GPG-KEY\\ Perform the additional steps: pdsh -w $ALL_NODES 'sudo zypper --non-interactive refresh' Install the DAOS server RPMs on the server nodes: pdsh -w $SERVER_NODES 'sudo zypper install -y daos-server' Install the DAOS client RPMs on the client and admin nodes: pdsh -w $ALL_NODES -x $SERVER_NODES 'sudo zypper install -y daos-client' (Optionally) Install the DAOS test RPMs on the client nodes - typically not required pdsh -w $ALL_NODES -x $SERVER_NODES 'sudo zypper install -y daos-tests' Hardware Provisioning \u00b6 In this section, PMem (Intel(R) Optane(TM) persistent memory) and NVME SSDs will be prepared and configured to be used by DAOS. Note PMem preparation is required once per DAOS installation. Note For OpenSUSE 15.2 installation, update ipmctl to the latest package available from https://build.opensuse.org/package/binaries/hardware:nvdimm/ipmctl/openSUSE_Leap_15.2 Prepare the pmem devices on Server nodes: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. Please ensure namespaces are unmounted and locally attached SCM & NVMe devices are not in use. Please be patient as it may take several minutes and subsequent reboot maybe required. Are you sure you want to continue? (yes/no) yes A reboot is required to process new SCM memory allocation goals. Reboot the server node. Run the prepare cmdline again: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... SCM namespaces: SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 0 3.2 TB Prepare the NVME devices on Server nodes: daos\\_server storage prepare \\--nvme-only -u root Preparing locally-attached NVMe storage\\... Scan the available storage on the Server nodes: daos\\_server storage scan Scanning locally-attached storage\\... NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:5e:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:5f:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:81:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB 0000:da:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 1 3.2 TB Generate certificates \u00b6 In this section certificates will be generated and installed for encrypting the DAOS control plane communications. Administrative nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the current user Admin certificate (admin.crt) owned by the current user Admin key (admin.key) owned by the current user Client nodes require the following certificate files: CA root certificate (daosCa.crt) owned by the current user Agent certificate (agent.crt) owned by the daos_agent user Agent key (agent.key) owned by the daos_agent user Server nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the daos_server user Server certificate (server.crt) owned by the daos_server user Server key (server.key) owned by the daos_server user A copy of the Client certificate (client.crt) owned by the daos_server user See https://daos-stack.github.io/admin/deployment/#certificate-configuration for more informaation. Note The following commands are run from the \\$ADMIN\\_NODE . Generate a new set of certificates: cd /tmp /usr/lib64/daos/certgen/gen\\_certificates.sh Note These files should be protected from unauthorized access and preserved for future use. Copy the certificates to a common location on each node in order to move them to the final location: pdsh -S -w \\$ALL\\_NODES -x \\$(hostname -s) scp -r \\$(hostname -s):/tmp/daosCA /tmp Copy the certificates to their default location (/etc/daos) on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the admin nodes then use the following command to create it: pdsh -S -w \\$ADMIN\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the client nodes, use the following command to create it: pdsh -S -w \\$CLIENT\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each server node: pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.key /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/clients/agent.crt Set the ownership of the admin certificates on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/admin.\\* Set the ownership of the client certificates on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$CLIENT\\_NODES sudo chown daos\\_agent:daos\\_agent /etc/daos/certs/agent.\\* Set the ownership of the server certificates on each server node: pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/daosCA.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/server.\\* pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients/agent.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients Create Configuration Files \u00b6 In this section the daos\\_server , daos\\_agent , and dmg command configuration files will be defined. Examples are available at https://github.com/daos-stack/daos/tree/release/1.2/utils/config/examples Determine the addresses for the NVMe devices on the server nodes: pdsh -S -w \\$SERVER\\_NODES sudo lspci \\| grep -i nvme Note Save the addresses of the NVMe devices to use with each DAOS server, e.g. \\\"81:00.0\\\", from each server node. This information will be used to populate the \\\"bdev_list\\\" server configuration parameter below. Create a server configuration file by modifying the default /etc/daos/daos\\_server.yml file on the server nodes. An example of the daos_server.yml is presented below. Copy the modified server yaml file to all the server nodes at `/etc/daos/daos_server.yml. More details on configuring the daos_server.yml file are available at Server configuration file details . name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false client_cert_dir: /etc/daos/certs/clients ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/server.crt key: /etc/daos/certs/server.key provider: ofi+verbs;ofi_rxm socket_dir: /var/run/daos_server nr_hugepages: 4096 control_log_mask: DEBUG control_log_file: /tmp/daos_server.log helper_log_file: /tmp/daos_admin.log engines: - targets: 8 nr_xs_helpers: 0 fabric_iface: ib0 fabric_iface_port: 31316 log_mask: INFO log_file: /tmp/daos_engine_0.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos0 scm_class: dcpm scm_list: [/dev/pmem0] bdev_class: nvme bdev_list: [\"0000:81:00.0\"] # generate regular nvme.conf - targets: 8 nr_xs_helpers: 0 fabric_iface: ib1 fabric_iface_port: 31416 log_mask: INFO log_file: /tmp/daos_engine_1.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos1 scm_class: dcpm scm_list: [/dev/pmem1] bdev_class: nvme bdev_list: [\"0000:83:00.0\"] # generate regular nvme.conf Copy the modified server yaml file to all the server nodes at /etc/daos/daos_server.yml . Create an agent configuration file by modifying the default /etc/daos/daos_agent.yml file on the client nodes. The following is an example daos_agent.yml. Copy the modified agent yaml file to all the client nodes at /etc/daos/daos_agent.yml . More details on configuring the daos_agent.yml file are available at Agent configuration file details name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/agent.crt key: /etc/daos/certs/agent.key runtime_dir: /var/run/daos_agent log_file: /tmp/daos_agent.log Create a dmg configuration file by modifying the default /etc/daos/daos_control.yml file on the admin node. The following is an example of the daos_control.yml . More details on configuring the daos_control.yml file are available in the DMG configuration file details . name: daos_server port: 10001 hostlist: ['node-4', 'node-5', 'node-6'] transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/admin.crt key: /etc/daos/certs/admin.key Start the DAOS Servers \u00b6 Start daos engines on server nodes: pdsh -S -w $SERVER_NODES \"sudo systemctl daemon-reload\" pdsh -S -w $SERVER_NODES \"sudo systemctl start daos_server\" Check status and format storage: # check status pdsh -S -w $SERVER_NODES \"sudo systemctl status daos_server\" # if you see following format messages (depending on number of servers), proceed to storage format wolf-179: May 05 22:21:03 wolf-179.wolf.hpdd.intel.com daos_server[37431]: Metadata format required on instance 0 # format storage dmg storage format -l $SERVER_NODES --reformat Verify that all servers have started: # system query from ADMIN_NODE dmg system query -v # all the server ranks should show 'Joined' STATE Rank UUID Control Address Fault Domain State Reason ---- ---- --------------- ------------ ----- ------ 0 604c4ffa-563a-49dc-b702-3c87293dbcf3 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 1 f0791f98-4379-4ace-a083-6ca3ffa65756 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 2 745d2a5b-46dd-42c5-b90a-d2e46e178b3e 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined 3 ba6a7800-3952-46ce-af92-bba9daa35048 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined Start the DAOS Agents \u00b6 Start the daos agents on the client nodes: # start agents pdsh -S -w $CLIENT_NODES \"sudo systemctl start daos_agent\" (Optional) Check daos_agent status: # check status pdsh -S -w $CLIENT_NODES \"cat /tmp/daos_agent.log\" # Sample output depending on number of client nodes node-2: agent INFO 2021/05/05 22:38:46 DAOS Agent v1.2 (pid 47580) listening on /var/run/daos_agent/daos_agent.sock node-3: agent INFO 2021/05/05 22:38:53 DAOS Agent v1.2 (pid 39135) listening on /var/run/daos_agent/daos_agent.sock","title":"DAOS openSUSE Setup"},{"location":"QSG/suse_setup/#daos-set-up-on-opensuse","text":"","title":"DAOS Set-Up on OpenSUSE"},{"location":"QSG/suse_setup/#introduction","text":"The following instructions detail how to install, set up and start DAOS servers and clients on two or more nodes. This document includes instructions for OpenSUSE. For setup instructions on CentOS, refer to the CentOS setup . For more details reference the DAOS administration guide: https://daos-stack.github.io/admin/hardware/","title":"Introduction"},{"location":"QSG/suse_setup/#requirements","text":"The following steps require two or more hosts which will be divided up into admin, client, and server roles. One node can be used for both the admin and client node. All nodes must have: sudo access configured password-less ssh configured pdsh installed (or some other means of running multiple remote commands in parallel) In addition the server nodes should also have: an InfiniBand network adapter configured one or more NVMe devices IOMMU is enabled https://daos-stack.github.io/admin/predeployment_check/#enable-iommu-optional For the use of the commands outlined on this page the following shell variables will need to be defined: ADMIN_NODE CLIENT_NODES SERVER_NODES ALL_NODES For example, if one wanted to use node-1 as their admin node, node-2 and node-3 as client nodes, and node-[4-6] as their server nodes then these variables would be defined as: ADMIN\\_NODE=node-1 CLIENT\\_NODES=node-2,node-3 SERVER\\_NODES=node-4,node-5,node-6 ALL\\_NODES=\\$ADMIN\\_NODE,\\$CLIENT\\_NODES,\\$SERVER\\_NODES Note If a client node is also serving as an admin node, exclude \\$ADMIN\\_NODE from the ALL\\_NODES assignment to prevent duplication. For example: ALL\\_NODES=\\$CLIENT\\_NODES,\\$SERVER\\_NODES","title":"Requirements"},{"location":"QSG/suse_setup/#rpm-installation","text":"In this section the required RPMs will be installed on each of the nodes based upon their role. Admin and client nodes require the installation of the daos-client RPM and the server nodes require the installation of the daos-server RPM. Configure access to the DAOS package repository at https://packages.daos.io/v1.2 . pdsh -w $ALL_NODES 'sudo zypper ar https://packages.daos.io/v1.2/Leap15/packages/x86_64/ daos_packages' Import GPG key on all nodes: pdsh -w \\$ALL\\_NODES \\'sudo rpm \\--import https://packages.daos.io/RPM-GPG-KEY\\ Perform the additional steps: pdsh -w $ALL_NODES 'sudo zypper --non-interactive refresh' Install the DAOS server RPMs on the server nodes: pdsh -w $SERVER_NODES 'sudo zypper install -y daos-server' Install the DAOS client RPMs on the client and admin nodes: pdsh -w $ALL_NODES -x $SERVER_NODES 'sudo zypper install -y daos-client' (Optionally) Install the DAOS test RPMs on the client nodes - typically not required pdsh -w $ALL_NODES -x $SERVER_NODES 'sudo zypper install -y daos-tests'","title":"RPM Installation"},{"location":"QSG/suse_setup/#hardware-provisioning","text":"In this section, PMem (Intel(R) Optane(TM) persistent memory) and NVME SSDs will be prepared and configured to be used by DAOS. Note PMem preparation is required once per DAOS installation. Note For OpenSUSE 15.2 installation, update ipmctl to the latest package available from https://build.opensuse.org/package/binaries/hardware:nvdimm/ipmctl/openSUSE_Leap_15.2 Prepare the pmem devices on Server nodes: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... Memory allocation goals for SCM will be changed and namespaces modified, this will be a destructive operation. Please ensure namespaces are unmounted and locally attached SCM & NVMe devices are not in use. Please be patient as it may take several minutes and subsequent reboot maybe required. Are you sure you want to continue? (yes/no) yes A reboot is required to process new SCM memory allocation goals. Reboot the server node. Run the prepare cmdline again: daos\\_server storage prepare \\--scm-only Sample Script: Preparing locally-attached SCM\\... SCM namespaces: SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 0 3.2 TB Prepare the NVME devices on Server nodes: daos\\_server storage prepare \\--nvme-only -u root Preparing locally-attached NVMe storage\\... Scan the available storage on the Server nodes: daos\\_server storage scan Scanning locally-attached storage\\... NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:5e:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:5f:00.0 INTEL SSDPE2KE016T8 VDV10170 0 1.6 TB 0000:81:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB 0000:da:00.0 INTEL SSDPED1K750GA E2010475 1 750 GB SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 1 3.2 TB","title":"Hardware Provisioning"},{"location":"QSG/suse_setup/#generate-certificates","text":"In this section certificates will be generated and installed for encrypting the DAOS control plane communications. Administrative nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the current user Admin certificate (admin.crt) owned by the current user Admin key (admin.key) owned by the current user Client nodes require the following certificate files: CA root certificate (daosCa.crt) owned by the current user Agent certificate (agent.crt) owned by the daos_agent user Agent key (agent.key) owned by the daos_agent user Server nodes require the following certificate files: CA root certificate (daosCA.crt) owned by the daos_server user Server certificate (server.crt) owned by the daos_server user Server key (server.key) owned by the daos_server user A copy of the Client certificate (client.crt) owned by the daos_server user See https://daos-stack.github.io/admin/deployment/#certificate-configuration for more informaation. Note The following commands are run from the \\$ADMIN\\_NODE . Generate a new set of certificates: cd /tmp /usr/lib64/daos/certgen/gen\\_certificates.sh Note These files should be protected from unauthorized access and preserved for future use. Copy the certificates to a common location on each node in order to move them to the final location: pdsh -S -w \\$ALL\\_NODES -x \\$(hostname -s) scp -r \\$(hostname -s):/tmp/daosCA /tmp Copy the certificates to their default location (/etc/daos) on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.crt /etc/daos/certs/. pdsh -S -w \\$ADMIN\\_NODE sudo cp /tmp/daosCA/certs/admin.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the admin nodes then use the following command to create it: pdsh -S -w \\$ADMIN\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/. pdsh -S -w \\$CLIENT\\_NODES sudo cp /tmp/daosCA/certs/agent.key /etc/daos/certs/. Note If the /etc/daos/certs directory does not exist on the client nodes, use the following command to create it: pdsh -S -w \\$CLIENT\\_NODES sudo mkdir /etc/daos/certs Copy the certificates to their default location (/etc/daos) on each server node: pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/daosCA.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.crt /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/server.key /etc/daos/certs/. pdsh -S -w \\$SERVER\\_NODES sudo cp /tmp/daosCA/certs/agent.crt /etc/daos/certs/clients/agent.crt Set the ownership of the admin certificates on each admin node: pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$ADMIN\\_NODE sudo chown \\$USER:\\$USER /etc/daos/certs/admin.\\* Set the ownership of the client certificates on each client node: pdsh -S -w \\$CLIENT\\_NODES sudo chown \\$USER:\\$USER /etc/daos/certs/daosCA.crt pdsh -S -w \\$CLIENT\\_NODES sudo chown daos\\_agent:daos\\_agent /etc/daos/certs/agent.\\* Set the ownership of the server certificates on each server node: pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/daosCA.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/server.\\* pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients/agent.crt pdsh -S -w \\$SERVER\\_NODES sudo chown daos\\_server:daos\\_server /etc/daos/certs/clients","title":"Generate certificates"},{"location":"QSG/suse_setup/#create-configuration-files","text":"In this section the daos\\_server , daos\\_agent , and dmg command configuration files will be defined. Examples are available at https://github.com/daos-stack/daos/tree/release/1.2/utils/config/examples Determine the addresses for the NVMe devices on the server nodes: pdsh -S -w \\$SERVER\\_NODES sudo lspci \\| grep -i nvme Note Save the addresses of the NVMe devices to use with each DAOS server, e.g. \\\"81:00.0\\\", from each server node. This information will be used to populate the \\\"bdev_list\\\" server configuration parameter below. Create a server configuration file by modifying the default /etc/daos/daos\\_server.yml file on the server nodes. An example of the daos_server.yml is presented below. Copy the modified server yaml file to all the server nodes at `/etc/daos/daos_server.yml. More details on configuring the daos_server.yml file are available at Server configuration file details . name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false client_cert_dir: /etc/daos/certs/clients ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/server.crt key: /etc/daos/certs/server.key provider: ofi+verbs;ofi_rxm socket_dir: /var/run/daos_server nr_hugepages: 4096 control_log_mask: DEBUG control_log_file: /tmp/daos_server.log helper_log_file: /tmp/daos_admin.log engines: - targets: 8 nr_xs_helpers: 0 fabric_iface: ib0 fabric_iface_port: 31316 log_mask: INFO log_file: /tmp/daos_engine_0.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos0 scm_class: dcpm scm_list: [/dev/pmem0] bdev_class: nvme bdev_list: [\"0000:81:00.0\"] # generate regular nvme.conf - targets: 8 nr_xs_helpers: 0 fabric_iface: ib1 fabric_iface_port: 31416 log_mask: INFO log_file: /tmp/daos_engine_1.log env_vars: - CRT_TIMEOUT=30 scm_mount: /mnt/daos1 scm_class: dcpm scm_list: [/dev/pmem1] bdev_class: nvme bdev_list: [\"0000:83:00.0\"] # generate regular nvme.conf Copy the modified server yaml file to all the server nodes at /etc/daos/daos_server.yml . Create an agent configuration file by modifying the default /etc/daos/daos_agent.yml file on the client nodes. The following is an example daos_agent.yml. Copy the modified agent yaml file to all the client nodes at /etc/daos/daos_agent.yml . More details on configuring the daos_agent.yml file are available at Agent configuration file details name: daos_server access_points: ['node-4'] port: 10001 transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/agent.crt key: /etc/daos/certs/agent.key runtime_dir: /var/run/daos_agent log_file: /tmp/daos_agent.log Create a dmg configuration file by modifying the default /etc/daos/daos_control.yml file on the admin node. The following is an example of the daos_control.yml . More details on configuring the daos_control.yml file are available in the DMG configuration file details . name: daos_server port: 10001 hostlist: ['node-4', 'node-5', 'node-6'] transport_config: allow_insecure: false ca_cert: /etc/daos/certs/daosCA.crt cert: /etc/daos/certs/admin.crt key: /etc/daos/certs/admin.key","title":"Create Configuration Files"},{"location":"QSG/suse_setup/#start-the-daos-servers","text":"Start daos engines on server nodes: pdsh -S -w $SERVER_NODES \"sudo systemctl daemon-reload\" pdsh -S -w $SERVER_NODES \"sudo systemctl start daos_server\" Check status and format storage: # check status pdsh -S -w $SERVER_NODES \"sudo systemctl status daos_server\" # if you see following format messages (depending on number of servers), proceed to storage format wolf-179: May 05 22:21:03 wolf-179.wolf.hpdd.intel.com daos_server[37431]: Metadata format required on instance 0 # format storage dmg storage format -l $SERVER_NODES --reformat Verify that all servers have started: # system query from ADMIN_NODE dmg system query -v # all the server ranks should show 'Joined' STATE Rank UUID Control Address Fault Domain State Reason ---- ---- --------------- ------------ ----- ------ 0 604c4ffa-563a-49dc-b702-3c87293dbcf3 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 1 f0791f98-4379-4ace-a083-6ca3ffa65756 10.8.1.179:10001 /wolf-179.wolf.hpdd.intel.com Joined 2 745d2a5b-46dd-42c5-b90a-d2e46e178b3e 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined 3 ba6a7800-3952-46ce-af92-bba9daa35048 10.8.1.189:10001 /wolf-189.wolf.hpdd.intel.com Joined","title":"Start the DAOS Servers"},{"location":"QSG/suse_setup/#start-the-daos-agents","text":"Start the daos agents on the client nodes: # start agents pdsh -S -w $CLIENT_NODES \"sudo systemctl start daos_agent\" (Optional) Check daos_agent status: # check status pdsh -S -w $CLIENT_NODES \"cat /tmp/daos_agent.log\" # Sample output depending on number of client nodes node-2: agent INFO 2021/05/05 22:38:46 DAOS Agent v1.2 (pid 47580) listening on /var/run/daos_agent/daos_agent.sock node-3: agent INFO 2021/05/05 22:38:53 DAOS Agent v1.2 (pid 39135) listening on /var/run/daos_agent/daos_agent.sock","title":"Start the DAOS Agents"},{"location":"QSG/test_index/","text":"DAOS Tests \u00b6 The following test guides are available to verify and benchmark your DAOS installation: Run DAOS Autotest Run CaRT Self_test Run IOR Run mdtest Run dbench Datamover test","title":"DAOS Test and Benchmarking"},{"location":"QSG/test_index/#daos-tests","text":"The following test guides are available to verify and benchmark your DAOS installation: Run DAOS Autotest Run CaRT Self_test Run IOR Run mdtest Run dbench Datamover test","title":"DAOS Tests"},{"location":"QSG/tour/","text":"DAOS Tour \u00b6 Introduction \u00b6 This documentation provides a general tour to the DAOS management commands (dmg) for daos_admin, and DAOS tools (daos) for daos_client users. Help and setup for the following is provided in this chapter: Pool and Container create, list, query and destroy on DAOS server for daos_admin and daos_client users. Common errors and workarounds for new users when using the dmg and daos tools. Example runs of data transfer between DAOS file systems, by setting up of the DAOS dfuse mount point and run traffic with dfuse fio and mpirun mdtest. Examples of basic dmg and daos tools run on 2 host DAOS servers and 1 host client, and runs of DAOS rebuild over dfuse fio and mpirun mdtest on a 4 host DAOS server. Requirements \u00b6 Set environment variables for list of servers, client and admin node. # Example of 2 hosts server # For 1 host server, export SERVER\\_NODES=node-1 export SERVER\\_NODES=node-1,node-2 # Example to use admin and client on the same node export ADMIN\\_NODE=node-3 export CLIENT\\_NODE=node-3 export ALL\\_NODES=\\$SERVER\\_NODES,\\$CLIENT\\_NODE Set-Up \u00b6 Refer to the DAOS CentOS Setup or the DAOS openSUSE Setup for RPM installation, daos server/agent/admin configuration yml files, certificate generation, and bring-up DAOS servers and clients. DAOS management tool (dmg) usage for daos_admin \u00b6 dmg tool help \u00b6 # DAOS management tool full path /usr/bin/dmg dmg \\--help Usage: dmg \\[OPTIONS\\] \\<command\\> Application Options: --allow-proxy Allow proxy configuration via environment -l, --host-list= comma separated list of addresses <ipv4addr/hostname\\> -i, --insecure have dmg attempt to connect without certificates -d, --debug enable debug output -j, --json Enable JSON output -J, --json-logging Enable JSON-formatted log output -o, --config-path= Client config file path Help Options: -h, --help Show this help message Available commands: config Perform tasks related to configuration of hardware remote servers (aliases: co) cont Perform tasks related to DAOS containers (aliases: c) network Perform tasks related to network devices attached to remote servers (aliases: n) pool Perform tasks related to DAOS pools (aliases: p) storage Perform tasks related to storage attached to remote servers (aliases: st) system Perform distributed tasks related to DAOS system (aliases: sy) telemetry Perform telemetry operations version Print dmg version dmg system query \u00b6 # system query output for a 2 hosts DAOS server dmg system query Rank State ---- ---- [0-1] Joined dmg system query verbose output \u00b6 dmg system query \\--verbose (-v) Rank UUID Control Address Fault Domain State Reason ---- ---- --------------- ------------ ----- ------ 0 570660ae-1727-4ce0-9650-6c31e81c9d30 10.7.1.8:10001 /boro-8.boro.hpdd.intel.com Joined 1 74390dd0-7fbc-4309-8665-d5f24218c8d9 10.7.1.35:10001 /boro-35.boro.hpdd.intel.com Joined dmg system query with debug \u00b6 dmg system query \\--debug (-d) DEBUG 21:17:29.765815 main.go:216: debug output enabled DEBUG 21:17:29.766483 main.go:243: control config loaded from /etc/daos/daos\\_control.yml DEBUG 21:17:29.768661 system.go:368: DAOS system query request: &{unaryRequest:{request:{deadline:{wall:0 ext:0 loc:\\<nil\\>} Sys: HostList [\\]} rpc:0xc83b40} msRequest:{} sysRequest:{Ranks:{RWMutex:{w:{state:0 sema:0} writerSem:0 readerSem:0 readerCount:0 readerWait:0} HostSet:{Mutex:{state:0 sema:0} list:0xc0001909c0}} Hosts:{Mutex:{state:0 sema:0} list:0xc000190980}} retryableRequest:{retryTimeout:0 retryInterval:0 retryMaxTries:0 retryTestFn:0xc83ca0 retryFn:0xc83de0} FailOnUnavailable:false} DEBUG 21:17:29.769332 rpc.go:196: request hosts: [boro-8:10001boro-35:10001] DEBUG 21:17:29.823432 system.go:200: System-Query command succeeded, absent hosts: , absent ranks: Rank State ---- ----- [0-1] Joined dmg storage query usage \u00b6 # system storage query usage output for a 2 hosts DAOS server dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-35 17 GB 17 GB 0 % 0 B 0 B N/A boro-8 17 GB 17 GB 0 % 0 B 0 B N/A dmg pool create help \u00b6 dmg pool create \\--help Usage: dmg [OPTIONS] pool create [create-OPTIONS] Application Options: --allow-proxy Allow proxy configuration via environment -l, --host-list= comma separated list of addresses <ipv4addr/hostname> -i, --insecure have dmg attempt to connect without certificates -d, --debug enable debug output -j, --json Enable JSON output -J, --json-logging Enable JSON-formatted log output -o, --config-path= Client config file path Help Options: -h, --help Show this help message [create command options] -g, --group= DAOS pool to be owned by given group, format name\\@domain -u, --user= DAOS pool to be owned by given user, format name\\@domain -p, --name= Unique name for pool (set as label) -a, --acl-file= Access Control List file path for DAOS pool -z, --size= Total size of DAOS pool (auto) -t, --scm-ratio= Percentage of SCM:NVMe for pool storage (auto) (default: 6) -k, --nranks= Number of ranks to use (auto) -v, --nsvc= Number of pool service replicas -s, --scm-size= Per-server SCM allocation for DAOS pool (manual) -n, --nvme-size= Per-server NVMe allocation for DAOS pool (manual) -r, --ranks= Storage server unique identifiers (ranks) for DAOS pool -S, --sys= DAOS system that pool is to be a part of (default: daos\\_server) dmg pool create \u00b6 # Create a 10GB pool dmg pool create \\--size=10G Creating DAOS pool with automatic storage allocation: 10 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 0a6003c6-23a7-4cb5-8895-c004ca2b75f5 Service Ranks : 0 Storage Ranks : \\[0-1\\] Total Size : 10 GB SCM : 10 GB (5.0 GB / rank) NVMe : 0 B (0 B / rank) dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-35 17 GB 12 GB 29 % 0 B 0 B N/A boro-8 17 GB 11 GB 36 % 0 B 0 B N/A dmg pool create for specified user and group \u00b6 # Create a 1GB pool for user:user\\_1 group:admin\\_group1 dmg pool create \\--group=admin\\_group1 \\--user=user\\_1 \\--size=1G Creating DAOS pool with automatic storage allocation: 1.0 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 64efd827-6bcb-434b-ab78-2010984539ff Service Ranks : 0 Storage Ranks : 0 Total Size : 1.0 GB SCM : 1.0 GB (1.0 GB / rank) NVMe : 0 B (0 B / rank) dmg pool create with security setting \u00b6 # Create a pool with access-control via a access-list test file dmg pool create \\--size=1G \\--acl-file=/tmp/acl\\_test.txt Creating DAOS pool with automatic storage allocation: 1.0 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 4533f724-7234-4c70-946c-b7a53d7d0ddf Service Ranks : 0 Storage Ranks : 0 Total Size : 1.0 GB SCM : 1.0 GB (1.0 GB / rank) NVMe : 0 B (0 B / rank) # Example of access entries on /tmp/acl\\_test.txt # pool OWNER: read-write permission # pool owner GROUP: read-write permission # test_user1: write-only permission # test_user2: read-only permission # test_group1: write-only permission # test_group2: read-only permission # EVERYONE else: no permission A::OWNER@:rw A:G:GROUP@:rw A::test_user1@:w A::test_user2@:r A:G:test_group1@:w A:G:test_group2@:r A::EVERYONE@: # Get pool security acl dmg pool get-acl --pool=$DAOS_POOL # Entries: A::OWNER@:rw A::test_user1@:w A::test_user2@:r A:G:GROUP@:rw A:G:test_group1@:w A:G:test_group2@:r A::EVERYONE@: # Update pool access entry for the existing test_group1 to no-permission dmg pool update-acl -e A:G:test_group1@: --pool=$DAOS_POOL # Update pool access entry for a new user test_user3 with rw permission dmg pool update-acl -e A::test_user3@:rw --pool=$DAOS_POOL # Get pool security acl after update-acl dmg pool get-acl --pool=$DAOS_POOL # Entries: A::OWNER@:rw A::test_user1@:w A::test_user2@:r A::test_user3@:rw A:G:GROUP@:rw A:G:test_group1@: A:G:test_group2@:r A::EVERYONE@: dmg pool list \u00b6 dmg pool list Pool UUID Svc Replicas --------- ------------ 5f362dc2-6154-44c7-8348-9de6f0a3d5d1 0 dmg pool destroy \u00b6 dmg pool destroy --pool=$DAOS_POOL Pool-destroy command succeeded dmg pool list no pools in system dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-35 17 GB 17 GB 0 % 0 B 0 B N/A boro-8 17 GB 17 GB 0 % 0 B 0 B N/A dmg pool query \u00b6 dmg pool create --size=10G Creating DAOS pool with automatic storage allocation: 10 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : cf860261-4fde-4403-b10b-abe8eb9dd32f Service Ranks : 0 Storage Ranks : \\[0-1\\] Total Size : 10 GB SCM : 10 GB (5.0 GB / rank) NVMe : 0 B (0 B / rank) dmg pool list Pool UUID Svc Replicas --------- ------------ cf860261-4fde-4403-b10b-abe8eb9dd32f 0 dmg pool query --pool=$DAOS_POOL Pool cf860261-4fde-4403-b10b-abe8eb9dd32f, ntarget=16, disabled=0, leader=0, version=1 Pool space info: - Target(VOS) count:16 - SCM: Total size: 10 GB Free: 10 GB, min:625 MB, max:625 MB, mean:625 MB - NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild idle, 0 objs, 0 recs DAOS tool (daos) usage for daos_client \u00b6 daos tool help \u00b6 /usr/bin/daos help daos command (v1.2), libdaos 1.2.0 usage: daos RESOURCE COMMAND \\[OPTIONS\\] resources: pool pool container (cont) container filesystem (fs) copy to and from a POSIX filesystem object (obj) object shell Interactive obj ctl shell for DAOS version print command version help print this message and exit use 'daos help RESOURCE' for resource specifics daos help cont daos command (v1.2), libdaos 1.2.0 container (cont) commands: create create a container clone clone a container destroy destroy a container list-objects list all objects in container list-obj query query a container get-prop get all container\\'s properties set-prop set container\\'s properties get-acl get a container\\'s ACL overwrite-acl replace a container\\'s ACL update-acl add/modify entries in a container\\'s ACL delete-acl delete an entry from a container\\'s ACL set-owner change the user and/or group that own a container stat get container statistics check check objects consistency in container list-attrs list container user-defined attributes del-attr delete container user-defined attribute get-attr get container user-defined attribute set-attr set container user-defined attribute create-snap create container snapshot (optional name) at most recent committed epoch list-snaps list container snapshots taken destroy-snap destroy container snapshots by name, epoch or range rollback roll back container to specified snapshot use 'daos help cont|container COMMAND' for command specific options daos container create \u00b6 dmg pool create \\--size=10G Creating DAOS pool with automatic storage allocation: 10 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 528f4710-7eb8-4850-b6aa-09e4b3c8f532 Service Ranks : 0 Storage Ranks : 0 Total Size : 10 GB SCM : 10 GB (10 GB / rank) NVMe : 0 B (0 B / rank) daos cont create \\--pool=\\$DAOS\\_POOL Successfully created container bfef23e9-bbfa-4743-a95c-144c44078f16 daos container create with HDF5 type \u00b6 # Create a HDF5 container # By default: type = POSIX daos cont create --type=HDF5 --pool=$DAOS_POOL Successfully created container bc4fe707-7470-4b7d-83bf-face75cc98fc daos container create with redundancy factor \u00b6 # Create a container with oclass RP_2G1, redundancy factor = 1 daos cont create --oclass=RP_2G1 --properties=rf:1 --pool=$DAOS_POOL Successfully created container 0d121c02-a42d-4029-8dce-3919b964b7b3 daos container list \u00b6 daos pool list-cont \\--pool=\\$DAOS\\_POOL bc4fe707-7470-4b7d-83bf-face75cc98fc 0d121c02-a42d-4029-8dce-3919b964b7b3 daos container destroy \u00b6 daos cont destroy \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Successfully destroyed container bc4fe707-7470-4b7d-83bf-face75cc98fc daos container query \u00b6 daos cont query \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Pool UUID: 528f4710-7eb8-4850-b6aa-09e4b3c8f532 Container UUID: bc4fe707-7470-4b7d-83bf-face75cc98fc Number of snapshots: 0 Latest Persistent Snapshot: 0 Highest Aggregated Epoch: 172477977191481344 Container redundancy factor: 1 daos container snapshot help/create/list/destroy \u00b6 daos help cont create-snap daos command (v1.2), libdaos 1.2.0 container options (snapshot and rollback-related): --snap=NAME container snapshot (create/destroy-snap, rollback) --epc=EPOCHNUM container epoch (destroy-snap, rollback) --epcrange=B-E container epoch range (destroy-snap) container options (query, and all commands except create): <pool options\\> with \\--cont use: (\\--pool, \\--sys-name) <pool options\\> with \\--path use: (\\--sys-name) --cont=UUID (mandatory, or use \\--path) --path=PATHSTR daos cont create-snap \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT snapshot/epoch 172646116775952384 has been created daos container list-snaps \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Container\\'s snapshots : 172478166024060928 172646116775952384 daos container destroy-snap \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT --epc=172646116775952384 daos container list-snaps \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Container\\'s snapshots : 172478166024060928 Common errors and workarounds \u00b6 Use dmg command without daos_admin privilege \u00b6 # Error message or timeout after dmg system query dmg system query ERROR: dmg: Unable to load Certificate Data: could not load cert: stat /etc/daos/certs/admin.crt: no such file or directory # Workaround # 1. Make sure the admin-host /etc/daos/daos_control.yml is correctly configured. # including: # hostlist: <daos_server_lists> # port: <port_num> # transport\\config: # allow_insecure: <true/false> # ca\\cert: /etc/daos/certs/daosCA.crt # cert: /etc/daos/certs/admin.crt # key: /etc/daos/certs/admin.key # 2. Make sure the admin-host allow_insecure mode matches the applicable servers. use daos command before daos_agent started \u00b6 daos cont create \\--pool=\\$DAOS\\_POOL daos cont create --pool=$DAOS_POOL daos ERR src/common/drpc.c:217 unixcomm_connect() Failed to connect to /var/run/daos_agent/daos_agent.sock, errno=2(No such file or directory) mgmt ERR src/mgmt/cli_mgmt.c:222 get_attach_info() failed to connect to /var/run/daos_agent/daos_agent.sock DER_MISC(-1025): 'Miscellaneous error' failed to initialize daos: Miscellaneous error (-1025) # Work around to check for daos_agent certification and start daos_agent #check for /etc/daos/certs/daosCA.crt, agent.crt and agent.key sudo systemctl enable daos_agent.service sudo systemctl start daos_agent.service use daos command with invalid or wrong parameters \u00b6 # Lack of providing daos pool_uuid daos pool list-cont pool UUID required rc: 2 daos command (v1.2), libdaos 1.2.0 usage: daos RESOURCE COMMAND [OPTIONS] resources: pool pool container (cont) container filesystem (fs) copy to and from a POSIX filesystem object (obj) object shell Interactive obj ctl shell for DAOS version print command version help print this message and exit use 'daos help RESOURCE' for resource specifics # Invalid sub-command cont-list $ daos pool cont-list --pool=$DAOS_POOL invalid pool command: cont-list error parsing command line arguments daos command (v1.2), libdaos 1.2.0 usage: daos RESOURCE COMMAND [OPTIONS] resources: pool pool container (cont) container filesystem (fs) copy to and from a POSIX filesystem object (obj) object shell Interactive obj ctl shell for DAOS version print command version help print this message and exit use 'daos help RESOURCE' for resource specifics # Working daos pool command $ daos pool list-cont --pool=$DAOS_POOL bc4fe707-7470-4b7d-83bf-face75cc98fc dmg pool create failed due to no space \u00b6 dmg pool create --size=50G Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM ERROR: dmg: pool create failed: DER_NOSPACE(-1007): No space on storage target # Workaround: dmg storage query scan to find current available storage dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-8 17 GB 6.0 GB 65 % 0 B 0 B N/A dmg pool create --size=2G Creating DAOS pool with automatic storage allocation: 2.0 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ----------------------------------------- UUID : b5ce2954-3f3e-4519-be04-ea298d776132 Service Ranks : 0 Storage Ranks : 0 Total Size : 2.0 GB SCM : 2.0 GB (2.0 GB / rank) NVMe : 0 B (0 B / rank) $ dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-8 17 GB 2.9 GB 83 % 0 B 0 B N/A dmg pool destroy timeout \u00b6 # dmg pool destroy Timeout or failed due to pool has active container(s) # Workaround pool destroy --force option dmg pool destroy --pool=$DAOS_POOL --force Pool-destroy command succeeded Run with dfuse fio \u00b6 required rpm \u00b6 sudo yum install -y fio or sudo yum install -y daos-tests run fio \u00b6 $ dmg pool create --size=10G $ daos cont create --pool=$DAOS_POOL --type=POSIX $ daos cont query --pool=$DAOS_POOL --cont=$DAOS_CONT Pool UUID: f688f2ad-76ae-4368-8d1b-5697ca016a43 Container UUID: bcc5c793-60dc-4ec1-8bab-9d63ea18e794 Number of snapshots: 0 Latest Persistent Snapshot: 0 Highest Aggregated Epoch: 0 Container redundancy factor: 0 $ /usr/bin/mkdir /tmp/daos_test1 $ /usr/bin/touch /tmp/daos_test1/testfile $ /usr/bin/df -h -t fuse.daos df: no file systems processed $ /usr/bin/dfuse --m=/tmp/daos_test1 --pool=$DAOS_POOL --cont=$DAOS_CONT $ /usr/bin/df -h -t fuse.daos Filesystem Size Used Avail Use% Mounted on dfuse 954M 144K 954M 1% /tmp/daos_test1 $ /usr/bin/fio --name=random-write --ioengine=pvsync --rw=randwrite --bs=4k --size=128M --nrfiles=4 --directory=/tmp/daos_test1 --numjobs=8 --iodepth=16 --runtime=60 --time_based --direct=1 --buffered=0 --randrepeat=0 --norandommap --refill_buffers --group_reportingrandom-write: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync, iodepth=16 ... fio-3.7 Starting 8 processes random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) write: IOPS=19.9k, BW=77.9MiB/s (81.7MB/s)(731MiB/9379msec) clat (usec): min=224, max=6539, avg=399.16, stdev=70.52 lat (usec): min=224, max=6539, avg=399.19, stdev=70.52 clat percentiles (usec): ... bw ( KiB/s): min= 9368, max=10096, per=12.50%, avg=9972.06, stdev=128.28, samples=144 iops : min= 2342, max= 2524, avg=2493.01, stdev=32.07, samples=144 lat (usec) : 250=0.01%, 500=96.81%, 750=3.17%, 1000=0.01% lat (msec) : 10=0.01% cpu : usr=0.43%, sys=1.05%, ctx=187242, majf=0, minf=488 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,187022,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): WRITE: bw=77.9MiB/s (81.7MB/s), 77.9MiB/s-77.9MiB/s (81.7MB/s-81.7MB/s), io=731MiB (766MB), run=9379-9379msec # Data after fio completed $ ll /tmp/daos_test1 total 1048396 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.0.0 rw-rr- 1 user1 user1 33546240 Apr 21 23:28 random-write.0.1 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.0.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.0.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.0 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.0 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.3 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.3.0 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.3.1 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.3.2 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.3.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.4.0 rw-rr- 1 user1 user1 33525760 Apr 21 23:28 random-write.4.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.4.2 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.4.3 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.5.0 rw-rr- 1 user1 user1 33546240 Apr 21 23:28 random-write.5.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.5.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.5.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.6.0 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.6.1 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.6.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.6.3 rw-rr- 1 user1 user1 33525760 Apr 21 23:28 random-write.7.0 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.7.1 rw-rr- 1 user1 user1 33525760 Apr 21 23:28 random-write.7.2 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.7.3 unmount \u00b6 $ fusermount -u /tmp/daos_test1/ $ df -h -t fuse.daos df: no file systems processed Run with mpirun mdtest \u00b6 required rpms \u00b6 $ sudo yum install -y mpich $ sudo yum install -y mdtest $ sudo yum install -y Lmod $ sudo module load mpi/mpich-x86_64 $ /usr/bin/touch /tmp/daos_test1/testfile run mpirun ior and mdtest \u00b6 Run mpirun ior \u00b6 $ /usr/lib64/mpich/bin/mpirun -host -np 30 ior -a POSIX -b 26214400 -v -w -k -i 1 -o /tmp/daos_test1/testfile -t 25M IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O Began : Fri Apr 16 18:07:56 2021 Command line : ior -a POSIX -b 26214400 -v -w -k -i 1 -o /tmp/daos_test1/testfile -t 25M Machine : Linux boro-8.boro.hpdd.intel.com Start time skew across all tasks: 0.00 sec TestID : 0 StartTime : Fri Apr 16 18:07:56 2021 Path : /tmp/daos_test1/testfile FS : 3.8 GiB Used FS: 1.1% Inodes: 0.2 Mi Used Inodes: 0.1% Participating tasks : 30 Options: api : POSIX apiVersion : test filename : /tmp/daos_test1/testfile access : single-shared-file type : independent segments : 1 ordering in a file : sequential ordering inter file : no tasks offsets nodes : 1 tasks : 30 clients per node : 30 repetitions : 1 xfersize : 25 MiB blocksize : 25 MiB aggregate filesize : 750 MiB verbose : 1 Results: access bw(MiB/s) IOPS Latency(s) block(KiB) xfer(KiB) open(s) wr/rd(s) close(s) total(s) iter Commencing write performance test: Fri Apr 16 18:07:56 2021 write 1499.68 59.99 0.480781 25600 25600 0.300237 0.500064 0.483573 0.500107 0 Max Write: 1499.68 MiB/sec (1572.53 MB/sec) Summary of all tests: Operation Max(MiB) Min(MiB) Mean(MiB) StdDev Max(OPs) Min(OPs) Mean(OPs) StdDev Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggs(MiB) API RefNum write 1499.68 1499.68 1499.68 0.00 59.99 59.99 59.99 0.00 0.50011 NA NA 0 30 30 1 0 0 1 0 0 1 26214400 26214400 750.0 POSIX 0 Finished : Fri Apr 16 18:07:57 2021 Run mpirun mdtest \u00b6 $ /usr/lib64/mpich/bin/mpirun -host <host1> -np 30 mdtest -a DFS -z 0 -F -C -i 1 -n 1667 -e 4096 -d / -w 4096 --dfs.chunk_size 1048576 --dfs.cont <container.uuid> --dfs.destroy --dfs.dir_oclass RP_3G1 --dfs.group daos_server --dfs.oclass RP_3G1 --dfs.pool <pool_uuid> \u2013 started at 04/16/2021 22:01:55 \u2013 mdtest-3.4.0+dev was launched with 30 total task(s) on 1 node(s) Command line used: mdtest 'a' 'DFS' '-z' '0' '-F' '-C' '-i' '1' '-n' '1667' '-e' '4096' '-d' '/' '-w' '4096' 'dfs.chunk_size' '1048576' 'dfs.cont' '3e661024-2f1f-4d7a-9cd4-1b05601e0789' 'dfs.destroy' 'dfs.dir_oclass' 'SX' 'dfs.group' 'daos_server' 'dfs.oclass' 'SX' '-dfs.pool' 'd546a7f5-586c-4d8f-aecd-372878df7b97' WARNING: unable to use realpath() on file system. Path: FS: 0.0 GiB Used FS: -nan% Inodes: 0.0 Mi Used Inodes: -nan% Nodemap: 111111111111111111111111111111 30 tasks, 50010 files SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- \u2014 \u2014 ---- ------- File creation : 14206.584 14206.334 14206.511 0.072 File stat : 0.000 0.000 0.000 0.000 File read : 0.000 0.000 0.000 0.000 File removal : 0.000 0.000 0.000 0.000 Tree creation : 1869.791 1869.791 1869.791 0.000 Tree removal : 0.000 0.000 0.000 0.000 \u2013 finished at 04/16/2021 22:01:58 \u2013 $ /usr/lib64/mpich/bin/mpirun -host <host1> -np 50 mdtest -a DFS -z 0 -F -C -i 1 -n 1667 -e 4096 -d / -w 4096 --dfs.chunk_size 1048576 --dfs.cont 3e661024-2f1f-4d7a-9cd4-1b05601e0789 --dfs.destroy --dfs.dir_oclass SX --dfs.group daos_server --dfs.oclass SX --dfs.pool d546a7f5-586c-4d8f-aecd-372878df7b97 \u2013 started at 04/16/2021 22:02:21 \u2013 mdtest-3.4.0+dev was launched with 50 total task(s) on 1 node(s) Command line used: mdtest 'a' 'DFS' '-z' '0' '-F' '-C' '-i' '1' '-n' '1667' '-e' '4096' '-d' '/' '-w' '4096' 'dfs.chunk_size' '1048576' 'dfs.cont' '3e661024-2f1f-4d7a-9cd4-1b05601e0789' 'dfs.destroy' 'dfs.dir_oclass' 'SX' 'dfs.group' 'daos_server' 'dfs.oclass' 'SX' '-dfs.pool' 'd546a7f5-586c-4d8f-aecd-372878df7b97' WARNING: unable to use realpath() on file system. Path: FS: 0.0 GiB Used FS: -nan% Inodes: 0.0 Mi Used Inodes: -nan% Nodemap: 11111111111111111111111111111111111111111111111111 50 tasks, 83350 files SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- \u2014 \u2014 ---- ------- File creation : 13342.303 13342.093 13342.228 0.059 File stat : 0.000 0.000 0.000 0.000 File read : 0.000 0.000 0.000 0.000 File removal : 0.000 0.000 0.000 0.000 Tree creation : 1782.938 1782.938 1782.938 0.000 Tree removal : 0.000 0.000 0.000 0.000 \u2013 finished at 04/16/2021 22:02:27 \u2013 Run with 4 DAOS hosts server, rebuild with dfuse_io and mpirun \u00b6 Environment variables setup \u00b6 export SERVER_NODES=node-1,node-2,node-3,node-4 export ADMIN_NODE=node-5 export CLIENT_NODE=node-5 export ALL_NODES=$SERVER_NODES,$CLIENT_NODE Run dfuse \u00b6 # Bring up 4 hosts server with appropriate daos_server.yml and # access-point, reference to DAOS Set-Up # After DAOS servers, DAOS admin and client started. $ dmg storage format Format Summary: Hosts SCM Devices NVMe Devices ----- ----------- ------------ boro-[8,35,52-53] 1 0 $ dmg pool list Pool UUID Svc Replicas --------- ------------ 733bee7b-c2af-499e-99dd-313b1ef092a9 [1-3] $ daos cont create --pool=$DAOS_POOL --type=POSIX --oclass=RP_3G1 --properties=rf:2 Successfully created container 2649aa0f-3ad7-4943-abf5-4343205a637b $ daos pool list-cont --pool=$DAOS_POOL 2649aa0f-3ad7-4943-abf5-4343205a637b $ dmg pool query --pool=$DAOS_POOL Pool 733bee7b-c2af-499e-99dd-313b1ef092a9, ntarget=32, disabled=0, leader=2, version=1 Pool space info: - Target(VOS) count:32 - SCM: Total size: 5.0 GB Free: 5.0 GB, min:156 MB, max:156 MB, mean:156 MB - NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild idle, 0 objs, 0 recs $ df -h -t fuse.daos df: no file systems processed $ mkdir /tmp/daos_test1 $ dfuse --mountpoint=/tmp/daos_test1 --pool=$DAOS_POOL --cont=$DAOS_CONT $ df -h -t fuse.daos Filesystem Size Used Avail Use% Mounted on dfuse 19G 1.1M 19G 1% /tmp/daos_test1 $ fio --name=random-write --ioengine=pvsync --rw=randwrite --bs=4k --size=128M --nrfiles=4 --directory=/tmp/daos_test1 --numjobs=8 --iodepth=16 --runtime=60 --time_based --direct=1 --buffered=0 --randrepeat=0 --norandommap --refill_buffers --group_reporting random-write: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync, iodepth=16 ... fio-3.7 Starting 8 processes random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) Jobs: 8 (f=32): [w(8)][100.0%][r=0KiB/s,w=96.1MiB/s][r=0,w=24.6k IOPS][eta 00m:00s] random-write: (groupid=0, jobs=8): err= 0: pid=27879: Sat Apr 17 01:12:57 2021 write: IOPS=24.4k, BW=95.3MiB/s (99.9MB/s)(5716MiB/60001msec) clat (usec): min=220, max=6687, avg=326.19, stdev=55.29 lat (usec): min=220, max=6687, avg=326.28, stdev=55.29 clat percentiles (usec): | 1.00th=[ 260], 5.00th=[ 273], 10.00th=[ 285], 20.00th=[ 293], | 30.00th=[ 306], 40.00th=[ 314], 50.00th=[ 322], 60.00th=[ 330], | 70.00th=[ 338], 80.00th=[ 355], 90.00th=[ 375], 95.00th=[ 396], | 99.00th=[ 445], 99.50th=[ 465], 99.90th=[ 523], 99.95th=[ 562], | 99.99th=[ 1827] bw ( KiB/s): min=10976, max=12496, per=12.50%, avg=12191.82, stdev=157.87, samples=952 iops : min= 2744, max= 3124, avg=3047.92, stdev=39.47, samples=952 lat (usec) : 250=0.23%, 500=99.61%, 750=0.15%, 1000=0.01% lat (msec) : 2=0.01%, 4=0.01%, 10=0.01% cpu : usr=0.81%, sys=1.69%, ctx=1463535, majf=0, minf=308 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,1463226,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): WRITE: bw=95.3MiB/s (99.9MB/s), 95.3MiB/s-95.3MiB/s (99.9MB/s-99.9MB/s), io=5716MiB (5993MB), run=60001-60001msec Run dfuse with rebuild \u00b6 # Start dfuse $ fio --name=random-write --ioengine=pvsync --rw=randwrite --bs=4k --size=128M --nrfiles=4 --directory=/tmp/daos_test1 --numjobs=8 --iodepth=16 --runtime=60 --time_based --direct=1 --buffered=0 --randrepeat=0 --norandommap --refill_buffers --group_reporting random-write: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync, iodepth=16 ... fio-3.7 Starting 8 processes fio: io_u error on file /tmp/daos_test1/random-write.2.1: Input/output error: write offset=8527872, buflen=4096 fio: pid=28242, err=5 file:io_u.c:1747 bw ( KiB/s): min= 3272, max=12384, per=30.14%, avg=11624.50, stdev=2181.01, samples=128 iops : min= 818, max= 3096, avg=2906.12, stdev=545.25, samples=128 lat (usec) : 250=0.23%, 500=99.59%, 750=0.12%, 1000=0.01% lat (msec) : 2=0.03%, 4=0.02% cpu : usr=0.27%, sys=0.66%, ctx=186210, majf=0, minf=494 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,186000,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): WRITE: bw=37.7MiB/s (39.5MB/s), 37.7MiB/s-37.7MiB/s (39.5MB/s-39.5MB/s), io=727MiB (762MB), run=19291-19291msec ... # from daos_admin console, stop leader-rank with debug $ dmg -d system stop --ranks=3 DEBUG 01:34:58.026753 main.go:217: debug output enabled DEBUG 01:34:58.027457 main.go:244: control config loaded from /etc/daos/daos_control.yml Rank Operation Result --------- ------ 3 stop OK $ daos pool list-cont --pool=$DAOS_POOL cf2a95ce-9910-4d5e-814c-cafb0a7f0944 $ dmg pool query --pool=$DAOS_POOL Pool 70f73efc-848e-4f6e-b4fd-909bcf9bd427, ntarget=32, disabled=8, leader=2, version=18 Pool space info: Target(VOS) count:24 SCM: Total size: 15 GB Free: 14 GB, min:575 MB, max:597 MB, mean:587 MB NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild done, 1 objs, 57 recs # Verify stopped server been evicted $ dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 boro-53.boro.hpdd.intel.com Joined 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 boro-52.boro.hpdd.intel.com Evicted system stop # Restart, after evicted server restarted, verify the server joined $ /usr/bin/dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 /boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 /boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 /boro-53.boro.hpdd.intel.com Joined 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 /boro-52.boro.hpdd.intel.com Joined # Unmount after test completed $ fusermount -u /tmp/daos_test1/ $ df -h -t fuse.daos df: no file systems processed Run mpirun mdtest with rebuild \u00b6 $ dmg pool create --size=50G Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ----------------------------------------- UUID : 4eda8a8c-028c-461c-afd3-704534961572 Service Ranks : [1-3] Storage Ranks : [0-3] Total Size : 50 GB SCM : 50 GB (12 GB / rank) NVMe : 0 B (0 B / rank) $ daos cont create --pool=$DAOS_POOL --type=POSIX --oclass=RP_3G1 --properties=rf:2 Successfully created container d71ff6a5-15a5-43fe-b829-bef9c65b9ccb $ /usr/lib64/mpich/bin/mpirun -host boro-8 -np 30 mdtest -a DFS -z 0 -F -C -i 100 -n 1667 -e 4096 -d / -w 4096 --dfs.chunk_size 1048576 --dfs.cont $DAOS_CONT --dfs.destroy --dfs.dir_oclass RP_3G1 --dfs.group daos_server --dfs.oclass RP_3G1 --dfs.pool $DAOS_POOL started at 04/22/2021 17:46:20 \u2013 mdtest-3.4.0+dev was launched with 30 total task(s) on 1 node(s) Command line used: mdtest 'a' 'DFS' '-z' '0' '-F' '-C' '-i' '100' '-n' '1667' '-e' '4096' '-d' '/' '-w' '4096' 'dfs.chunk_size' '1048576' 'dfs.cont' 'd71ff6a5-15a5-43fe-b829-bef9c65b9ccb' 'dfs.destroy' 'dfs.dir_oclass' 'RP_3G1' 'dfs.group' 'daos_server' 'dfs.oclass' 'RP_3G1' '-dfs.pool' '4eda8a8c-028c-461c-afd3-704534961572' WARNING: unable to use realpath() on file system. Path: FS: 0.0 GiB Used FS: -nan% Inodes: 0.0 Mi Used Inodes: -nan% Nodemap: 111111111111111111111111111111 30 tasks, 50010 files ... # from daos_admin console, stop a server rank $ dmg system stop --ranks=2 Rank Operation Result --------- ------ 2 stop OK # Verify stopped server been evicted $ dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 boro-53.boro.hpdd.intel.com Evicted system stop 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 boro-52.boro.hpdd.intel.com Joined # Restart, after evicted server restarted, verify the server joined $ /usr/bin/dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 /boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 /boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 /boro-53.boro.hpdd.intel.com Joined 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 /boro-52.boro.hpdd.intel.com Joined Clean-Up \u00b6 # pool reintegrate $ dmg pool reintegrate --pool=$DAOS_POOL --rank=2 Reintegration command succeeded # destroy container $ daos container destroy --pool=$DAOS_POOL --cont=$DAOS_CONT # destroy pool $ dmg pool destroy --pool=$DAOS_POOL Pool-destroy command succeeded # stop clients $ pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent.service\" # disable clients $ pdsh -S -w $CLIENT_NODES \"sudo systemctl disable daos_agent.service\" # stop servers $ pdsh -S -w $SERVER_NODES \"sudo systemctl stop daos_server.service\" # disable servers $ pdsh -S -w $SERVER_NODES \"sudo systemctl disable daos_server.service\"","title":"DAOS Admin/Client Tools"},{"location":"QSG/tour/#daos-tour","text":"","title":"DAOS Tour"},{"location":"QSG/tour/#introduction","text":"This documentation provides a general tour to the DAOS management commands (dmg) for daos_admin, and DAOS tools (daos) for daos_client users. Help and setup for the following is provided in this chapter: Pool and Container create, list, query and destroy on DAOS server for daos_admin and daos_client users. Common errors and workarounds for new users when using the dmg and daos tools. Example runs of data transfer between DAOS file systems, by setting up of the DAOS dfuse mount point and run traffic with dfuse fio and mpirun mdtest. Examples of basic dmg and daos tools run on 2 host DAOS servers and 1 host client, and runs of DAOS rebuild over dfuse fio and mpirun mdtest on a 4 host DAOS server.","title":"Introduction"},{"location":"QSG/tour/#requirements","text":"Set environment variables for list of servers, client and admin node. # Example of 2 hosts server # For 1 host server, export SERVER\\_NODES=node-1 export SERVER\\_NODES=node-1,node-2 # Example to use admin and client on the same node export ADMIN\\_NODE=node-3 export CLIENT\\_NODE=node-3 export ALL\\_NODES=\\$SERVER\\_NODES,\\$CLIENT\\_NODE","title":"Requirements"},{"location":"QSG/tour/#set-up","text":"Refer to the DAOS CentOS Setup or the DAOS openSUSE Setup for RPM installation, daos server/agent/admin configuration yml files, certificate generation, and bring-up DAOS servers and clients.","title":"Set-Up"},{"location":"QSG/tour/#daos-management-tool-dmg-usage-for-daos_admin","text":"","title":"DAOS management tool (dmg) usage for daos_admin"},{"location":"QSG/tour/#dmg-tool-help","text":"# DAOS management tool full path /usr/bin/dmg dmg \\--help Usage: dmg \\[OPTIONS\\] \\<command\\> Application Options: --allow-proxy Allow proxy configuration via environment -l, --host-list= comma separated list of addresses <ipv4addr/hostname\\> -i, --insecure have dmg attempt to connect without certificates -d, --debug enable debug output -j, --json Enable JSON output -J, --json-logging Enable JSON-formatted log output -o, --config-path= Client config file path Help Options: -h, --help Show this help message Available commands: config Perform tasks related to configuration of hardware remote servers (aliases: co) cont Perform tasks related to DAOS containers (aliases: c) network Perform tasks related to network devices attached to remote servers (aliases: n) pool Perform tasks related to DAOS pools (aliases: p) storage Perform tasks related to storage attached to remote servers (aliases: st) system Perform distributed tasks related to DAOS system (aliases: sy) telemetry Perform telemetry operations version Print dmg version","title":"dmg tool help"},{"location":"QSG/tour/#dmg-system-query","text":"# system query output for a 2 hosts DAOS server dmg system query Rank State ---- ---- [0-1] Joined","title":"dmg system query"},{"location":"QSG/tour/#dmg-system-query-verbose-output","text":"dmg system query \\--verbose (-v) Rank UUID Control Address Fault Domain State Reason ---- ---- --------------- ------------ ----- ------ 0 570660ae-1727-4ce0-9650-6c31e81c9d30 10.7.1.8:10001 /boro-8.boro.hpdd.intel.com Joined 1 74390dd0-7fbc-4309-8665-d5f24218c8d9 10.7.1.35:10001 /boro-35.boro.hpdd.intel.com Joined","title":"dmg system query verbose output"},{"location":"QSG/tour/#dmg-system-query-with-debug","text":"dmg system query \\--debug (-d) DEBUG 21:17:29.765815 main.go:216: debug output enabled DEBUG 21:17:29.766483 main.go:243: control config loaded from /etc/daos/daos\\_control.yml DEBUG 21:17:29.768661 system.go:368: DAOS system query request: &{unaryRequest:{request:{deadline:{wall:0 ext:0 loc:\\<nil\\>} Sys: HostList [\\]} rpc:0xc83b40} msRequest:{} sysRequest:{Ranks:{RWMutex:{w:{state:0 sema:0} writerSem:0 readerSem:0 readerCount:0 readerWait:0} HostSet:{Mutex:{state:0 sema:0} list:0xc0001909c0}} Hosts:{Mutex:{state:0 sema:0} list:0xc000190980}} retryableRequest:{retryTimeout:0 retryInterval:0 retryMaxTries:0 retryTestFn:0xc83ca0 retryFn:0xc83de0} FailOnUnavailable:false} DEBUG 21:17:29.769332 rpc.go:196: request hosts: [boro-8:10001boro-35:10001] DEBUG 21:17:29.823432 system.go:200: System-Query command succeeded, absent hosts: , absent ranks: Rank State ---- ----- [0-1] Joined","title":"dmg system query with debug"},{"location":"QSG/tour/#dmg-storage-query-usage","text":"# system storage query usage output for a 2 hosts DAOS server dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-35 17 GB 17 GB 0 % 0 B 0 B N/A boro-8 17 GB 17 GB 0 % 0 B 0 B N/A","title":"dmg storage query usage"},{"location":"QSG/tour/#dmg-pool-create-help","text":"dmg pool create \\--help Usage: dmg [OPTIONS] pool create [create-OPTIONS] Application Options: --allow-proxy Allow proxy configuration via environment -l, --host-list= comma separated list of addresses <ipv4addr/hostname> -i, --insecure have dmg attempt to connect without certificates -d, --debug enable debug output -j, --json Enable JSON output -J, --json-logging Enable JSON-formatted log output -o, --config-path= Client config file path Help Options: -h, --help Show this help message [create command options] -g, --group= DAOS pool to be owned by given group, format name\\@domain -u, --user= DAOS pool to be owned by given user, format name\\@domain -p, --name= Unique name for pool (set as label) -a, --acl-file= Access Control List file path for DAOS pool -z, --size= Total size of DAOS pool (auto) -t, --scm-ratio= Percentage of SCM:NVMe for pool storage (auto) (default: 6) -k, --nranks= Number of ranks to use (auto) -v, --nsvc= Number of pool service replicas -s, --scm-size= Per-server SCM allocation for DAOS pool (manual) -n, --nvme-size= Per-server NVMe allocation for DAOS pool (manual) -r, --ranks= Storage server unique identifiers (ranks) for DAOS pool -S, --sys= DAOS system that pool is to be a part of (default: daos\\_server)","title":"dmg pool create help"},{"location":"QSG/tour/#dmg-pool-create","text":"# Create a 10GB pool dmg pool create \\--size=10G Creating DAOS pool with automatic storage allocation: 10 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 0a6003c6-23a7-4cb5-8895-c004ca2b75f5 Service Ranks : 0 Storage Ranks : \\[0-1\\] Total Size : 10 GB SCM : 10 GB (5.0 GB / rank) NVMe : 0 B (0 B / rank) dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-35 17 GB 12 GB 29 % 0 B 0 B N/A boro-8 17 GB 11 GB 36 % 0 B 0 B N/A","title":"dmg pool create"},{"location":"QSG/tour/#dmg-pool-create-for-specified-user-and-group","text":"# Create a 1GB pool for user:user\\_1 group:admin\\_group1 dmg pool create \\--group=admin\\_group1 \\--user=user\\_1 \\--size=1G Creating DAOS pool with automatic storage allocation: 1.0 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 64efd827-6bcb-434b-ab78-2010984539ff Service Ranks : 0 Storage Ranks : 0 Total Size : 1.0 GB SCM : 1.0 GB (1.0 GB / rank) NVMe : 0 B (0 B / rank)","title":"dmg pool create for specified user and group"},{"location":"QSG/tour/#dmg-pool-create-with-security-setting","text":"# Create a pool with access-control via a access-list test file dmg pool create \\--size=1G \\--acl-file=/tmp/acl\\_test.txt Creating DAOS pool with automatic storage allocation: 1.0 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 4533f724-7234-4c70-946c-b7a53d7d0ddf Service Ranks : 0 Storage Ranks : 0 Total Size : 1.0 GB SCM : 1.0 GB (1.0 GB / rank) NVMe : 0 B (0 B / rank) # Example of access entries on /tmp/acl\\_test.txt # pool OWNER: read-write permission # pool owner GROUP: read-write permission # test_user1: write-only permission # test_user2: read-only permission # test_group1: write-only permission # test_group2: read-only permission # EVERYONE else: no permission A::OWNER@:rw A:G:GROUP@:rw A::test_user1@:w A::test_user2@:r A:G:test_group1@:w A:G:test_group2@:r A::EVERYONE@: # Get pool security acl dmg pool get-acl --pool=$DAOS_POOL # Entries: A::OWNER@:rw A::test_user1@:w A::test_user2@:r A:G:GROUP@:rw A:G:test_group1@:w A:G:test_group2@:r A::EVERYONE@: # Update pool access entry for the existing test_group1 to no-permission dmg pool update-acl -e A:G:test_group1@: --pool=$DAOS_POOL # Update pool access entry for a new user test_user3 with rw permission dmg pool update-acl -e A::test_user3@:rw --pool=$DAOS_POOL # Get pool security acl after update-acl dmg pool get-acl --pool=$DAOS_POOL # Entries: A::OWNER@:rw A::test_user1@:w A::test_user2@:r A::test_user3@:rw A:G:GROUP@:rw A:G:test_group1@: A:G:test_group2@:r A::EVERYONE@:","title":"dmg pool create with security setting"},{"location":"QSG/tour/#dmg-pool-list","text":"dmg pool list Pool UUID Svc Replicas --------- ------------ 5f362dc2-6154-44c7-8348-9de6f0a3d5d1 0","title":"dmg pool list"},{"location":"QSG/tour/#dmg-pool-destroy","text":"dmg pool destroy --pool=$DAOS_POOL Pool-destroy command succeeded dmg pool list no pools in system dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-35 17 GB 17 GB 0 % 0 B 0 B N/A boro-8 17 GB 17 GB 0 % 0 B 0 B N/A","title":"dmg pool destroy"},{"location":"QSG/tour/#dmg-pool-query","text":"dmg pool create --size=10G Creating DAOS pool with automatic storage allocation: 10 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : cf860261-4fde-4403-b10b-abe8eb9dd32f Service Ranks : 0 Storage Ranks : \\[0-1\\] Total Size : 10 GB SCM : 10 GB (5.0 GB / rank) NVMe : 0 B (0 B / rank) dmg pool list Pool UUID Svc Replicas --------- ------------ cf860261-4fde-4403-b10b-abe8eb9dd32f 0 dmg pool query --pool=$DAOS_POOL Pool cf860261-4fde-4403-b10b-abe8eb9dd32f, ntarget=16, disabled=0, leader=0, version=1 Pool space info: - Target(VOS) count:16 - SCM: Total size: 10 GB Free: 10 GB, min:625 MB, max:625 MB, mean:625 MB - NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild idle, 0 objs, 0 recs","title":"dmg pool query"},{"location":"QSG/tour/#daos-tool-daos-usage-for-daos_client","text":"","title":"DAOS tool (daos) usage for daos_client"},{"location":"QSG/tour/#daos-tool-help","text":"/usr/bin/daos help daos command (v1.2), libdaos 1.2.0 usage: daos RESOURCE COMMAND \\[OPTIONS\\] resources: pool pool container (cont) container filesystem (fs) copy to and from a POSIX filesystem object (obj) object shell Interactive obj ctl shell for DAOS version print command version help print this message and exit use 'daos help RESOURCE' for resource specifics daos help cont daos command (v1.2), libdaos 1.2.0 container (cont) commands: create create a container clone clone a container destroy destroy a container list-objects list all objects in container list-obj query query a container get-prop get all container\\'s properties set-prop set container\\'s properties get-acl get a container\\'s ACL overwrite-acl replace a container\\'s ACL update-acl add/modify entries in a container\\'s ACL delete-acl delete an entry from a container\\'s ACL set-owner change the user and/or group that own a container stat get container statistics check check objects consistency in container list-attrs list container user-defined attributes del-attr delete container user-defined attribute get-attr get container user-defined attribute set-attr set container user-defined attribute create-snap create container snapshot (optional name) at most recent committed epoch list-snaps list container snapshots taken destroy-snap destroy container snapshots by name, epoch or range rollback roll back container to specified snapshot use 'daos help cont|container COMMAND' for command specific options","title":"daos tool help"},{"location":"QSG/tour/#daos-container-create","text":"dmg pool create \\--size=10G Creating DAOS pool with automatic storage allocation: 10 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ---------------------------------------- UUID : 528f4710-7eb8-4850-b6aa-09e4b3c8f532 Service Ranks : 0 Storage Ranks : 0 Total Size : 10 GB SCM : 10 GB (10 GB / rank) NVMe : 0 B (0 B / rank) daos cont create \\--pool=\\$DAOS\\_POOL Successfully created container bfef23e9-bbfa-4743-a95c-144c44078f16","title":"daos container create"},{"location":"QSG/tour/#daos-container-create-with-hdf5-type","text":"# Create a HDF5 container # By default: type = POSIX daos cont create --type=HDF5 --pool=$DAOS_POOL Successfully created container bc4fe707-7470-4b7d-83bf-face75cc98fc","title":"daos container create with HDF5 type"},{"location":"QSG/tour/#daos-container-create-with-redundancy-factor","text":"# Create a container with oclass RP_2G1, redundancy factor = 1 daos cont create --oclass=RP_2G1 --properties=rf:1 --pool=$DAOS_POOL Successfully created container 0d121c02-a42d-4029-8dce-3919b964b7b3","title":"daos container create with redundancy factor"},{"location":"QSG/tour/#daos-container-list","text":"daos pool list-cont \\--pool=\\$DAOS\\_POOL bc4fe707-7470-4b7d-83bf-face75cc98fc 0d121c02-a42d-4029-8dce-3919b964b7b3","title":"daos container list"},{"location":"QSG/tour/#daos-container-destroy","text":"daos cont destroy \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Successfully destroyed container bc4fe707-7470-4b7d-83bf-face75cc98fc","title":"daos container destroy"},{"location":"QSG/tour/#daos-container-query","text":"daos cont query \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Pool UUID: 528f4710-7eb8-4850-b6aa-09e4b3c8f532 Container UUID: bc4fe707-7470-4b7d-83bf-face75cc98fc Number of snapshots: 0 Latest Persistent Snapshot: 0 Highest Aggregated Epoch: 172477977191481344 Container redundancy factor: 1","title":"daos container query"},{"location":"QSG/tour/#daos-container-snapshot-helpcreatelistdestroy","text":"daos help cont create-snap daos command (v1.2), libdaos 1.2.0 container options (snapshot and rollback-related): --snap=NAME container snapshot (create/destroy-snap, rollback) --epc=EPOCHNUM container epoch (destroy-snap, rollback) --epcrange=B-E container epoch range (destroy-snap) container options (query, and all commands except create): <pool options\\> with \\--cont use: (\\--pool, \\--sys-name) <pool options\\> with \\--path use: (\\--sys-name) --cont=UUID (mandatory, or use \\--path) --path=PATHSTR daos cont create-snap \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT snapshot/epoch 172646116775952384 has been created daos container list-snaps \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Container\\'s snapshots : 172478166024060928 172646116775952384 daos container destroy-snap \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT --epc=172646116775952384 daos container list-snaps \\--pool=\\$DAOS\\_POOL \\--cont=\\$DAOS\\_CONT Container\\'s snapshots : 172478166024060928","title":"daos container snapshot help/create/list/destroy"},{"location":"QSG/tour/#common-errors-and-workarounds","text":"","title":"Common errors and workarounds"},{"location":"QSG/tour/#use-dmg-command-without-daos_admin-privilege","text":"# Error message or timeout after dmg system query dmg system query ERROR: dmg: Unable to load Certificate Data: could not load cert: stat /etc/daos/certs/admin.crt: no such file or directory # Workaround # 1. Make sure the admin-host /etc/daos/daos_control.yml is correctly configured. # including: # hostlist: <daos_server_lists> # port: <port_num> # transport\\config: # allow_insecure: <true/false> # ca\\cert: /etc/daos/certs/daosCA.crt # cert: /etc/daos/certs/admin.crt # key: /etc/daos/certs/admin.key # 2. Make sure the admin-host allow_insecure mode matches the applicable servers.","title":"Use dmg command without daos_admin privilege"},{"location":"QSG/tour/#use-daos-command-before-daos_agent-started","text":"daos cont create \\--pool=\\$DAOS\\_POOL daos cont create --pool=$DAOS_POOL daos ERR src/common/drpc.c:217 unixcomm_connect() Failed to connect to /var/run/daos_agent/daos_agent.sock, errno=2(No such file or directory) mgmt ERR src/mgmt/cli_mgmt.c:222 get_attach_info() failed to connect to /var/run/daos_agent/daos_agent.sock DER_MISC(-1025): 'Miscellaneous error' failed to initialize daos: Miscellaneous error (-1025) # Work around to check for daos_agent certification and start daos_agent #check for /etc/daos/certs/daosCA.crt, agent.crt and agent.key sudo systemctl enable daos_agent.service sudo systemctl start daos_agent.service","title":"use daos command before daos_agent started"},{"location":"QSG/tour/#use-daos-command-with-invalid-or-wrong-parameters","text":"# Lack of providing daos pool_uuid daos pool list-cont pool UUID required rc: 2 daos command (v1.2), libdaos 1.2.0 usage: daos RESOURCE COMMAND [OPTIONS] resources: pool pool container (cont) container filesystem (fs) copy to and from a POSIX filesystem object (obj) object shell Interactive obj ctl shell for DAOS version print command version help print this message and exit use 'daos help RESOURCE' for resource specifics # Invalid sub-command cont-list $ daos pool cont-list --pool=$DAOS_POOL invalid pool command: cont-list error parsing command line arguments daos command (v1.2), libdaos 1.2.0 usage: daos RESOURCE COMMAND [OPTIONS] resources: pool pool container (cont) container filesystem (fs) copy to and from a POSIX filesystem object (obj) object shell Interactive obj ctl shell for DAOS version print command version help print this message and exit use 'daos help RESOURCE' for resource specifics # Working daos pool command $ daos pool list-cont --pool=$DAOS_POOL bc4fe707-7470-4b7d-83bf-face75cc98fc","title":"use daos command with invalid or wrong parameters"},{"location":"QSG/tour/#dmg-pool-create-failed-due-to-no-space","text":"dmg pool create --size=50G Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM ERROR: dmg: pool create failed: DER_NOSPACE(-1007): No space on storage target # Workaround: dmg storage query scan to find current available storage dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-8 17 GB 6.0 GB 65 % 0 B 0 B N/A dmg pool create --size=2G Creating DAOS pool with automatic storage allocation: 2.0 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ----------------------------------------- UUID : b5ce2954-3f3e-4519-be04-ea298d776132 Service Ranks : 0 Storage Ranks : 0 Total Size : 2.0 GB SCM : 2.0 GB (2.0 GB / rank) NVMe : 0 B (0 B / rank) $ dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- boro-8 17 GB 2.9 GB 83 % 0 B 0 B N/A","title":"dmg pool create failed due to no space"},{"location":"QSG/tour/#dmg-pool-destroy-timeout","text":"# dmg pool destroy Timeout or failed due to pool has active container(s) # Workaround pool destroy --force option dmg pool destroy --pool=$DAOS_POOL --force Pool-destroy command succeeded","title":"dmg pool destroy timeout"},{"location":"QSG/tour/#run-with-dfuse-fio","text":"","title":"Run with dfuse fio"},{"location":"QSG/tour/#required-rpm","text":"sudo yum install -y fio or sudo yum install -y daos-tests","title":"required rpm"},{"location":"QSG/tour/#run-fio","text":"$ dmg pool create --size=10G $ daos cont create --pool=$DAOS_POOL --type=POSIX $ daos cont query --pool=$DAOS_POOL --cont=$DAOS_CONT Pool UUID: f688f2ad-76ae-4368-8d1b-5697ca016a43 Container UUID: bcc5c793-60dc-4ec1-8bab-9d63ea18e794 Number of snapshots: 0 Latest Persistent Snapshot: 0 Highest Aggregated Epoch: 0 Container redundancy factor: 0 $ /usr/bin/mkdir /tmp/daos_test1 $ /usr/bin/touch /tmp/daos_test1/testfile $ /usr/bin/df -h -t fuse.daos df: no file systems processed $ /usr/bin/dfuse --m=/tmp/daos_test1 --pool=$DAOS_POOL --cont=$DAOS_CONT $ /usr/bin/df -h -t fuse.daos Filesystem Size Used Avail Use% Mounted on dfuse 954M 144K 954M 1% /tmp/daos_test1 $ /usr/bin/fio --name=random-write --ioengine=pvsync --rw=randwrite --bs=4k --size=128M --nrfiles=4 --directory=/tmp/daos_test1 --numjobs=8 --iodepth=16 --runtime=60 --time_based --direct=1 --buffered=0 --randrepeat=0 --norandommap --refill_buffers --group_reportingrandom-write: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync, iodepth=16 ... fio-3.7 Starting 8 processes random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) write: IOPS=19.9k, BW=77.9MiB/s (81.7MB/s)(731MiB/9379msec) clat (usec): min=224, max=6539, avg=399.16, stdev=70.52 lat (usec): min=224, max=6539, avg=399.19, stdev=70.52 clat percentiles (usec): ... bw ( KiB/s): min= 9368, max=10096, per=12.50%, avg=9972.06, stdev=128.28, samples=144 iops : min= 2342, max= 2524, avg=2493.01, stdev=32.07, samples=144 lat (usec) : 250=0.01%, 500=96.81%, 750=3.17%, 1000=0.01% lat (msec) : 10=0.01% cpu : usr=0.43%, sys=1.05%, ctx=187242, majf=0, minf=488 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,187022,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): WRITE: bw=77.9MiB/s (81.7MB/s), 77.9MiB/s-77.9MiB/s (81.7MB/s-81.7MB/s), io=731MiB (766MB), run=9379-9379msec # Data after fio completed $ ll /tmp/daos_test1 total 1048396 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.0.0 rw-rr- 1 user1 user1 33546240 Apr 21 23:28 random-write.0.1 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.0.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.0.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.0 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.1.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.0 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.2.3 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.3.0 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.3.1 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.3.2 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.3.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.4.0 rw-rr- 1 user1 user1 33525760 Apr 21 23:28 random-write.4.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.4.2 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.4.3 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.5.0 rw-rr- 1 user1 user1 33546240 Apr 21 23:28 random-write.5.1 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.5.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.5.3 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.6.0 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.6.1 rw-rr- 1 user1 user1 33550336 Apr 21 23:28 random-write.6.2 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.6.3 rw-rr- 1 user1 user1 33525760 Apr 21 23:28 random-write.7.0 rw-rr- 1 user1 user1 33554432 Apr 21 23:28 random-write.7.1 rw-rr- 1 user1 user1 33525760 Apr 21 23:28 random-write.7.2 rw-rr- 1 user1 user1 33542144 Apr 21 23:28 random-write.7.3","title":"run fio"},{"location":"QSG/tour/#unmount","text":"$ fusermount -u /tmp/daos_test1/ $ df -h -t fuse.daos df: no file systems processed","title":"unmount"},{"location":"QSG/tour/#run-with-mpirun-mdtest","text":"","title":"Run with mpirun mdtest"},{"location":"QSG/tour/#required-rpms","text":"$ sudo yum install -y mpich $ sudo yum install -y mdtest $ sudo yum install -y Lmod $ sudo module load mpi/mpich-x86_64 $ /usr/bin/touch /tmp/daos_test1/testfile","title":"required rpms"},{"location":"QSG/tour/#run-mpirun-ior-and-mdtest","text":"","title":"run mpirun ior and mdtest"},{"location":"QSG/tour/#run-mpirun-ior","text":"$ /usr/lib64/mpich/bin/mpirun -host -np 30 ior -a POSIX -b 26214400 -v -w -k -i 1 -o /tmp/daos_test1/testfile -t 25M IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O Began : Fri Apr 16 18:07:56 2021 Command line : ior -a POSIX -b 26214400 -v -w -k -i 1 -o /tmp/daos_test1/testfile -t 25M Machine : Linux boro-8.boro.hpdd.intel.com Start time skew across all tasks: 0.00 sec TestID : 0 StartTime : Fri Apr 16 18:07:56 2021 Path : /tmp/daos_test1/testfile FS : 3.8 GiB Used FS: 1.1% Inodes: 0.2 Mi Used Inodes: 0.1% Participating tasks : 30 Options: api : POSIX apiVersion : test filename : /tmp/daos_test1/testfile access : single-shared-file type : independent segments : 1 ordering in a file : sequential ordering inter file : no tasks offsets nodes : 1 tasks : 30 clients per node : 30 repetitions : 1 xfersize : 25 MiB blocksize : 25 MiB aggregate filesize : 750 MiB verbose : 1 Results: access bw(MiB/s) IOPS Latency(s) block(KiB) xfer(KiB) open(s) wr/rd(s) close(s) total(s) iter Commencing write performance test: Fri Apr 16 18:07:56 2021 write 1499.68 59.99 0.480781 25600 25600 0.300237 0.500064 0.483573 0.500107 0 Max Write: 1499.68 MiB/sec (1572.53 MB/sec) Summary of all tests: Operation Max(MiB) Min(MiB) Mean(MiB) StdDev Max(OPs) Min(OPs) Mean(OPs) StdDev Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggs(MiB) API RefNum write 1499.68 1499.68 1499.68 0.00 59.99 59.99 59.99 0.00 0.50011 NA NA 0 30 30 1 0 0 1 0 0 1 26214400 26214400 750.0 POSIX 0 Finished : Fri Apr 16 18:07:57 2021","title":"Run mpirun ior"},{"location":"QSG/tour/#run-mpirun-mdtest","text":"$ /usr/lib64/mpich/bin/mpirun -host <host1> -np 30 mdtest -a DFS -z 0 -F -C -i 1 -n 1667 -e 4096 -d / -w 4096 --dfs.chunk_size 1048576 --dfs.cont <container.uuid> --dfs.destroy --dfs.dir_oclass RP_3G1 --dfs.group daos_server --dfs.oclass RP_3G1 --dfs.pool <pool_uuid> \u2013 started at 04/16/2021 22:01:55 \u2013 mdtest-3.4.0+dev was launched with 30 total task(s) on 1 node(s) Command line used: mdtest 'a' 'DFS' '-z' '0' '-F' '-C' '-i' '1' '-n' '1667' '-e' '4096' '-d' '/' '-w' '4096' 'dfs.chunk_size' '1048576' 'dfs.cont' '3e661024-2f1f-4d7a-9cd4-1b05601e0789' 'dfs.destroy' 'dfs.dir_oclass' 'SX' 'dfs.group' 'daos_server' 'dfs.oclass' 'SX' '-dfs.pool' 'd546a7f5-586c-4d8f-aecd-372878df7b97' WARNING: unable to use realpath() on file system. Path: FS: 0.0 GiB Used FS: -nan% Inodes: 0.0 Mi Used Inodes: -nan% Nodemap: 111111111111111111111111111111 30 tasks, 50010 files SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- \u2014 \u2014 ---- ------- File creation : 14206.584 14206.334 14206.511 0.072 File stat : 0.000 0.000 0.000 0.000 File read : 0.000 0.000 0.000 0.000 File removal : 0.000 0.000 0.000 0.000 Tree creation : 1869.791 1869.791 1869.791 0.000 Tree removal : 0.000 0.000 0.000 0.000 \u2013 finished at 04/16/2021 22:01:58 \u2013 $ /usr/lib64/mpich/bin/mpirun -host <host1> -np 50 mdtest -a DFS -z 0 -F -C -i 1 -n 1667 -e 4096 -d / -w 4096 --dfs.chunk_size 1048576 --dfs.cont 3e661024-2f1f-4d7a-9cd4-1b05601e0789 --dfs.destroy --dfs.dir_oclass SX --dfs.group daos_server --dfs.oclass SX --dfs.pool d546a7f5-586c-4d8f-aecd-372878df7b97 \u2013 started at 04/16/2021 22:02:21 \u2013 mdtest-3.4.0+dev was launched with 50 total task(s) on 1 node(s) Command line used: mdtest 'a' 'DFS' '-z' '0' '-F' '-C' '-i' '1' '-n' '1667' '-e' '4096' '-d' '/' '-w' '4096' 'dfs.chunk_size' '1048576' 'dfs.cont' '3e661024-2f1f-4d7a-9cd4-1b05601e0789' 'dfs.destroy' 'dfs.dir_oclass' 'SX' 'dfs.group' 'daos_server' 'dfs.oclass' 'SX' '-dfs.pool' 'd546a7f5-586c-4d8f-aecd-372878df7b97' WARNING: unable to use realpath() on file system. Path: FS: 0.0 GiB Used FS: -nan% Inodes: 0.0 Mi Used Inodes: -nan% Nodemap: 11111111111111111111111111111111111111111111111111 50 tasks, 83350 files SUMMARY rate: (of 1 iterations) Operation Max Min Mean Std Dev --------- \u2014 \u2014 ---- ------- File creation : 13342.303 13342.093 13342.228 0.059 File stat : 0.000 0.000 0.000 0.000 File read : 0.000 0.000 0.000 0.000 File removal : 0.000 0.000 0.000 0.000 Tree creation : 1782.938 1782.938 1782.938 0.000 Tree removal : 0.000 0.000 0.000 0.000 \u2013 finished at 04/16/2021 22:02:27 \u2013","title":"Run mpirun mdtest"},{"location":"QSG/tour/#run-with-4-daos-hosts-server-rebuild-with-dfuse_io-and-mpirun","text":"","title":"Run with 4 DAOS hosts server, rebuild with dfuse_io and mpirun"},{"location":"QSG/tour/#environment-variables-setup","text":"export SERVER_NODES=node-1,node-2,node-3,node-4 export ADMIN_NODE=node-5 export CLIENT_NODE=node-5 export ALL_NODES=$SERVER_NODES,$CLIENT_NODE","title":"Environment variables setup"},{"location":"QSG/tour/#run-dfuse","text":"# Bring up 4 hosts server with appropriate daos_server.yml and # access-point, reference to DAOS Set-Up # After DAOS servers, DAOS admin and client started. $ dmg storage format Format Summary: Hosts SCM Devices NVMe Devices ----- ----------- ------------ boro-[8,35,52-53] 1 0 $ dmg pool list Pool UUID Svc Replicas --------- ------------ 733bee7b-c2af-499e-99dd-313b1ef092a9 [1-3] $ daos cont create --pool=$DAOS_POOL --type=POSIX --oclass=RP_3G1 --properties=rf:2 Successfully created container 2649aa0f-3ad7-4943-abf5-4343205a637b $ daos pool list-cont --pool=$DAOS_POOL 2649aa0f-3ad7-4943-abf5-4343205a637b $ dmg pool query --pool=$DAOS_POOL Pool 733bee7b-c2af-499e-99dd-313b1ef092a9, ntarget=32, disabled=0, leader=2, version=1 Pool space info: - Target(VOS) count:32 - SCM: Total size: 5.0 GB Free: 5.0 GB, min:156 MB, max:156 MB, mean:156 MB - NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild idle, 0 objs, 0 recs $ df -h -t fuse.daos df: no file systems processed $ mkdir /tmp/daos_test1 $ dfuse --mountpoint=/tmp/daos_test1 --pool=$DAOS_POOL --cont=$DAOS_CONT $ df -h -t fuse.daos Filesystem Size Used Avail Use% Mounted on dfuse 19G 1.1M 19G 1% /tmp/daos_test1 $ fio --name=random-write --ioengine=pvsync --rw=randwrite --bs=4k --size=128M --nrfiles=4 --directory=/tmp/daos_test1 --numjobs=8 --iodepth=16 --runtime=60 --time_based --direct=1 --buffered=0 --randrepeat=0 --norandommap --refill_buffers --group_reporting random-write: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync, iodepth=16 ... fio-3.7 Starting 8 processes random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) random-write: Laying out IO files (4 files / total 128MiB) Jobs: 8 (f=32): [w(8)][100.0%][r=0KiB/s,w=96.1MiB/s][r=0,w=24.6k IOPS][eta 00m:00s] random-write: (groupid=0, jobs=8): err= 0: pid=27879: Sat Apr 17 01:12:57 2021 write: IOPS=24.4k, BW=95.3MiB/s (99.9MB/s)(5716MiB/60001msec) clat (usec): min=220, max=6687, avg=326.19, stdev=55.29 lat (usec): min=220, max=6687, avg=326.28, stdev=55.29 clat percentiles (usec): | 1.00th=[ 260], 5.00th=[ 273], 10.00th=[ 285], 20.00th=[ 293], | 30.00th=[ 306], 40.00th=[ 314], 50.00th=[ 322], 60.00th=[ 330], | 70.00th=[ 338], 80.00th=[ 355], 90.00th=[ 375], 95.00th=[ 396], | 99.00th=[ 445], 99.50th=[ 465], 99.90th=[ 523], 99.95th=[ 562], | 99.99th=[ 1827] bw ( KiB/s): min=10976, max=12496, per=12.50%, avg=12191.82, stdev=157.87, samples=952 iops : min= 2744, max= 3124, avg=3047.92, stdev=39.47, samples=952 lat (usec) : 250=0.23%, 500=99.61%, 750=0.15%, 1000=0.01% lat (msec) : 2=0.01%, 4=0.01%, 10=0.01% cpu : usr=0.81%, sys=1.69%, ctx=1463535, majf=0, minf=308 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,1463226,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): WRITE: bw=95.3MiB/s (99.9MB/s), 95.3MiB/s-95.3MiB/s (99.9MB/s-99.9MB/s), io=5716MiB (5993MB), run=60001-60001msec","title":"Run dfuse"},{"location":"QSG/tour/#run-dfuse-with-rebuild","text":"# Start dfuse $ fio --name=random-write --ioengine=pvsync --rw=randwrite --bs=4k --size=128M --nrfiles=4 --directory=/tmp/daos_test1 --numjobs=8 --iodepth=16 --runtime=60 --time_based --direct=1 --buffered=0 --randrepeat=0 --norandommap --refill_buffers --group_reporting random-write: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=pvsync, iodepth=16 ... fio-3.7 Starting 8 processes fio: io_u error on file /tmp/daos_test1/random-write.2.1: Input/output error: write offset=8527872, buflen=4096 fio: pid=28242, err=5 file:io_u.c:1747 bw ( KiB/s): min= 3272, max=12384, per=30.14%, avg=11624.50, stdev=2181.01, samples=128 iops : min= 818, max= 3096, avg=2906.12, stdev=545.25, samples=128 lat (usec) : 250=0.23%, 500=99.59%, 750=0.12%, 1000=0.01% lat (msec) : 2=0.03%, 4=0.02% cpu : usr=0.27%, sys=0.66%, ctx=186210, majf=0, minf=494 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,186000,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): WRITE: bw=37.7MiB/s (39.5MB/s), 37.7MiB/s-37.7MiB/s (39.5MB/s-39.5MB/s), io=727MiB (762MB), run=19291-19291msec ... # from daos_admin console, stop leader-rank with debug $ dmg -d system stop --ranks=3 DEBUG 01:34:58.026753 main.go:217: debug output enabled DEBUG 01:34:58.027457 main.go:244: control config loaded from /etc/daos/daos_control.yml Rank Operation Result --------- ------ 3 stop OK $ daos pool list-cont --pool=$DAOS_POOL cf2a95ce-9910-4d5e-814c-cafb0a7f0944 $ dmg pool query --pool=$DAOS_POOL Pool 70f73efc-848e-4f6e-b4fd-909bcf9bd427, ntarget=32, disabled=8, leader=2, version=18 Pool space info: Target(VOS) count:24 SCM: Total size: 15 GB Free: 14 GB, min:575 MB, max:597 MB, mean:587 MB NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild done, 1 objs, 57 recs # Verify stopped server been evicted $ dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 boro-53.boro.hpdd.intel.com Joined 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 boro-52.boro.hpdd.intel.com Evicted system stop # Restart, after evicted server restarted, verify the server joined $ /usr/bin/dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 /boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 /boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 /boro-53.boro.hpdd.intel.com Joined 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 /boro-52.boro.hpdd.intel.com Joined # Unmount after test completed $ fusermount -u /tmp/daos_test1/ $ df -h -t fuse.daos df: no file systems processed","title":"Run dfuse with rebuild"},{"location":"QSG/tour/#run-mpirun-mdtest-with-rebuild","text":"$ dmg pool create --size=50G Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ----------------------------------------- UUID : 4eda8a8c-028c-461c-afd3-704534961572 Service Ranks : [1-3] Storage Ranks : [0-3] Total Size : 50 GB SCM : 50 GB (12 GB / rank) NVMe : 0 B (0 B / rank) $ daos cont create --pool=$DAOS_POOL --type=POSIX --oclass=RP_3G1 --properties=rf:2 Successfully created container d71ff6a5-15a5-43fe-b829-bef9c65b9ccb $ /usr/lib64/mpich/bin/mpirun -host boro-8 -np 30 mdtest -a DFS -z 0 -F -C -i 100 -n 1667 -e 4096 -d / -w 4096 --dfs.chunk_size 1048576 --dfs.cont $DAOS_CONT --dfs.destroy --dfs.dir_oclass RP_3G1 --dfs.group daos_server --dfs.oclass RP_3G1 --dfs.pool $DAOS_POOL started at 04/22/2021 17:46:20 \u2013 mdtest-3.4.0+dev was launched with 30 total task(s) on 1 node(s) Command line used: mdtest 'a' 'DFS' '-z' '0' '-F' '-C' '-i' '100' '-n' '1667' '-e' '4096' '-d' '/' '-w' '4096' 'dfs.chunk_size' '1048576' 'dfs.cont' 'd71ff6a5-15a5-43fe-b829-bef9c65b9ccb' 'dfs.destroy' 'dfs.dir_oclass' 'RP_3G1' 'dfs.group' 'daos_server' 'dfs.oclass' 'RP_3G1' '-dfs.pool' '4eda8a8c-028c-461c-afd3-704534961572' WARNING: unable to use realpath() on file system. Path: FS: 0.0 GiB Used FS: -nan% Inodes: 0.0 Mi Used Inodes: -nan% Nodemap: 111111111111111111111111111111 30 tasks, 50010 files ... # from daos_admin console, stop a server rank $ dmg system stop --ranks=2 Rank Operation Result --------- ------ 2 stop OK # Verify stopped server been evicted $ dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 boro-53.boro.hpdd.intel.com Evicted system stop 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 boro-52.boro.hpdd.intel.com Joined # Restart, after evicted server restarted, verify the server joined $ /usr/bin/dmg system query -v Rank UUID Control Address Fault Domain State Reason ---- --------------- ------------ ----- ------ 0 2bf0e083-33d6-4ce3-83c4-c898c2a7ddbd 10.7.1.8:10001 /boro-8.boro.hpdd.intel.com Joined 1 c9ac1dd9-0f9d-4684-90d3-038b720fd26b 10.7.1.35:10001 /boro-35.boro.hpdd.intel.com Joined 2 80e44fe9-3a2b-4808-9a0f-88c3cbe7f565 10.7.1.53:10001 /boro-53.boro.hpdd.intel.com Joined 3 a26fd44a-6089-4cc3-a06b-278a85607fd3 10.7.1.52:10001 /boro-52.boro.hpdd.intel.com Joined","title":"Run mpirun mdtest with rebuild"},{"location":"QSG/tour/#clean-up","text":"# pool reintegrate $ dmg pool reintegrate --pool=$DAOS_POOL --rank=2 Reintegration command succeeded # destroy container $ daos container destroy --pool=$DAOS_POOL --cont=$DAOS_CONT # destroy pool $ dmg pool destroy --pool=$DAOS_POOL Pool-destroy command succeeded # stop clients $ pdsh -S -w $CLIENT_NODES \"sudo systemctl stop daos_agent.service\" # disable clients $ pdsh -S -w $CLIENT_NODES \"sudo systemctl disable daos_agent.service\" # stop servers $ pdsh -S -w $SERVER_NODES \"sudo systemctl stop daos_server.service\" # disable servers $ pdsh -S -w $SERVER_NODES \"sudo systemctl disable daos_server.service\"","title":"Clean-Up"},{"location":"admin/","text":"DAOS Administrator Guide \u00b6","title":"DAOS Administrator Guide"},{"location":"admin/#daos-administrator-guide","text":"","title":"DAOS Administrator Guide"},{"location":"admin/administration/","text":"DAOS System Administration \u00b6 System RAS Events \u00b6 Reliability, Availability and Serviceability (RAS) related events are communicated and logged within DAOS. RAS Event Structure \u00b6 The following table describes the structure of a DAOS RAS event including descriptions of mandatory and optional fields. Field Optional/Mandatory Description ID Mandatory Unique event identifier referenced in the manual. Type Mandatory Event type of STATE_CHANGE causes an update to the Management Service (MS) database in addition to event being written to SYSLOG. INFO_ONLY type events are only written to SYSLOG. Timestamp Mandatory Resolution at the microseconds and include the timezone offset to avoid locality issues. Severity Mandatory Indicates event severity, Error/Warning/Notice. Msg Mandatory Human readable message. HID Optional Identify hardware component involved in the event. E.g. PCI address for SSD, network interface Rank Optional DAOS rank involved in the event. PID Optional Identifier of the process involved in the RAS event TID Optional Identifier of the thread involved in the RAS event. JOBID Optional Identifier of the job involved in the RAS event. Hostname Optional Hostname of the node involved in the event. PUUID Optional Pool UUID involved in the event, if any. CUUID Optional Container UUID involved in the event, if relevant. OID Optional Object identifier involved in the event, if relevant. Control Operation Optional Recommended automatic action, if any. Data Optional Specific instance data treated as a blob. RAS Event IDs \u00b6 The following table lists supported DAOS RAS events including IDs, type, severity, message, description and cause. Event Event type Severity Message Description Cause engine_format required INFO_ONLY NOTICE DAOS engine <idx> requires a <type> format Indicates engine is waiting for allocated storage to be formatted on formatted on instance <idx> with dmg tool. <type> can be either SCM or Metadata. DAOS server attempts to bring-up an engine which has unformatted storage. engine_died STATE_CHANGE ERROR DAOS engine <idx> exited exited unexpectedly: <error> Indicates engine instance <idx> unexpectedly. describes the exit state returned from exited daos_engine process. N/A engine_asserted STATE_CHANGE ERROR TBD Indicates engine instance threw a runtime assertion, causing a crash. An unexpected internal state resulted in assert failure. engine_clock_drift INFO_ONLY ERROR clock drift detected Indicates CART comms layer has detected clock skew between engines. NTP may not be syncing clocks across DAOS system. pool_rebuild_started INFO_ONLY NOTICE Pool rebuild started. Indicates a pool rebuild has started. Event data field contains pool map version and pool operation identifier. When a pool rank becomes unavailable a rebuild will be triggered. pool_rebuild_finished INFO_ONLY NOTICE Pool rebuild finished. Indicates a pool rebuild has finished successfully. Event data field includes the pool map version and pool operation identifier. N/A pool_rebuild_failed INFO_ONLY ERROR Pool rebuild failed: <rc>. Indicates a pool rebuild has failed. Event data field includes the pool map version and pool operation identifier. <rc> provides a string representation of DER code. N/A pool_replicas_updated STATE_CHANGE NOTICE List of pool service replica ranks has been updated. Indicates a pool service replica list has changed. The event contains the new service replica list in a custom payload. When a pool service replica rank becomes unavailable a new rank is selected to replace it (if available). pool_durable_format_incompat INFO_ONLY ERROR incompatible layout version: <current> not in [<min>, <max>] Indicates the given pool's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with pool data in local storage that has an incompatible layout version. container_durable_format_incompat INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>] Indicates the given container's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with container data in local storage that has an incompatible layout version. rdb_durable_format_incompatible INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>]] Indicates the given rdb's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with rdb data in local storage that has an incompatible layout version. swim_rank_alive STATE_CHANGE NOTICE TBD The SWIM protocol has detected the specified rank is responsive. A remote DAOS engine has become responsive. swim_rank_dead STATE_CHANGE NOTICE SWIM rank marked as dead. The SWIM protocol has detected the specified rank is unresponsive. A remote DAOS engine has become unresponsive. system_start_failed INFO_ONLY ERROR System startup failed, <errors> Indicates that a user initiated controlled startup failed. <errors> shows which ranks failed. Ranks failed to start. system_stop_failed INFO_ONLY ERROR System shutdown failed during <action> action, <errors> Indicates that a user initiated controlled shutdown failed. <action> identifies the failing shutdown action and <errors> shows which ranks failed. Ranks failed to stop. System Monitoring \u00b6 System monitoring and telemetry data will be provided as part of the control plane and will be documented in a future revision. Storage Operations \u00b6 Space Utilization \u00b6 To query SCM and NVMe storage space usage and show how much space is available to create new DAOS pools with, run the following command: bash-4.2$ dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- wolf-71 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % wolf-72 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % The command output shows online DAOS storage utilization, only including storage statistics for devices that have been formatted by DAOS control-plane and assigned to a currently running rank of the DAOS system. This represents the storage that can host DAOS pools. Note that the table values are per-host (storage server) and SCM/NVMe capacity pool component values specified in dmg pool create are per rank. If multiple ranks (I/O processes) have been configured per host in the server configuration file daos_server.yml then the values supplied to dmg pool create should be a maximum of the SCM/NVMe free space divided by the number of ranks per host. For example if 2.0 TB SCM and 10.0 TB NVMe free space is reported by dmg storage query usage and the server configuration file used to start the system specifies 2 I/O processes (2 \"server\" sections), the maximum pool size that can be specified is approximately dmg pool create -s 1T -n 5T (may need to specify slightly below the maximum to take account of negligible metadata overhead). Storage Scrubbing \u00b6 Support for end-to-end data integrity is planned for DAOS v1.2 and background checksum scrubbing for v2.2. Once available, that functionality will be documented here. SSD Management \u00b6 Health Monitoring \u00b6 Useful admin dmg commands to query NVMe SSD health: Query Per-Server Metadata: dmg storage query (list-devices|list-pools) dmg storage scan --nvme-meta shows mapping of metadata to NVMe controllers The NVMe storage query list-devices and list-pools commands query the persistently stored SMD device and pool tables respectively. The device table maps the internal device UUID to attached VOS target IDs. The rank number of the server where the device is located is also listed, along with the current device state. The current device states are the following: - NORMAL: a fully, functional device in-use by DAOS - EVICTED: the device is no longer in-use by DAOS - UNPLUGGED: the device is currently unplugged from the system (may or not be evicted) - NEW: the device is plugged and available, and not currently in-use by DAOS The transport address is also listed for the device. This is either the PCIe address for normal NVMe SSDs, or the BDF format address of the backing NVMe SSDs behind a VMD (Volume Management Device) address. In the example below, the last two listed devices are both VMD devices with transport addresses in the BDF format behind the VMD address 0000:5d:05.5. The pool table maps the DAOS pool UUID to attached VOS target IDs and will list all of the server ranks that the pool is distributed on. With the additional verbose flag, the mapping of SPDK blob IDs to VOS target IDs will also be displayed. $ dmg -l boro-11,boro-13 storage query list-devices ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 2] Rank:0 State:NORMAL UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b [TrAddr:0000:8b:00.0] Targets:[1 3] Rank:0 State:NORMAL UUID:051b77e4-1524-4662-9f32-f8e4d2542c2d [TrAddr:0000:8c:00.0] Targets:[] Rank:0 State:NEW UUID:81905b24-be44-4106-8ff9-03002e9dd86a [TrAddr:5d0505:01:00.0] Targets:[0 2] Rank:1 State:EVICTED UUID:2ccb8afb-5d32-454e-86e3-762ec5dca7be [TrAddr:5d0505:03:00.0] Targets:[1 3] Rank:1 State:NORMAL $ dmg -l boro-11,boro-13 storage query list-pools ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Rank:1 Targets:[0 1 2 3] $ dmg -l boro-11,boro-13 storage query list-pools --verbose ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Blobs:[4294967404 4294967405 4294967407 4294967406] Rank:1 Targets:[0 1 2 3] Blobs:[4294967410 4294967411 4294967413 4294967412] Query Storage Device Health Data: dmg storage query (device-health|target-health) dmg storage scan --nvme-health shows NVMe controller health stats The NVMe storage query device-health and target-health commands query the device health data, including NVMe SSD health stats and in-memory I/O error and checksum error counters. The server rank and device state are also listed. The device health data can either be queried by device UUID (device-health command) or by VOS target ID along with the server rank (target-health command). The same device health information is displayed with both command options. $ dmg -l boro-11 storage query device-health --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 or $ dmg -l boro-11 storage query target-health --rank=0 --tgtid=0 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 1 2 3] Rank:0 State:NORMAL Health Stats: Temperature:289K(15C) Controller Busy Time:0s Power Cycles:0 Power On Duration:0s Unsafe Shutdowns:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK Eviction and Hotplug \u00b6 Manually Evict an NVMe SSD: dmg storage set nvme-faulty To manually evict an NVMe SSD (auto eviction will be supported in a future release), the device state needs to be set to \"FAULTY\" by running the following command: $ dmg -l boro-11 storage set nvme-faulty --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:FAULTY The device state will transition from \"NORMAL\" to \"FAULTY\" (shown above), which will trigger the faulty device reaction (all targets on the SSD will be rebuilt and the SSD will remain evicted until device replacement occurs). Note Full NVMe hot plug capability will be available and supported in DAOS 2.2 release. Use is currently intended for testing only and is not supported for production. Replace an Evicted SSD with a New Device: dmg storage replace nvme To replace an NVMe SSD with an evicted device and reintegrate it into use with DAOS, run the following command: $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=80c9f1be-84b9-4318-a1be-c416c96ca48b ------- boro-11 ------- Devices UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b Targets:[] Rank:1 State:NORMAL The old, now replaced device will remain in an \"EVICTED\" state until it is unplugged. The new device will transition from a \"NEW\" state to a \"NORMAL\" state (shown above). Reuse a FAULTY Device: dmg storage replace nvme In order to reuse a device that was previously set as FAULTY and evicted from the DAOS system, an admin can run the following command (setting the old device UUID to be the new device UUID): $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:NORMAL The FAULTY device will transition from an \"EVICTED\" state back to a \"NORMAL\" state, and will again be available for use with DAOS. The use case of this command will mainly be for testing, or for accidental device eviction. Identification \u00b6 The SSD identification feature is simply a way to quickly and visually locate a device. It requires the use of Intel VMD (Volume Management Device), which needs to be physically available on the hardware as well as enabled in the system BIOS. The feature supports two LED device events: locating a healthy device and locating an evicted device. Locate a Healthy SSD: dmg storage identify vmd To quickly identify an SSD in question, an administrator can run the following command: $ dmg -l boro-11 storage identify vmd --uuid=6fccb374-413b-441a-bfbe-860099ac5e8d If a non-VMD device UUID is used with the command, the following error will occur: localhost DAOS error (-1010): DER_NOSYS The status LED on the VMD device is now set to an \"IDENTIFY\" state, represented by a quick, 4Hz blinking amber light. The device will quickly blink by default for about 60 seconds and then return to the default \"OFF\" state. The LED event duration can be customized by setting the VMD_LED_PERIOD environment variable if a duration other than the default value is desired. Locate an Evicted SSD: If an NVMe SSD is evicted, the status LED on the VMD device is set to a \"FAULT\" state, represented by a solidly ON amber light. No additional command apart from the SSD eviction command would be needed, and this would visually indicate that the device needs to be replaced and is no longer in use by DAOS. The LED of the VMD device would remain in this state until replaced by a new device. System Operations \u00b6 The DAOS Control Server acting as the access point records details of DAOS I/O Server instances that join the DAOS system. Once an I/O Engine has joined the DAOS system, it is identified by a unique system \"rank\". Multiple ranks can reside on the same host machine, accessible via the same network address. A DAOS system can be shutdown and restarted to perform maintenance and/or reboot hosts. Pool data and state will be maintained providing no changes are made to the rank's metadata stored on persistent memory. Storage reformat can also be performed after system shutdown. Pools will be removed and storage wiped. System commands will be handled by the DAOS Server listening at the access point address specified as the first entry in the DMG config file \"hostlist\" parameter. See daos_control.yml for details. The \"access point\" address should be the same as that specified in the server config file daos_server.yml specified when starting daos_server instances. Warning Controlled start/stop/reformat have some known limitations. Whilst individual system instances can be stopped, if a subset is restarted, existing pools will not be automatically integrated with restarted instances. Membership \u00b6 The system membership can be queried using the command: $ dmg system query [--verbose] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] --verbose flag gives more information on each rank Output table will provide system rank mappings to host address and instance UUID, in addition to rank state. Shutdown \u00b6 When up and running, the entire system can be shutdown with the command: $ dmg system stop [--force] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS Control Servers will continue to operate and listen on the management network. Start \u00b6 To start the system after a controlled shutdown run the command: $ dmg system start [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS I/O Engines will be started. Reformat \u00b6 To reformat the system after a controlled shutdown run the command: $ dmg storage format --reformat --reformat flag indicates that a reformat operation should be performed disregarding existing filesystems if no record of previously running ranks can be found, reformat is performed on hosts in dmg config file hostlist if system membership has records of previously running ranks, storage allocated to those ranks will be formatted Output table will indicate action and result. DAOS I/O Engines will be started and all DAOS pools will have been removed. Manual Fresh Start \u00b6 To reset the DAOS metadata across all hosts, the system must be reformatted. First, ensure all daos_server processes on all hosts have been stopped, then for each SCM mount specified in the config file ( scm_mount in the servers section) umount and wipe FS signatures. Example illustration with two IO instances specified in the config file: clush -w wolf-[118-121,130-133] umount /mnt/daos1 clush -w wolf-[118-121,130-133] umount /mnt/daos0 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem1 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem0 Then restart DAOS Servers and format. Fault Domain \u00b6 Details on how to drain an individual storage node or fault domain (e.g. rack) in preparation for maintenance activity and how to reintegrate it will be provided in a future revision. System Extension \u00b6 Ability to add new DAOS server instances to a pre-existing DAOS system will be documented in a future revision. Fault Management \u00b6 DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. Fault Detection & Isolation \u00b6 DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM 1 that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure makes an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Once detected, the faulty target or servers (effectively a set of targets) must be excluded from each pool membership. This process is triggered either manually by the administrator or automatically (see the next section for more information). Upon exclusion from the pool map, each target starts the collective rebuild process automatically to restore data redundancy. The rebuild process is designed to operate online while servers continue to process incoming I/O operations from applications. Tools to monitor and manage rebuild are still under development. Rebuild Throttling \u00b6 The rebuild process may consume many resources on each server and can be throttled to reduce the impact on application performance. This current logic relies on CPU cycles on the storage nodes. By default, the rebuild process is configured to consume up to 30% of the CPU cycles, leaving the other 70% for regular I/O operations. During the rebuild process, the user can set the throttle to guarantee that the rebuild will not use more resources than the user setting. The user can only set the CPU cycle for now. For example, if the user set the throttle to 50, then the rebuild will at most use 50% of the CPU cycle to do the rebuild job. The default rebuild throttle for CPU cycle is 30. This parameter can be changed via the daos_mgmt_set_params() API call and will be eventually available through the management tools. Software Upgrade \u00b6 Interoperability in DAOS is handled via protocol and schema versioning for persistent data structures. Further instructions on how to manage DAOS software upgrades will be provided in a future revision. Protocol Interoperability \u00b6 Limited protocol interoperability is provided by the DAOS storage stack. Version compatibility checks will be performed to verify that: All targets in the same pool run the same protocol version. Client libraries linked with the application may be up to one protocol version older than the targets. If a protocol version mismatch is detected among storage targets in the same pool, the entire DAOS system will fail to start up and will report failure to the control API. Similarly, the connection from clients running a protocol version incompatible with the targets will return an error. Persistent Layout \u00b6 The schema of persistent data structures may evolve from time to time to fix bugs, add new optimizations, or support new features. To that end, the persistent data structures support schema versioning. Upgrading the schema version will not be performed automatically and must be initiated by the administrator. A dedicated upgrade tool will be provided to upgrade the schema version to the latest one. All targets in the same pool must have the same schema version. Version checks are performed at system initialization time to enforce this constraint. To limit the validation matrix, each new DAOS release will be published with a list of supported schema versions. To run with the new DAOS release, administrators will then need to upgrade the DAOS system to one of the supported schema versions. New pool shards will always be formatted with the latest version. This versioning schema only applies to a data structure stored in persistent memory and not to block storage that only stores user data with no metadata. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028914 \u21a9","title":"System Administration"},{"location":"admin/administration/#daos-system-administration","text":"","title":"DAOS System Administration"},{"location":"admin/administration/#system-ras-events","text":"Reliability, Availability and Serviceability (RAS) related events are communicated and logged within DAOS.","title":"System RAS Events"},{"location":"admin/administration/#ras-event-structure","text":"The following table describes the structure of a DAOS RAS event including descriptions of mandatory and optional fields. Field Optional/Mandatory Description ID Mandatory Unique event identifier referenced in the manual. Type Mandatory Event type of STATE_CHANGE causes an update to the Management Service (MS) database in addition to event being written to SYSLOG. INFO_ONLY type events are only written to SYSLOG. Timestamp Mandatory Resolution at the microseconds and include the timezone offset to avoid locality issues. Severity Mandatory Indicates event severity, Error/Warning/Notice. Msg Mandatory Human readable message. HID Optional Identify hardware component involved in the event. E.g. PCI address for SSD, network interface Rank Optional DAOS rank involved in the event. PID Optional Identifier of the process involved in the RAS event TID Optional Identifier of the thread involved in the RAS event. JOBID Optional Identifier of the job involved in the RAS event. Hostname Optional Hostname of the node involved in the event. PUUID Optional Pool UUID involved in the event, if any. CUUID Optional Container UUID involved in the event, if relevant. OID Optional Object identifier involved in the event, if relevant. Control Operation Optional Recommended automatic action, if any. Data Optional Specific instance data treated as a blob.","title":"RAS Event Structure"},{"location":"admin/administration/#ras-event-ids","text":"The following table lists supported DAOS RAS events including IDs, type, severity, message, description and cause. Event Event type Severity Message Description Cause engine_format required INFO_ONLY NOTICE DAOS engine <idx> requires a <type> format Indicates engine is waiting for allocated storage to be formatted on formatted on instance <idx> with dmg tool. <type> can be either SCM or Metadata. DAOS server attempts to bring-up an engine which has unformatted storage. engine_died STATE_CHANGE ERROR DAOS engine <idx> exited exited unexpectedly: <error> Indicates engine instance <idx> unexpectedly. describes the exit state returned from exited daos_engine process. N/A engine_asserted STATE_CHANGE ERROR TBD Indicates engine instance threw a runtime assertion, causing a crash. An unexpected internal state resulted in assert failure. engine_clock_drift INFO_ONLY ERROR clock drift detected Indicates CART comms layer has detected clock skew between engines. NTP may not be syncing clocks across DAOS system. pool_rebuild_started INFO_ONLY NOTICE Pool rebuild started. Indicates a pool rebuild has started. Event data field contains pool map version and pool operation identifier. When a pool rank becomes unavailable a rebuild will be triggered. pool_rebuild_finished INFO_ONLY NOTICE Pool rebuild finished. Indicates a pool rebuild has finished successfully. Event data field includes the pool map version and pool operation identifier. N/A pool_rebuild_failed INFO_ONLY ERROR Pool rebuild failed: <rc>. Indicates a pool rebuild has failed. Event data field includes the pool map version and pool operation identifier. <rc> provides a string representation of DER code. N/A pool_replicas_updated STATE_CHANGE NOTICE List of pool service replica ranks has been updated. Indicates a pool service replica list has changed. The event contains the new service replica list in a custom payload. When a pool service replica rank becomes unavailable a new rank is selected to replace it (if available). pool_durable_format_incompat INFO_ONLY ERROR incompatible layout version: <current> not in [<min>, <max>] Indicates the given pool's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with pool data in local storage that has an incompatible layout version. container_durable_format_incompat INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>] Indicates the given container's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with container data in local storage that has an incompatible layout version. rdb_durable_format_incompatible INFO_ONLY ERROR incompatible layout version[: <current> not in [<min>, <max>]] Indicates the given rdb's layout version does not match any of the versions supported by the currently running DAOS software. DAOS engine is started with rdb data in local storage that has an incompatible layout version. swim_rank_alive STATE_CHANGE NOTICE TBD The SWIM protocol has detected the specified rank is responsive. A remote DAOS engine has become responsive. swim_rank_dead STATE_CHANGE NOTICE SWIM rank marked as dead. The SWIM protocol has detected the specified rank is unresponsive. A remote DAOS engine has become unresponsive. system_start_failed INFO_ONLY ERROR System startup failed, <errors> Indicates that a user initiated controlled startup failed. <errors> shows which ranks failed. Ranks failed to start. system_stop_failed INFO_ONLY ERROR System shutdown failed during <action> action, <errors> Indicates that a user initiated controlled shutdown failed. <action> identifies the failing shutdown action and <errors> shows which ranks failed. Ranks failed to stop.","title":"RAS Event IDs"},{"location":"admin/administration/#system-monitoring","text":"System monitoring and telemetry data will be provided as part of the control plane and will be documented in a future revision.","title":"System Monitoring"},{"location":"admin/administration/#storage-operations","text":"","title":"Storage Operations"},{"location":"admin/administration/#space-utilization","text":"To query SCM and NVMe storage space usage and show how much space is available to create new DAOS pools with, run the following command: bash-4.2$ dmg storage query usage Hosts SCM-Total SCM-Free SCM-Used NVMe-Total NVMe-Free NVMe-Used ----- --------- -------- -------- ---------- --------- --------- wolf-71 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % wolf-72 6.4 TB 2.0 TB 68 % 1.5 TB 1.1 TB 27 % The command output shows online DAOS storage utilization, only including storage statistics for devices that have been formatted by DAOS control-plane and assigned to a currently running rank of the DAOS system. This represents the storage that can host DAOS pools. Note that the table values are per-host (storage server) and SCM/NVMe capacity pool component values specified in dmg pool create are per rank. If multiple ranks (I/O processes) have been configured per host in the server configuration file daos_server.yml then the values supplied to dmg pool create should be a maximum of the SCM/NVMe free space divided by the number of ranks per host. For example if 2.0 TB SCM and 10.0 TB NVMe free space is reported by dmg storage query usage and the server configuration file used to start the system specifies 2 I/O processes (2 \"server\" sections), the maximum pool size that can be specified is approximately dmg pool create -s 1T -n 5T (may need to specify slightly below the maximum to take account of negligible metadata overhead).","title":"Space Utilization"},{"location":"admin/administration/#storage-scrubbing","text":"Support for end-to-end data integrity is planned for DAOS v1.2 and background checksum scrubbing for v2.2. Once available, that functionality will be documented here.","title":"Storage Scrubbing"},{"location":"admin/administration/#ssd-management","text":"","title":"SSD Management"},{"location":"admin/administration/#health-monitoring","text":"Useful admin dmg commands to query NVMe SSD health: Query Per-Server Metadata: dmg storage query (list-devices|list-pools) dmg storage scan --nvme-meta shows mapping of metadata to NVMe controllers The NVMe storage query list-devices and list-pools commands query the persistently stored SMD device and pool tables respectively. The device table maps the internal device UUID to attached VOS target IDs. The rank number of the server where the device is located is also listed, along with the current device state. The current device states are the following: - NORMAL: a fully, functional device in-use by DAOS - EVICTED: the device is no longer in-use by DAOS - UNPLUGGED: the device is currently unplugged from the system (may or not be evicted) - NEW: the device is plugged and available, and not currently in-use by DAOS The transport address is also listed for the device. This is either the PCIe address for normal NVMe SSDs, or the BDF format address of the backing NVMe SSDs behind a VMD (Volume Management Device) address. In the example below, the last two listed devices are both VMD devices with transport addresses in the BDF format behind the VMD address 0000:5d:05.5. The pool table maps the DAOS pool UUID to attached VOS target IDs and will list all of the server ranks that the pool is distributed on. With the additional verbose flag, the mapping of SPDK blob IDs to VOS target IDs will also be displayed. $ dmg -l boro-11,boro-13 storage query list-devices ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 2] Rank:0 State:NORMAL UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b [TrAddr:0000:8b:00.0] Targets:[1 3] Rank:0 State:NORMAL UUID:051b77e4-1524-4662-9f32-f8e4d2542c2d [TrAddr:0000:8c:00.0] Targets:[] Rank:0 State:NEW UUID:81905b24-be44-4106-8ff9-03002e9dd86a [TrAddr:5d0505:01:00.0] Targets:[0 2] Rank:1 State:EVICTED UUID:2ccb8afb-5d32-454e-86e3-762ec5dca7be [TrAddr:5d0505:03:00.0] Targets:[1 3] Rank:1 State:NORMAL $ dmg -l boro-11,boro-13 storage query list-pools ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Rank:1 Targets:[0 1 2 3] $ dmg -l boro-11,boro-13 storage query list-pools --verbose ------- boro-11 ------- Pools UUID:08d6839b-c71a-4af6-901c-28e141b2b429 Rank:0 Targets:[0 1 2 3] Blobs:[4294967404 4294967405 4294967407 4294967406] Rank:1 Targets:[0 1 2 3] Blobs:[4294967410 4294967411 4294967413 4294967412] Query Storage Device Health Data: dmg storage query (device-health|target-health) dmg storage scan --nvme-health shows NVMe controller health stats The NVMe storage query device-health and target-health commands query the device health data, including NVMe SSD health stats and in-memory I/O error and checksum error counters. The server rank and device state are also listed. The device health data can either be queried by device UUID (device-health command) or by VOS target ID along with the server rank (target-health command). The same device health information is displayed with both command options. $ dmg -l boro-11 storage query device-health --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 or $ dmg -l boro-11 storage query target-health --rank=0 --tgtid=0 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 [TrAddr:0000:8a:00.0] Targets:[0 1 2 3] Rank:0 State:NORMAL Health Stats: Temperature:289K(15C) Controller Busy Time:0s Power Cycles:0 Power On Duration:0s Unsafe Shutdowns:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK","title":"Health Monitoring"},{"location":"admin/administration/#eviction-and-hotplug","text":"Manually Evict an NVMe SSD: dmg storage set nvme-faulty To manually evict an NVMe SSD (auto eviction will be supported in a future release), the device state needs to be set to \"FAULTY\" by running the following command: $ dmg -l boro-11 storage set nvme-faulty --uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:FAULTY The device state will transition from \"NORMAL\" to \"FAULTY\" (shown above), which will trigger the faulty device reaction (all targets on the SSD will be rebuilt and the SSD will remain evicted until device replacement occurs). Note Full NVMe hot plug capability will be available and supported in DAOS 2.2 release. Use is currently intended for testing only and is not supported for production. Replace an Evicted SSD with a New Device: dmg storage replace nvme To replace an NVMe SSD with an evicted device and reintegrate it into use with DAOS, run the following command: $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=80c9f1be-84b9-4318-a1be-c416c96ca48b ------- boro-11 ------- Devices UUID:80c9f1be-84b9-4318-a1be-c416c96ca48b Targets:[] Rank:1 State:NORMAL The old, now replaced device will remain in an \"EVICTED\" state until it is unplugged. The new device will transition from a \"NEW\" state to a \"NORMAL\" state (shown above). Reuse a FAULTY Device: dmg storage replace nvme In order to reuse a device that was previously set as FAULTY and evicted from the DAOS system, an admin can run the following command (setting the old device UUID to be the new device UUID): $ dmg -l boro-11 storage replace nvme --old-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 --new-uuid=5bd91603-d3c7-4fb7-9a71-76bc25690c19 ------- boro-11 ------- Devices UUID:5bd91603-d3c7-4fb7-9a71-76bc25690c19 Targets:[] Rank:1 State:NORMAL The FAULTY device will transition from an \"EVICTED\" state back to a \"NORMAL\" state, and will again be available for use with DAOS. The use case of this command will mainly be for testing, or for accidental device eviction.","title":"Eviction and Hotplug"},{"location":"admin/administration/#identification","text":"The SSD identification feature is simply a way to quickly and visually locate a device. It requires the use of Intel VMD (Volume Management Device), which needs to be physically available on the hardware as well as enabled in the system BIOS. The feature supports two LED device events: locating a healthy device and locating an evicted device. Locate a Healthy SSD: dmg storage identify vmd To quickly identify an SSD in question, an administrator can run the following command: $ dmg -l boro-11 storage identify vmd --uuid=6fccb374-413b-441a-bfbe-860099ac5e8d If a non-VMD device UUID is used with the command, the following error will occur: localhost DAOS error (-1010): DER_NOSYS The status LED on the VMD device is now set to an \"IDENTIFY\" state, represented by a quick, 4Hz blinking amber light. The device will quickly blink by default for about 60 seconds and then return to the default \"OFF\" state. The LED event duration can be customized by setting the VMD_LED_PERIOD environment variable if a duration other than the default value is desired. Locate an Evicted SSD: If an NVMe SSD is evicted, the status LED on the VMD device is set to a \"FAULT\" state, represented by a solidly ON amber light. No additional command apart from the SSD eviction command would be needed, and this would visually indicate that the device needs to be replaced and is no longer in use by DAOS. The LED of the VMD device would remain in this state until replaced by a new device.","title":"Identification"},{"location":"admin/administration/#system-operations","text":"The DAOS Control Server acting as the access point records details of DAOS I/O Server instances that join the DAOS system. Once an I/O Engine has joined the DAOS system, it is identified by a unique system \"rank\". Multiple ranks can reside on the same host machine, accessible via the same network address. A DAOS system can be shutdown and restarted to perform maintenance and/or reboot hosts. Pool data and state will be maintained providing no changes are made to the rank's metadata stored on persistent memory. Storage reformat can also be performed after system shutdown. Pools will be removed and storage wiped. System commands will be handled by the DAOS Server listening at the access point address specified as the first entry in the DMG config file \"hostlist\" parameter. See daos_control.yml for details. The \"access point\" address should be the same as that specified in the server config file daos_server.yml specified when starting daos_server instances. Warning Controlled start/stop/reformat have some known limitations. Whilst individual system instances can be stopped, if a subset is restarted, existing pools will not be automatically integrated with restarted instances.","title":"System Operations"},{"location":"admin/administration/#membership","text":"The system membership can be queried using the command: $ dmg system query [--verbose] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] --verbose flag gives more information on each rank Output table will provide system rank mappings to host address and instance UUID, in addition to rank state.","title":"Membership"},{"location":"admin/administration/#shutdown","text":"When up and running, the entire system can be shutdown with the command: $ dmg system stop [--force] [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS Control Servers will continue to operate and listen on the management network.","title":"Shutdown"},{"location":"admin/administration/#start","text":"To start the system after a controlled shutdown run the command: $ dmg system start [--ranks <rankset>|--host-ranks <hostset>] <rankset> is a pattern describing rank ranges e.g. 0,5-10,20-100 <hostset> is a pattern describing host ranges e.g. storagehost[0,5-10],10.8.1.[20-100] Output table will indicate action and result. DAOS I/O Engines will be started.","title":"Start"},{"location":"admin/administration/#reformat","text":"To reformat the system after a controlled shutdown run the command: $ dmg storage format --reformat --reformat flag indicates that a reformat operation should be performed disregarding existing filesystems if no record of previously running ranks can be found, reformat is performed on hosts in dmg config file hostlist if system membership has records of previously running ranks, storage allocated to those ranks will be formatted Output table will indicate action and result. DAOS I/O Engines will be started and all DAOS pools will have been removed.","title":"Reformat"},{"location":"admin/administration/#manual-fresh-start","text":"To reset the DAOS metadata across all hosts, the system must be reformatted. First, ensure all daos_server processes on all hosts have been stopped, then for each SCM mount specified in the config file ( scm_mount in the servers section) umount and wipe FS signatures. Example illustration with two IO instances specified in the config file: clush -w wolf-[118-121,130-133] umount /mnt/daos1 clush -w wolf-[118-121,130-133] umount /mnt/daos0 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem1 clush -w wolf-[118-121,130-133] wipefs -a /dev/pmem0 Then restart DAOS Servers and format.","title":"Manual Fresh Start"},{"location":"admin/administration/#fault-domain","text":"Details on how to drain an individual storage node or fault domain (e.g. rack) in preparation for maintenance activity and how to reintegrate it will be provided in a future revision.","title":"Fault Domain"},{"location":"admin/administration/#system-extension","text":"Ability to add new DAOS server instances to a pre-existing DAOS system will be documented in a future revision.","title":"System Extension"},{"location":"admin/administration/#fault-management","text":"DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains.","title":"Fault Management"},{"location":"admin/administration/#fault-detection-isolation","text":"DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM 1 that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure makes an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Once detected, the faulty target or servers (effectively a set of targets) must be excluded from each pool membership. This process is triggered either manually by the administrator or automatically (see the next section for more information). Upon exclusion from the pool map, each target starts the collective rebuild process automatically to restore data redundancy. The rebuild process is designed to operate online while servers continue to process incoming I/O operations from applications. Tools to monitor and manage rebuild are still under development.","title":"Fault Detection &amp; Isolation"},{"location":"admin/administration/#rebuild-throttling","text":"The rebuild process may consume many resources on each server and can be throttled to reduce the impact on application performance. This current logic relies on CPU cycles on the storage nodes. By default, the rebuild process is configured to consume up to 30% of the CPU cycles, leaving the other 70% for regular I/O operations. During the rebuild process, the user can set the throttle to guarantee that the rebuild will not use more resources than the user setting. The user can only set the CPU cycle for now. For example, if the user set the throttle to 50, then the rebuild will at most use 50% of the CPU cycle to do the rebuild job. The default rebuild throttle for CPU cycle is 30. This parameter can be changed via the daos_mgmt_set_params() API call and will be eventually available through the management tools.","title":"Rebuild Throttling"},{"location":"admin/administration/#software-upgrade","text":"Interoperability in DAOS is handled via protocol and schema versioning for persistent data structures. Further instructions on how to manage DAOS software upgrades will be provided in a future revision.","title":"Software Upgrade"},{"location":"admin/administration/#protocol-interoperability","text":"Limited protocol interoperability is provided by the DAOS storage stack. Version compatibility checks will be performed to verify that: All targets in the same pool run the same protocol version. Client libraries linked with the application may be up to one protocol version older than the targets. If a protocol version mismatch is detected among storage targets in the same pool, the entire DAOS system will fail to start up and will report failure to the control API. Similarly, the connection from clients running a protocol version incompatible with the targets will return an error.","title":"Protocol Interoperability"},{"location":"admin/administration/#persistent-layout","text":"The schema of persistent data structures may evolve from time to time to fix bugs, add new optimizations, or support new features. To that end, the persistent data structures support schema versioning. Upgrading the schema version will not be performed automatically and must be initiated by the administrator. A dedicated upgrade tool will be provided to upgrade the schema version to the latest one. All targets in the same pool must have the same schema version. Version checks are performed at system initialization time to enforce this constraint. To limit the validation matrix, each new DAOS release will be published with a list of supported schema versions. To run with the new DAOS release, administrators will then need to upgrade the DAOS system to one of the supported schema versions. New pool shards will always be formatted with the latest version. This versioning schema only applies to a data structure stored in persistent memory and not to block storage that only stores user data with no metadata. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028914 \u21a9","title":"Persistent Layout"},{"location":"admin/deployment/","text":"System Deployment \u00b6 The DAOS deployment workflow requires to start the DAOS server instances early on to enable administrators to perform remote operations in parallel across multiple storage nodes via the dmg management utility. Security is guaranteed via the use of certificates. The first type of commands run after installation include network and storage hardware provisioning and would typically be run from a login node. After daos_server instances have been started on each storage node for the first time, daos_server storage prepare --scm-only will set PMem storage into the necessary state for use with DAOS when run on each host. Then dmg storage format formats persistent storage devices (specified in the server configuration file) on the storage nodes and writes necessary metadata before starting DAOS Engine processes that will operate across the fabric. To sum up, the typical workflow of a DAOS system deployment consists of the following steps: Configure and start the DAOS server . Provision Hardware on all the storage nodes via the dmg utility. Format the DAOS system Set up and start the agent on the client nodes Validate that the DAOS system is operational Note that starting the DAOS server instances can be performed automatically on boot if start-up scripts are registered with systemd. The following subsections will cover each step in more detail. DAOS Server Setup \u00b6 First of all, the DAOS server should be started to allow remote administration command to be executed via the dmg tool. This section describes the minimal DAOS server configuration and how to start it on all the storage nodes. Example RPM Deployment Workflow \u00b6 A recommended workflow to get up and running is as follows: Install DAOS Server RPMs - daos_server systemd services will start in listening mode which means DAOS Engine processes will not be started as the server config file (default location at /etc/daos/daos_server.yml ) has not yet been populated. Run dmg config generate -l <hostset> -a <access_points> across the entire hostset (all the storage servers that are now running the daos_server service after RPM install). The command will only generate a config if hardware setups on all the hosts are similar and have been given sensible NUMA mappings. Adjust the hostset until you have a set with homogeneous hardware configurations. Once a recommended config file can be generated, copy it to the server config file default location ( /etc/daos/daos_server.yml ) on each DAOS Server host and restart all daos_server services. An example command to restart the services is clush -w machines-[118-121,130-133] \"sudo systemctl restart daos_server\" . The services should prompt for format on restart and after format is triggered from dmg , the DAOS Engine processes should start. Server Configuration File \u00b6 The daos_server configuration file is parsed when starting the daos_server process. The configuration file location can be specified on the command line ( daos_server -h for usage) or it will be read from the default location ( /etc/daos/daos_server.yml ). Parameter descriptions are specified in daos_server.yml and example configuration files in the examples directory. Any option supplied to daos_server as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed configuration values are written to a temporary file for reference, and the location will be written to the log. Configuration Options \u00b6 The example configuration file lists the default empty configuration, listing all the options (living documentation of the config file). Live examples are available at https://github.com/daos-stack/daos/tree/master/utils/config/examples The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_server command line, if unspecified then /etc/daos/daos_server.yml is used. Refer to the example configuration file ( daos_server.yml ) for latest information and examples. At this point of the process, the servers: and provider: section of the yaml file can be left blank and will be populated in the subsequent sections. Auto generate configuration file \u00b6 DAOS can attempt to produce a server configuration file that makes optimal use of hardware on a given set of hosts through the 'dmg config generate' command: $ dmg config generate --help ERROR: dmg: Usage: dmg [OPTIONS] config generate [generate-OPTIONS] Application Options: ... -l, --host-list= comma separated list of addresses <ipv4addr/hostname> ... [generate command options] -a, --access-points= Comma separated list of access point addresses <ipv4addr/hostname> -e, --num-engines= Set the number of DAOS Engine sections to be populated in the config file output. If unset then the value will be set to the number of NUMA nodes on storage hosts in the DAOS system. -s, --min-ssds= Minimum number of NVMe SSDs required per DAOS Engine (SSDs must reside on the host that is managing the engine). Set to 0 to generate a config with no NVMe. (default: 1) -c, --net-class=[best-available|ethernet|infiniband] Network class preferred (default: best-available) The command will output recommended config file if supplied requirements are met. Requirements will be derived based on the number of NUMA nodes present on the hosts if '--num-engines' is not specified on the commandline. '--num-engines' specifies the number of engine sections to populate in the config file output. Each section will specify a persistent memory (PMem) block devices that must be present on the host in addition to a fabric network interface and SSDs all bound to the same NUMA node. If not set explicitly on the commandline, default is the number of NUMA nodes detected on the host. '--min-ssds' specifies the minimum number of NVMe SSDs per-engine that need to be present on each host. For each engine entry in the generated config, at least this number of SSDs must be bound to the NUMA node that matches the affinity of the PMem device and fabric network interface associated with the engine. If not set on the commandline, default is \"1\". If set to \"0\" NVMe SSDs will not be added to the generated config and SSD validation will be disabled. '--net-class' specifies preference for network interface class, options are 'ethernet', 'infiband' or 'best-available'. 'best-available' will attempt to choose the most performant (as judged by libfabric) sets of interfaces and supported provider that match the number and NUMA affinity of PMem devices. If not set on the commandline, default is \"best-available\". The configuration file that is generated by the command and output to stdout can be copied to a file and used on the relevant hosts and used as server config to determine the starting environment for 'daos_server' instances. Config file output will not be generated in the following cases: - PMem device count, capacity or NUMA mappings differ on any of the hosts in the hostlist (the hostlist can be specified either in the 'dmg' config file or on the commandline). - NVMe SSD count, PCI address distribution or NUMA affinity differs on any of the hosts in the host list. - NUMA node count can't be detected on the hosts or differs on any host in the host list. - PMem device count or NUMA affinity doesn't meet the 'num-engines' requirement. - NVMe device count or NUMA affinity doesn't meet the 'min-ssds' requirement. - network device count or NUMA affinity doesn't match the configured PMem devices, taking into account any specified network device class preference (ethernet or infiniband). Some CentOS 7.x kernels from before the 7.9 release were known to have a defect that prevented ndctl from being able to report the NUMA affinity for a namespace. This prevents generation of dual engine configs using dmg config generate when running with one of the above-mentioned affected kernels. Certificate Configuration \u00b6 The DAOS security framework relies on certificates to authenticate components and administrators in addition to encrypting DAOS control plane communications. A set of certificates for a given DAOS system may be generated by running the gen_certificates.sh script provided with the DAOS software if there is not an existing TLS certificate infrastructure. The gen_certificates.sh script uses the openssl tool to generate all of the necessary files. We highly recommend using OpenSSL Version 1.1.1h or higher as keys and certificates generated with earlier versions are vulnerable to attack. When DAOS is installed from RPMs, this script is provided in the base daos RPM, and may be invoked in the directory to which the certificates will be written. As part of the generation process, a new local Certificate Authority is created to handle certificate signing, and three role certificates are created: # /usr/lib64/daos/certgen/gen_certificates.sh Generating Private CA Root Certificate Private CA Root Certificate created in ./daosCA ... Generating Server Certificate Required Server Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/server.key ./daosCA/certs/server.crt ... Generating Agent Certificate Required Agent Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/agent.key ./daosCA/certs/agent.crt ... Generating Admin Certificate Required Admin Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/admin.key ./daosCA/certs/admin.crt The files generated under ./daosCA should be protected from unauthorized access and preserved for future use. The generated keys and certificates must then be securely distributed to all nodes participating in the DAOS system (servers, clients, and admin nodes). Permissions for these files should be set to prevent unauthorized access to the keys and certificates. Client nodes require: - CA root cert - Agent cert - Agent key Administrative nodes require: - CA root cert - Admin cert - Admin key Server nodes require: - CA root cert - Server cert - Server key - All valid agent certs in the DAOS system (in the client cert directory, see config file below) After the certificates have been securely distributed, the DAOS configuration files must be updated in order to enable authentication and secure communications. These examples assume that the configuration and certificate files have been installed under /etc/daos : # /etc/daos/daos_server.yml (servers) transport_config: # Location where daos_server will look for Client certificates client_cert_dir: /etc/daos/certs/clients # Custom CA Root certificate for generated certs ca_cert: /etc/daos/certs/daosCA.crt # Server certificate for use in TLS handshakes cert: /etc/daos/certs/server.crt # Key portion of Server Certificate key: /etc/daos/certs/server.key # /etc/daos/daos_agent.yml (clients) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/certs/daosCA.crt # Agent certificate for use in TLS handshakes cert: /etc/daos/certs/agent.crt # Key portion of Agent Certificate key: /etc/daos/certs/agent.key # /etc/daos/daos_control.yml (dmg/admin) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/certs/daosCA.crt # Admin certificate for use in TLS handshakes cert: /etc/daos/certs/admin.crt # Key portion of Admin Certificate key: /etc/daos/certs/admin.key Server Startup \u00b6 The DAOS Server is started as a systemd service. The DAOS Server unit file is installed in the correct location when installing from RPMs. The DAOS Server will be run as daos-server user which will be created during RPM install. If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system . Once the file is copied, modify the ExecStart line to point to your daos_server binary. After modifying ExecStart, run the following command: $ sudo systemctl daemon-reload Once the service file is installed you can start daos_server with the following commands: $ systemctl enable daos_server.service $ systemctl start daos_server.service To check the component status use: $ systemctl status daos_server.service If DAOS Server failed to start, check the logs with: $ journalctl --unit daos_server.service After RPM install, daos_server service starts automatically running as user \"daos\". The server config is read from /etc/daos/daos_server.yml and certificates are read from /etc/daos/certs . With no other admin intervention other than the loading of certificates, daos_server will enter a listening state enabling discovery of storage and network hardware through the dmg tool without any I/O Engines specified in the configuration file. After device discovery and provisioning, an updated configuration file with a populated per-engine section can be stored in /etc/daos/daos_server.yml , and after reestarting the daos_server service it is then ready for the storage to be formatted. DAOS Server Remote Access \u00b6 Remote tasking of the DAOS system and individual DAOS Server processes can be performed via the dmg utility. To set the addresses of which DAOS Servers to task, provide either: - -l <hostlist> on the commandline when invoking, or - hostlist: <hostlist> in the control configuration file daos_control.yml Where <hostlist> represents a slurm-style hostlist string e.g. foo-1[28-63],bar[256-511] . The first entry in the hostlist (after alphabetic then numeric sorting) will be assumed to be the access point as set in the server configuration file. Local configuration files stored in the user directory will be used in preference to the default location e.g. ~/.daos_control.yml . Hardware Provisioning \u00b6 Once the DAOS server started, the storage and network can be configured on the storage nodes via the dmg utility. SCM Preparation \u00b6 This section addresses how to verify that PMem (Intel(R) Optane(TM) persistent memory) modules are correctly installed on the storage nodes and how to configure in interleaved mode to be used by DAOS. Instructions for other types of SCM may be covered in the future. Provisioning the SCM occurs by configuring PMem modules in interleaved memory regions (interleaved mode) in groups of modules local to a specific socket (NUMA), and resultant nvdimm namespaces are defined by a device identifier (e.g., /dev/pmem0). PMem preparation is required once per DAOS installation. This step requires a reboot to enable PMem resource allocation changes to be read by BIOS. PMem preparation can be performed with daos_server storage prepare --scm-only . The first time the command is run, the SCM interleaved regions will be created as resource allocations on any available PMem modules (one region per NUMA node/socket). The regions are activated after BIOS reads the new resource allocations. Upon completion, the storage prepare command will prompt the admin to reboot the storage node(s) in order for the BIOS to activate the new storage allocations. The storage prepare command does not initiate the reboot itself. After running the command a reboot will be required, the command will then need to be run for a second time to expose the namespace device to be used by DAOS. Example usage: clush -w wolf-[118-121,130-133] daos_server storage prepare --scm-only after running, the user should be prompted for a reboot. clush -w wolf-[118-121,130-133] reboot clush -w wolf-[118-121,130-133] daos_server storage prepare --scm-only after running, PMem devices (/dev/pmemX namespaces created on the new SCM regions) should be available on each of the hosts. On the second run, one namespace per region is created, and each namespace may take up to a few minutes to create. Details of the pmem devices will be displayed in JSON format on command completion. Upon successful creation of the pmem devices, the Intel(R) Optane(TM) persistent memory is configured and one can move on to the next step. If required, the pmem devices can be destroyed with the command daos_server storage prepare --scm-only --reset . All namespaces are disabled and destroyed. The SCM regions are removed by resetting modules into \"MemoryMode\" through resource allocations. Note that undefined behavior may result if the namespaces/pmem kernel devices are mounted before running reset (as per the printed warning). A subsequent reboot is required for BIOS to read the new resource allocations. Storage Selection \u00b6 While the DAOS server auto-detects all the usable storage, the administrator will still be provided with the ability through the configuration file (see next section) to whitelist or blacklist the storage devices to be (or not) used. This section covers how to manually detect the storage devices potentially usable by DAOS to populate the configuration file when the administrator wants to have finer control over the storage selection. dmg storage scan can be run to query remote running daos_server processes over the management network. daos_server storage scan can be used to query daos_server directly (scans locally-attached SSDs and Intel Persistent Memory Modules usable by DAOS) but SSDs need to be made accessible first by running daos_server storage prepare --nvme-only -u <current_user first. The output will be equivalent running dmg storage scan --verbose remotely. bash-4.2$ dmg storage scan Hosts SCM Total NVMe Total ----- --------- ---------- wolf-[71-72] 6.4 TB (2 namespaces) 3.1 TB (3 controllers) bash-4.2$ dmg storage scan --verbose ------------ wolf-[71-72] ------------ SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 1 3.2 TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 1 750 GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 1 1.6 TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750 GB The NVMe PCI field above is what should be used in the server configuration file to identified NVMe SSDs. Devices with the same NUMA node/socket should be used in the same per-engine section of the server configuration file for best performance. For further info on command usage run dmg storage --help . SSD health state can be verified via dmg storage scan --nvme-health : bash-4.2$ dmg storage scan --nvme-health ------- wolf-71 ------- PCI:0000:81:00.0 Model:INTEL SSDPED1K750GA FW:E2010325 Socket:1 Capacity:750 GB Health Stats: Temperature:318K(44.85C) Controller Busy Time:0s Power Cycles:15 Power On Duration:10402h0m0s Unsafe Shutdowns:13 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK PCI:0000:da:00.0 Model:INTEL SSDPED1K750GA FW:E2010325 Socket:1 Capacity:750 GB Health Stats: Temperature:320K(46.85C) Controller Busy Time:0s Power Cycles:15 Power On Duration:10402h0m0s Unsafe Shutdowns:13 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK ------- wolf-72 ------- PCI:0000:81:00.0 Model:INTEL SSDPED1K750GA FW:E2010435 Socket:1 Capacity:750 GB Health Stats: Temperature:316K(42.85C) Controller Busy Time:8m0s Power Cycles:23 Power On Duration:10399h0m0s Unsafe Shutdowns:18 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK PCI:0000:da:00.0 Model:INTEL SSDPED1K750GA FW:E2010435 Socket:1 Capacity:750 GB Health Stats: Temperature:320K(46.85C) Controller Busy Time:1m0s Power Cycles:23 Power On Duration:10399h0m0s Unsafe Shutdowns:19 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK The next step consists of adjusting in the server configuration the storage devices that should be used by DAOS. The servers section of the yaml is a list specifying details for each DAOS I/O instance to be started on the host (currently a maximum of 2 per host is imposed). Devices with the same NUMA rating/node/socket should be colocated on a single DAOS I/O instance where possible. more details bdev_list should be populated with NVMe PCI addresses scm_list should be populated with PMem interleaved set namespaces (e.g. /dev/pmem1 ) DAOS Control Servers will need to be restarted on all hosts after updates to the server configuration file. Pick one host in the system and set access_points to list of that host's hostname or IP address (don't need to specify port). This will be the host which bootstraps the DAOS management service (MS). To illustrate, assume a cluster with homogeneous hardware configurations that returns the following from scan for each host: [daos@wolf-72 daos_m]$ dmg -l wolf-7[1-2] -i storage scan --verbose ------- wolf-7[1-2] ------- SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 2.90TB pmem1 1 2.90TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 0 750.00GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 0 1.56TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB In this situation, the configuration file servers section could be populated as follows: <snip> port: 10001 access_points: [\"wolf-71\"] # <----- updated <snip> engines: - targets: 16 # number of I/O service threads per-engine first_core: 0 # offset of the first core to bind service threads nr_xs_helpers: 0 # count of I/O offload threads fabric_iface: eth0 # network interface to use for this engine fabric_iface_port: 31416 # network port log_mask: ERR # debug level to start with the engine with log_file: /tmp/server1.log # where to store engine logs scm_mount: /mnt/daos # where to mount SCM scm_class: dcpm # type of SCM scm_list: [/dev/pmem0] # <----- updated bdev_class: nvme # type of block device bdev_list: [\"0000:87:00.0\"] # <----- updated - targets: 16 first_core: 0 nr_xs_helpers: 0 fabric_iface: eth0 fabric_iface_port: 32416 log_mask: ERR log_file: /tmp/server2.log scm_mount: /mnt/daos scm_class: dcpm scm_list: [/dev/pmem1] # <----- updated bdev_class: nvme bdev_list: [\"0000:da:00.0\"] # <----- updated <end> There are a few optional providers that are not built by default. For detailed information, please refer to the DAOS build documentation . NOTE The support of the optional providers is not guarantee and can be removed without further notification. Network Configuration \u00b6 Network Scan \u00b6 The dmg utility supports the network scan function to display the network interfaces, related OFI fabric providers and associated NUMA node for each device. This information is used to configure the global fabric provider and the unique local network interface for each I/O Engine instance on the storage nodes. This section will help you determine what to provide for the provider , fabric_iface and pinned_numa_node entries in the daos_server.yml file. The following commands are typical examples: $ dmg network scan $ dmg network scan -p all $ dmg network scan -p ofi+sockets $ dmg network scan --provider 'ofi+verbs;ofi_rxm' In the early stages when a daos_server has not yet been fully configured and lacks a declaration of the system's fabric provider, it may be helpful to view an unfiltered list of scan results. Use either of these dmg commands in the early stages to accomplish this goal: $ dmg network scan $ dgm network scan -p all Typical network scan results look as follows: $ dmg network scan ------- wolf-29 ------- ------------- NUMA Socket 1 ------------- Provider Interfaces -------- ---------- ofi+sockets ib1 --------- localhost --------- ------------- NUMA Socket 0 ------------- Provider Interfaces -------- ---------- ofi+sockets ib0, eth0 ------------- NUMA Socket 1 ------------- Provider Interfaces -------- ---------- ofi+sockets ib1 Use one of these providers to configure the provider in the daos_server.yml . Only one provider may be specified for the entire DAOS installation. Client nodes must be capable of communicating to the daos_server nodes via the same provider. Therefore, it is helpful to choose network settings for the daos_server that are compatible with the expected client node configuration. After the daos_server.yml file has been edited and contains a provider, subsequent dmg network scan commands will filter the results based on that provider. If it is desired to view an unfiltered list again, issue dmg network scan -p all . Regardless of the provider in the daos_server.yml file, the results may be filtered to the specified provider with the command dmg network scan -p ofi_provider where ofi_provider is one of the available providers from the list. The results of the network scan may be used to help configure the I/O Engine instances. Each I/O Engine instance is configured with a unique fabric_iface and optional pinned_numa_node . The interfaces and NUMA Sockets listed in the scan results map to the daos_server.yml fabric_iface and pinned_numa_node respectively. The use of pinned_numa_node is optional, but recommended for best performance. When specified with the value that matches the network interface, the I/O Engine will bind itself to that NUMA node and to cores purely within that NUMA node. This configuration yields the fastest access to that network device. Changing Network Providers \u00b6 Information about the network configuration is stored as metadata on the DAOS storage. If, after initial deployment, the provider must be changed, it is necessary to reformat the storage devices using dmg storage format after the configuration file has been updated with the new provider. Provider Testing \u00b6 Then, the fi_pingpong test can be used to verify that the targeted OFI provider works fine: node1$ fi_pingpong -p psm2 node2$ fi_pingpong -p psm2 ${IP_ADDRESS_NODE1} bytes #sent #ack total time MB/sec usec/xfer Mxfers/sec 64 10 =10 1.2k 0.00s 21.69 2.95 0.34 256 10 =10 5k 0.00s 116.36 2.20 0.45 1k 10 =10 20k 0.00s 379.26 2.70 0.37 4k 10 =10 80k 0.00s 1077.89 3.80 0.26 64k 10 =10 1.2m 0.00s 2145.20 30.55 0.03 1m 10 =10 20m 0.00s 8867.45 118.25 0.01 Storage Formatting \u00b6 Once the daos_server has been restarted with the correct storage devices and network interface to use, one can move to the format phase. When daos_server is started for the first time, it enters \"maintenance mode\" and waits for a dmg storage format call to be issued from the management tool. This remote call will trigger the formatting of the locally attached storage on the host for use with DAOS using the parameters defined in the server config file. dmg -i -l <host>[,...] storage format will normally be run on a login node specifying a hostlist ( -l <host>[,...] ) of storage nodes with SCM/PMem modules and NVMe SSDs installed and prepared. Upon successful format, DAOS Control Servers will start DAOS IO instances that have been specified in the server config file. Successful start-up is indicated by the following on stdout: DAOS I/O Engine (v0.8.0) process 433456 started on rank 1 with 8 target, 2 helper XS per target, firstcore 0, host wolf-72.wolf.hpdd.intel.com. SCM Format \u00b6 When the command is run, the pmem kernel devices created on SCM/PMem regions are formatted and mounted based on the parameters provided in the server config file. scm_mount specifies the location of the mountpoint to create. scm_class can be set to ram to use a tmpfs in the situation that no SCM/PMem is available ( scm_size dictates the size of tmpfs in GB), when set to dcpm the device specified under scm_list will be mounted at scm_mount path. NVMe Format \u00b6 When the command is run, NVMe SSDs are formatted and set up to be used by DAOS based on the parameters provided in the server config file. bdev_class can be set to nvme to use actual NVMe devices with SPDK for DAOS storage. Other bdev_class values can be used for emulation of NVMe storage as specified in the server config file. bdev_list identifies devices to use with a list of PCI addresses (this can be populated after viewing results from storage scan command). After the format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain a file named daos_nvme.conf . The file should describe the devices with PCI addresses as listed in the bdev_list parameter of the server config file. The presence and contents of the file indicate that the specified NVMe SSDs have been configured correctly for use with DAOS. The contents of the NVMe SSDs listed in the server configuration file bdev_list parameter will be reset on format. Server Format \u00b6 Before the format command is run, no DAOS metadata should exist under the path specified by scm_mount parameter in the server configuration file. After the storage format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain the necessary DAOS metadata indicating that the server has been formatted. When starting, daos_server will skip maintenance mode and attempt to start I/O Engines if valid DAOS metadata is found in scm_mount . Agent Setup \u00b6 This section addresses how to configure the DAOS agents on the storage nodes before starting it. Agent Certificate Generation \u00b6 The DAOS security framework relies on certificates to authenticate administrators. The security infrastructure is currently under development and will be delivered in DAOS v1.0. Initial support for certificates has been added to DAOS and can be disabled either via the command line or in the DAOS Agent configuration file. Currently, the easiest way to disable certificate support is to pass the -i flag to daos_agent . Agent Configuration File \u00b6 The daos_agent configuration file is parsed when starting the daos_agent process. The configuration file location can be specified on the command line ( daos_agent -h for usage) or default location ( install/etc/daos_agent.yml ). If installed from rpms the default location is ( /etc/daos/daos_agent.yml ). Parameter descriptions are specified in daos_agent.yml . Any option supplied to daos_agent as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed config values are written to a temporary file for reference, and the location will be written to the log. The following section lists the format, options, defaults, and descriptions available in the configuration file. The example configuration file lists the default empty configuration listing all the options (living documentation of the config file). Live examples are available here . The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_agent command line, if not set then /etc/daos/daos_agent.yml is used. Refer to the example configuration file ( daos_agent.yml ) for latest information and examples. Agent Startup \u00b6 DAOS Agent is a standalone application to be run on each compute node. It can be configured to use secure communications (default) or can be allowed to communicate with the control plane over unencrypted channels. The following example shows daos_agent being configured to operate in insecure mode due to incomplete integration of certificate support as of the 0.6 release and configured to use a non-default agent configuration file. To start the DAOS Agent from the command line, run: $ daos_agent -i -o <'path to agent configuration file/daos_agent.yml'> & Alternatively, the DAOS Agent can be started as a systemd service. The DAOS Agent unit file is installed in the correct location when installing from RPMs. If you want to run the DAOS Agent without certificates (not recommended in production deployments), you need to add the -i option to the systemd ExecStart invocation (see below). If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system . Once the file is copied modify the ExecStart line to point to your in tree daos_agent binary. ExecStart=/usr/bin/daos_agent -i -o <'path to agent configuration file/daos_agent.yml'> Once the service file is installed, you can start daos_agent with the following commands: $ sudo systemctl daemon-reload $ sudo systemctl enable daos_agent.service $ sudo systemctl start daos_agent.service To check the component status use: $ sudo systemctl status daos_agent.service If DAOS Agent failed to start check the logs with: $ sudo journalctl --unit daos_agent.service Disable Agent Cache (Optional) \u00b6 In certain circumstances (e.g. for DAOS development or system evaluation), it may be desirable to disable the DAOS Agent's caching mechanism in order to avoid stale system information being retained across reformats of a system. The DAOS Agent normally caches a map of rank->fabric URI lookups as well as client network configuration data in order to reduce the number of management RPCs required to start an application. When this information becomes stale, the Agent must be restarted in order to repopulate the cache with new information. Alternatively, the caching mechanism may be disabled, with the tradeoff that each application launch will invoke management RPCs in order to obtain system connection information. To disable the DAOS Agent caching mechanism, set the following environment variable before starting the daos_agent process: DAOS_AGENT_DISABLE_CACHE=true If running from systemd, add the following to the daos_agent service file in the [Service] section before reloading systemd and restarting the daos_agent service: Environment=DAOS_AGENT_DISABLE_CACHE=true https://github.com/intel/ipmctl \u21a9 https://github.com/daos-stack/daos/tree/master/utils/config \u21a9 https://www.open-mpi.org/faq/?category=running#mpirun-hostfile \u21a9 https://github.com/daos-stack/daos/tree/master/src/control/README.md \u21a9 https://github.com/pmem/ndctl/issues/130 \u21a9","title":"System Deployment"},{"location":"admin/deployment/#system-deployment","text":"The DAOS deployment workflow requires to start the DAOS server instances early on to enable administrators to perform remote operations in parallel across multiple storage nodes via the dmg management utility. Security is guaranteed via the use of certificates. The first type of commands run after installation include network and storage hardware provisioning and would typically be run from a login node. After daos_server instances have been started on each storage node for the first time, daos_server storage prepare --scm-only will set PMem storage into the necessary state for use with DAOS when run on each host. Then dmg storage format formats persistent storage devices (specified in the server configuration file) on the storage nodes and writes necessary metadata before starting DAOS Engine processes that will operate across the fabric. To sum up, the typical workflow of a DAOS system deployment consists of the following steps: Configure and start the DAOS server . Provision Hardware on all the storage nodes via the dmg utility. Format the DAOS system Set up and start the agent on the client nodes Validate that the DAOS system is operational Note that starting the DAOS server instances can be performed automatically on boot if start-up scripts are registered with systemd. The following subsections will cover each step in more detail.","title":"System Deployment"},{"location":"admin/deployment/#daos-server-setup","text":"First of all, the DAOS server should be started to allow remote administration command to be executed via the dmg tool. This section describes the minimal DAOS server configuration and how to start it on all the storage nodes.","title":"DAOS Server Setup"},{"location":"admin/deployment/#example-rpm-deployment-workflow","text":"A recommended workflow to get up and running is as follows: Install DAOS Server RPMs - daos_server systemd services will start in listening mode which means DAOS Engine processes will not be started as the server config file (default location at /etc/daos/daos_server.yml ) has not yet been populated. Run dmg config generate -l <hostset> -a <access_points> across the entire hostset (all the storage servers that are now running the daos_server service after RPM install). The command will only generate a config if hardware setups on all the hosts are similar and have been given sensible NUMA mappings. Adjust the hostset until you have a set with homogeneous hardware configurations. Once a recommended config file can be generated, copy it to the server config file default location ( /etc/daos/daos_server.yml ) on each DAOS Server host and restart all daos_server services. An example command to restart the services is clush -w machines-[118-121,130-133] \"sudo systemctl restart daos_server\" . The services should prompt for format on restart and after format is triggered from dmg , the DAOS Engine processes should start.","title":"Example RPM Deployment Workflow"},{"location":"admin/deployment/#server-configuration-file","text":"The daos_server configuration file is parsed when starting the daos_server process. The configuration file location can be specified on the command line ( daos_server -h for usage) or it will be read from the default location ( /etc/daos/daos_server.yml ). Parameter descriptions are specified in daos_server.yml and example configuration files in the examples directory. Any option supplied to daos_server as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed configuration values are written to a temporary file for reference, and the location will be written to the log.","title":"Server Configuration File"},{"location":"admin/deployment/#configuration-options","text":"The example configuration file lists the default empty configuration, listing all the options (living documentation of the config file). Live examples are available at https://github.com/daos-stack/daos/tree/master/utils/config/examples The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_server command line, if unspecified then /etc/daos/daos_server.yml is used. Refer to the example configuration file ( daos_server.yml ) for latest information and examples. At this point of the process, the servers: and provider: section of the yaml file can be left blank and will be populated in the subsequent sections.","title":"Configuration Options"},{"location":"admin/deployment/#auto-generate-configuration-file","text":"DAOS can attempt to produce a server configuration file that makes optimal use of hardware on a given set of hosts through the 'dmg config generate' command: $ dmg config generate --help ERROR: dmg: Usage: dmg [OPTIONS] config generate [generate-OPTIONS] Application Options: ... -l, --host-list= comma separated list of addresses <ipv4addr/hostname> ... [generate command options] -a, --access-points= Comma separated list of access point addresses <ipv4addr/hostname> -e, --num-engines= Set the number of DAOS Engine sections to be populated in the config file output. If unset then the value will be set to the number of NUMA nodes on storage hosts in the DAOS system. -s, --min-ssds= Minimum number of NVMe SSDs required per DAOS Engine (SSDs must reside on the host that is managing the engine). Set to 0 to generate a config with no NVMe. (default: 1) -c, --net-class=[best-available|ethernet|infiniband] Network class preferred (default: best-available) The command will output recommended config file if supplied requirements are met. Requirements will be derived based on the number of NUMA nodes present on the hosts if '--num-engines' is not specified on the commandline. '--num-engines' specifies the number of engine sections to populate in the config file output. Each section will specify a persistent memory (PMem) block devices that must be present on the host in addition to a fabric network interface and SSDs all bound to the same NUMA node. If not set explicitly on the commandline, default is the number of NUMA nodes detected on the host. '--min-ssds' specifies the minimum number of NVMe SSDs per-engine that need to be present on each host. For each engine entry in the generated config, at least this number of SSDs must be bound to the NUMA node that matches the affinity of the PMem device and fabric network interface associated with the engine. If not set on the commandline, default is \"1\". If set to \"0\" NVMe SSDs will not be added to the generated config and SSD validation will be disabled. '--net-class' specifies preference for network interface class, options are 'ethernet', 'infiband' or 'best-available'. 'best-available' will attempt to choose the most performant (as judged by libfabric) sets of interfaces and supported provider that match the number and NUMA affinity of PMem devices. If not set on the commandline, default is \"best-available\". The configuration file that is generated by the command and output to stdout can be copied to a file and used on the relevant hosts and used as server config to determine the starting environment for 'daos_server' instances. Config file output will not be generated in the following cases: - PMem device count, capacity or NUMA mappings differ on any of the hosts in the hostlist (the hostlist can be specified either in the 'dmg' config file or on the commandline). - NVMe SSD count, PCI address distribution or NUMA affinity differs on any of the hosts in the host list. - NUMA node count can't be detected on the hosts or differs on any host in the host list. - PMem device count or NUMA affinity doesn't meet the 'num-engines' requirement. - NVMe device count or NUMA affinity doesn't meet the 'min-ssds' requirement. - network device count or NUMA affinity doesn't match the configured PMem devices, taking into account any specified network device class preference (ethernet or infiniband). Some CentOS 7.x kernels from before the 7.9 release were known to have a defect that prevented ndctl from being able to report the NUMA affinity for a namespace. This prevents generation of dual engine configs using dmg config generate when running with one of the above-mentioned affected kernels.","title":"Auto generate configuration file"},{"location":"admin/deployment/#certificate-configuration","text":"The DAOS security framework relies on certificates to authenticate components and administrators in addition to encrypting DAOS control plane communications. A set of certificates for a given DAOS system may be generated by running the gen_certificates.sh script provided with the DAOS software if there is not an existing TLS certificate infrastructure. The gen_certificates.sh script uses the openssl tool to generate all of the necessary files. We highly recommend using OpenSSL Version 1.1.1h or higher as keys and certificates generated with earlier versions are vulnerable to attack. When DAOS is installed from RPMs, this script is provided in the base daos RPM, and may be invoked in the directory to which the certificates will be written. As part of the generation process, a new local Certificate Authority is created to handle certificate signing, and three role certificates are created: # /usr/lib64/daos/certgen/gen_certificates.sh Generating Private CA Root Certificate Private CA Root Certificate created in ./daosCA ... Generating Server Certificate Required Server Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/server.key ./daosCA/certs/server.crt ... Generating Agent Certificate Required Agent Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/agent.key ./daosCA/certs/agent.crt ... Generating Admin Certificate Required Admin Certificate Files: ./daosCA/certs/daosCA.crt ./daosCA/certs/admin.key ./daosCA/certs/admin.crt The files generated under ./daosCA should be protected from unauthorized access and preserved for future use. The generated keys and certificates must then be securely distributed to all nodes participating in the DAOS system (servers, clients, and admin nodes). Permissions for these files should be set to prevent unauthorized access to the keys and certificates. Client nodes require: - CA root cert - Agent cert - Agent key Administrative nodes require: - CA root cert - Admin cert - Admin key Server nodes require: - CA root cert - Server cert - Server key - All valid agent certs in the DAOS system (in the client cert directory, see config file below) After the certificates have been securely distributed, the DAOS configuration files must be updated in order to enable authentication and secure communications. These examples assume that the configuration and certificate files have been installed under /etc/daos : # /etc/daos/daos_server.yml (servers) transport_config: # Location where daos_server will look for Client certificates client_cert_dir: /etc/daos/certs/clients # Custom CA Root certificate for generated certs ca_cert: /etc/daos/certs/daosCA.crt # Server certificate for use in TLS handshakes cert: /etc/daos/certs/server.crt # Key portion of Server Certificate key: /etc/daos/certs/server.key # /etc/daos/daos_agent.yml (clients) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/certs/daosCA.crt # Agent certificate for use in TLS handshakes cert: /etc/daos/certs/agent.crt # Key portion of Agent Certificate key: /etc/daos/certs/agent.key # /etc/daos/daos_control.yml (dmg/admin) transport_config: # Custom CA Root certificate for generated certs ca_cert: /etc/daos/certs/daosCA.crt # Admin certificate for use in TLS handshakes cert: /etc/daos/certs/admin.crt # Key portion of Admin Certificate key: /etc/daos/certs/admin.key","title":"Certificate Configuration"},{"location":"admin/deployment/#server-startup","text":"The DAOS Server is started as a systemd service. The DAOS Server unit file is installed in the correct location when installing from RPMs. The DAOS Server will be run as daos-server user which will be created during RPM install. If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system . Once the file is copied, modify the ExecStart line to point to your daos_server binary. After modifying ExecStart, run the following command: $ sudo systemctl daemon-reload Once the service file is installed you can start daos_server with the following commands: $ systemctl enable daos_server.service $ systemctl start daos_server.service To check the component status use: $ systemctl status daos_server.service If DAOS Server failed to start, check the logs with: $ journalctl --unit daos_server.service After RPM install, daos_server service starts automatically running as user \"daos\". The server config is read from /etc/daos/daos_server.yml and certificates are read from /etc/daos/certs . With no other admin intervention other than the loading of certificates, daos_server will enter a listening state enabling discovery of storage and network hardware through the dmg tool without any I/O Engines specified in the configuration file. After device discovery and provisioning, an updated configuration file with a populated per-engine section can be stored in /etc/daos/daos_server.yml , and after reestarting the daos_server service it is then ready for the storage to be formatted.","title":"Server Startup"},{"location":"admin/deployment/#daos-server-remote-access","text":"Remote tasking of the DAOS system and individual DAOS Server processes can be performed via the dmg utility. To set the addresses of which DAOS Servers to task, provide either: - -l <hostlist> on the commandline when invoking, or - hostlist: <hostlist> in the control configuration file daos_control.yml Where <hostlist> represents a slurm-style hostlist string e.g. foo-1[28-63],bar[256-511] . The first entry in the hostlist (after alphabetic then numeric sorting) will be assumed to be the access point as set in the server configuration file. Local configuration files stored in the user directory will be used in preference to the default location e.g. ~/.daos_control.yml .","title":"DAOS Server Remote Access"},{"location":"admin/deployment/#hardware-provisioning","text":"Once the DAOS server started, the storage and network can be configured on the storage nodes via the dmg utility.","title":"Hardware Provisioning"},{"location":"admin/deployment/#scm-preparation","text":"This section addresses how to verify that PMem (Intel(R) Optane(TM) persistent memory) modules are correctly installed on the storage nodes and how to configure in interleaved mode to be used by DAOS. Instructions for other types of SCM may be covered in the future. Provisioning the SCM occurs by configuring PMem modules in interleaved memory regions (interleaved mode) in groups of modules local to a specific socket (NUMA), and resultant nvdimm namespaces are defined by a device identifier (e.g., /dev/pmem0). PMem preparation is required once per DAOS installation. This step requires a reboot to enable PMem resource allocation changes to be read by BIOS. PMem preparation can be performed with daos_server storage prepare --scm-only . The first time the command is run, the SCM interleaved regions will be created as resource allocations on any available PMem modules (one region per NUMA node/socket). The regions are activated after BIOS reads the new resource allocations. Upon completion, the storage prepare command will prompt the admin to reboot the storage node(s) in order for the BIOS to activate the new storage allocations. The storage prepare command does not initiate the reboot itself. After running the command a reboot will be required, the command will then need to be run for a second time to expose the namespace device to be used by DAOS. Example usage: clush -w wolf-[118-121,130-133] daos_server storage prepare --scm-only after running, the user should be prompted for a reboot. clush -w wolf-[118-121,130-133] reboot clush -w wolf-[118-121,130-133] daos_server storage prepare --scm-only after running, PMem devices (/dev/pmemX namespaces created on the new SCM regions) should be available on each of the hosts. On the second run, one namespace per region is created, and each namespace may take up to a few minutes to create. Details of the pmem devices will be displayed in JSON format on command completion. Upon successful creation of the pmem devices, the Intel(R) Optane(TM) persistent memory is configured and one can move on to the next step. If required, the pmem devices can be destroyed with the command daos_server storage prepare --scm-only --reset . All namespaces are disabled and destroyed. The SCM regions are removed by resetting modules into \"MemoryMode\" through resource allocations. Note that undefined behavior may result if the namespaces/pmem kernel devices are mounted before running reset (as per the printed warning). A subsequent reboot is required for BIOS to read the new resource allocations.","title":"SCM Preparation"},{"location":"admin/deployment/#storage-selection","text":"While the DAOS server auto-detects all the usable storage, the administrator will still be provided with the ability through the configuration file (see next section) to whitelist or blacklist the storage devices to be (or not) used. This section covers how to manually detect the storage devices potentially usable by DAOS to populate the configuration file when the administrator wants to have finer control over the storage selection. dmg storage scan can be run to query remote running daos_server processes over the management network. daos_server storage scan can be used to query daos_server directly (scans locally-attached SSDs and Intel Persistent Memory Modules usable by DAOS) but SSDs need to be made accessible first by running daos_server storage prepare --nvme-only -u <current_user first. The output will be equivalent running dmg storage scan --verbose remotely. bash-4.2$ dmg storage scan Hosts SCM Total NVMe Total ----- --------- ---------- wolf-[71-72] 6.4 TB (2 namespaces) 3.1 TB (3 controllers) bash-4.2$ dmg storage scan --verbose ------------ wolf-[71-72] ------------ SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 3.2 TB pmem1 1 3.2 TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 1 750 GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 1 1.6 TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750 GB The NVMe PCI field above is what should be used in the server configuration file to identified NVMe SSDs. Devices with the same NUMA node/socket should be used in the same per-engine section of the server configuration file for best performance. For further info on command usage run dmg storage --help . SSD health state can be verified via dmg storage scan --nvme-health : bash-4.2$ dmg storage scan --nvme-health ------- wolf-71 ------- PCI:0000:81:00.0 Model:INTEL SSDPED1K750GA FW:E2010325 Socket:1 Capacity:750 GB Health Stats: Temperature:318K(44.85C) Controller Busy Time:0s Power Cycles:15 Power On Duration:10402h0m0s Unsafe Shutdowns:13 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK PCI:0000:da:00.0 Model:INTEL SSDPED1K750GA FW:E2010325 Socket:1 Capacity:750 GB Health Stats: Temperature:320K(46.85C) Controller Busy Time:0s Power Cycles:15 Power On Duration:10402h0m0s Unsafe Shutdowns:13 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK ------- wolf-72 ------- PCI:0000:81:00.0 Model:INTEL SSDPED1K750GA FW:E2010435 Socket:1 Capacity:750 GB Health Stats: Temperature:316K(42.85C) Controller Busy Time:8m0s Power Cycles:23 Power On Duration:10399h0m0s Unsafe Shutdowns:18 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK PCI:0000:da:00.0 Model:INTEL SSDPED1K750GA FW:E2010435 Socket:1 Capacity:750 GB Health Stats: Temperature:320K(46.85C) Controller Busy Time:1m0s Power Cycles:23 Power On Duration:10399h0m0s Unsafe Shutdowns:19 Error Count:0 Media Errors:0 Read Errors:0 Write Errors:0 Unmap Errors:0 Checksum Errors:0 Error Log Entries:0 Critical Warnings: Temperature: OK Available Spare: OK Device Reliability: OK Read Only: OK Volatile Memory Backup: OK The next step consists of adjusting in the server configuration the storage devices that should be used by DAOS. The servers section of the yaml is a list specifying details for each DAOS I/O instance to be started on the host (currently a maximum of 2 per host is imposed). Devices with the same NUMA rating/node/socket should be colocated on a single DAOS I/O instance where possible. more details bdev_list should be populated with NVMe PCI addresses scm_list should be populated with PMem interleaved set namespaces (e.g. /dev/pmem1 ) DAOS Control Servers will need to be restarted on all hosts after updates to the server configuration file. Pick one host in the system and set access_points to list of that host's hostname or IP address (don't need to specify port). This will be the host which bootstraps the DAOS management service (MS). To illustrate, assume a cluster with homogeneous hardware configurations that returns the following from scan for each host: [daos@wolf-72 daos_m]$ dmg -l wolf-7[1-2] -i storage scan --verbose ------- wolf-7[1-2] ------- SCM Namespace Socket ID Capacity ------------- --------- -------- pmem0 0 2.90TB pmem1 1 2.90TB NVMe PCI Model FW Revision Socket ID Capacity -------- ----- ----------- --------- -------- 0000:81:00.0 INTEL SSDPED1K750GA E2010325 0 750.00GB 0000:87:00.0 INTEL SSDPEDMD016T4 8DV10171 0 1.56TB 0000:da:00.0 INTEL SSDPED1K750GA E2010325 1 750.00GB In this situation, the configuration file servers section could be populated as follows: <snip> port: 10001 access_points: [\"wolf-71\"] # <----- updated <snip> engines: - targets: 16 # number of I/O service threads per-engine first_core: 0 # offset of the first core to bind service threads nr_xs_helpers: 0 # count of I/O offload threads fabric_iface: eth0 # network interface to use for this engine fabric_iface_port: 31416 # network port log_mask: ERR # debug level to start with the engine with log_file: /tmp/server1.log # where to store engine logs scm_mount: /mnt/daos # where to mount SCM scm_class: dcpm # type of SCM scm_list: [/dev/pmem0] # <----- updated bdev_class: nvme # type of block device bdev_list: [\"0000:87:00.0\"] # <----- updated - targets: 16 first_core: 0 nr_xs_helpers: 0 fabric_iface: eth0 fabric_iface_port: 32416 log_mask: ERR log_file: /tmp/server2.log scm_mount: /mnt/daos scm_class: dcpm scm_list: [/dev/pmem1] # <----- updated bdev_class: nvme bdev_list: [\"0000:da:00.0\"] # <----- updated <end> There are a few optional providers that are not built by default. For detailed information, please refer to the DAOS build documentation . NOTE The support of the optional providers is not guarantee and can be removed without further notification.","title":"Storage Selection"},{"location":"admin/deployment/#network-configuration","text":"","title":"Network Configuration"},{"location":"admin/deployment/#network-scan","text":"The dmg utility supports the network scan function to display the network interfaces, related OFI fabric providers and associated NUMA node for each device. This information is used to configure the global fabric provider and the unique local network interface for each I/O Engine instance on the storage nodes. This section will help you determine what to provide for the provider , fabric_iface and pinned_numa_node entries in the daos_server.yml file. The following commands are typical examples: $ dmg network scan $ dmg network scan -p all $ dmg network scan -p ofi+sockets $ dmg network scan --provider 'ofi+verbs;ofi_rxm' In the early stages when a daos_server has not yet been fully configured and lacks a declaration of the system's fabric provider, it may be helpful to view an unfiltered list of scan results. Use either of these dmg commands in the early stages to accomplish this goal: $ dmg network scan $ dgm network scan -p all Typical network scan results look as follows: $ dmg network scan ------- wolf-29 ------- ------------- NUMA Socket 1 ------------- Provider Interfaces -------- ---------- ofi+sockets ib1 --------- localhost --------- ------------- NUMA Socket 0 ------------- Provider Interfaces -------- ---------- ofi+sockets ib0, eth0 ------------- NUMA Socket 1 ------------- Provider Interfaces -------- ---------- ofi+sockets ib1 Use one of these providers to configure the provider in the daos_server.yml . Only one provider may be specified for the entire DAOS installation. Client nodes must be capable of communicating to the daos_server nodes via the same provider. Therefore, it is helpful to choose network settings for the daos_server that are compatible with the expected client node configuration. After the daos_server.yml file has been edited and contains a provider, subsequent dmg network scan commands will filter the results based on that provider. If it is desired to view an unfiltered list again, issue dmg network scan -p all . Regardless of the provider in the daos_server.yml file, the results may be filtered to the specified provider with the command dmg network scan -p ofi_provider where ofi_provider is one of the available providers from the list. The results of the network scan may be used to help configure the I/O Engine instances. Each I/O Engine instance is configured with a unique fabric_iface and optional pinned_numa_node . The interfaces and NUMA Sockets listed in the scan results map to the daos_server.yml fabric_iface and pinned_numa_node respectively. The use of pinned_numa_node is optional, but recommended for best performance. When specified with the value that matches the network interface, the I/O Engine will bind itself to that NUMA node and to cores purely within that NUMA node. This configuration yields the fastest access to that network device.","title":"Network Scan"},{"location":"admin/deployment/#changing-network-providers","text":"Information about the network configuration is stored as metadata on the DAOS storage. If, after initial deployment, the provider must be changed, it is necessary to reformat the storage devices using dmg storage format after the configuration file has been updated with the new provider.","title":"Changing Network Providers"},{"location":"admin/deployment/#provider-testing","text":"Then, the fi_pingpong test can be used to verify that the targeted OFI provider works fine: node1$ fi_pingpong -p psm2 node2$ fi_pingpong -p psm2 ${IP_ADDRESS_NODE1} bytes #sent #ack total time MB/sec usec/xfer Mxfers/sec 64 10 =10 1.2k 0.00s 21.69 2.95 0.34 256 10 =10 5k 0.00s 116.36 2.20 0.45 1k 10 =10 20k 0.00s 379.26 2.70 0.37 4k 10 =10 80k 0.00s 1077.89 3.80 0.26 64k 10 =10 1.2m 0.00s 2145.20 30.55 0.03 1m 10 =10 20m 0.00s 8867.45 118.25 0.01","title":"Provider Testing"},{"location":"admin/deployment/#storage-formatting","text":"Once the daos_server has been restarted with the correct storage devices and network interface to use, one can move to the format phase. When daos_server is started for the first time, it enters \"maintenance mode\" and waits for a dmg storage format call to be issued from the management tool. This remote call will trigger the formatting of the locally attached storage on the host for use with DAOS using the parameters defined in the server config file. dmg -i -l <host>[,...] storage format will normally be run on a login node specifying a hostlist ( -l <host>[,...] ) of storage nodes with SCM/PMem modules and NVMe SSDs installed and prepared. Upon successful format, DAOS Control Servers will start DAOS IO instances that have been specified in the server config file. Successful start-up is indicated by the following on stdout: DAOS I/O Engine (v0.8.0) process 433456 started on rank 1 with 8 target, 2 helper XS per target, firstcore 0, host wolf-72.wolf.hpdd.intel.com.","title":"Storage Formatting"},{"location":"admin/deployment/#scm-format","text":"When the command is run, the pmem kernel devices created on SCM/PMem regions are formatted and mounted based on the parameters provided in the server config file. scm_mount specifies the location of the mountpoint to create. scm_class can be set to ram to use a tmpfs in the situation that no SCM/PMem is available ( scm_size dictates the size of tmpfs in GB), when set to dcpm the device specified under scm_list will be mounted at scm_mount path.","title":"SCM Format"},{"location":"admin/deployment/#nvme-format","text":"When the command is run, NVMe SSDs are formatted and set up to be used by DAOS based on the parameters provided in the server config file. bdev_class can be set to nvme to use actual NVMe devices with SPDK for DAOS storage. Other bdev_class values can be used for emulation of NVMe storage as specified in the server config file. bdev_list identifies devices to use with a list of PCI addresses (this can be populated after viewing results from storage scan command). After the format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain a file named daos_nvme.conf . The file should describe the devices with PCI addresses as listed in the bdev_list parameter of the server config file. The presence and contents of the file indicate that the specified NVMe SSDs have been configured correctly for use with DAOS. The contents of the NVMe SSDs listed in the server configuration file bdev_list parameter will be reset on format.","title":"NVMe Format"},{"location":"admin/deployment/#server-format","text":"Before the format command is run, no DAOS metadata should exist under the path specified by scm_mount parameter in the server configuration file. After the storage format command is run, the path specified by the server configuration file scm_mount parameter should be mounted and should contain the necessary DAOS metadata indicating that the server has been formatted. When starting, daos_server will skip maintenance mode and attempt to start I/O Engines if valid DAOS metadata is found in scm_mount .","title":"Server Format"},{"location":"admin/deployment/#agent-setup","text":"This section addresses how to configure the DAOS agents on the storage nodes before starting it.","title":"Agent Setup"},{"location":"admin/deployment/#agent-certificate-generation","text":"The DAOS security framework relies on certificates to authenticate administrators. The security infrastructure is currently under development and will be delivered in DAOS v1.0. Initial support for certificates has been added to DAOS and can be disabled either via the command line or in the DAOS Agent configuration file. Currently, the easiest way to disable certificate support is to pass the -i flag to daos_agent .","title":"Agent Certificate Generation"},{"location":"admin/deployment/#agent-configuration-file","text":"The daos_agent configuration file is parsed when starting the daos_agent process. The configuration file location can be specified on the command line ( daos_agent -h for usage) or default location ( install/etc/daos_agent.yml ). If installed from rpms the default location is ( /etc/daos/daos_agent.yml ). Parameter descriptions are specified in daos_agent.yml . Any option supplied to daos_agent as a command line option or flag will take precedence over equivalent configuration file parameter. For convenience, active parsed config values are written to a temporary file for reference, and the location will be written to the log. The following section lists the format, options, defaults, and descriptions available in the configuration file. The example configuration file lists the default empty configuration listing all the options (living documentation of the config file). Live examples are available here . The location of this configuration file is determined by first checking for the path specified through the -o option of the daos_agent command line, if not set then /etc/daos/daos_agent.yml is used. Refer to the example configuration file ( daos_agent.yml ) for latest information and examples.","title":"Agent Configuration File"},{"location":"admin/deployment/#agent-startup","text":"DAOS Agent is a standalone application to be run on each compute node. It can be configured to use secure communications (default) or can be allowed to communicate with the control plane over unencrypted channels. The following example shows daos_agent being configured to operate in insecure mode due to incomplete integration of certificate support as of the 0.6 release and configured to use a non-default agent configuration file. To start the DAOS Agent from the command line, run: $ daos_agent -i -o <'path to agent configuration file/daos_agent.yml'> & Alternatively, the DAOS Agent can be started as a systemd service. The DAOS Agent unit file is installed in the correct location when installing from RPMs. If you want to run the DAOS Agent without certificates (not recommended in production deployments), you need to add the -i option to the systemd ExecStart invocation (see below). If you wish to use systemd with a development build, you must copy the service file from utils/systemd to /usr/lib/systemd/system . Once the file is copied modify the ExecStart line to point to your in tree daos_agent binary. ExecStart=/usr/bin/daos_agent -i -o <'path to agent configuration file/daos_agent.yml'> Once the service file is installed, you can start daos_agent with the following commands: $ sudo systemctl daemon-reload $ sudo systemctl enable daos_agent.service $ sudo systemctl start daos_agent.service To check the component status use: $ sudo systemctl status daos_agent.service If DAOS Agent failed to start check the logs with: $ sudo journalctl --unit daos_agent.service","title":"Agent Startup"},{"location":"admin/deployment/#disable-agent-cache-optional","text":"In certain circumstances (e.g. for DAOS development or system evaluation), it may be desirable to disable the DAOS Agent's caching mechanism in order to avoid stale system information being retained across reformats of a system. The DAOS Agent normally caches a map of rank->fabric URI lookups as well as client network configuration data in order to reduce the number of management RPCs required to start an application. When this information becomes stale, the Agent must be restarted in order to repopulate the cache with new information. Alternatively, the caching mechanism may be disabled, with the tradeoff that each application launch will invoke management RPCs in order to obtain system connection information. To disable the DAOS Agent caching mechanism, set the following environment variable before starting the daos_agent process: DAOS_AGENT_DISABLE_CACHE=true If running from systemd, add the following to the daos_agent service file in the [Service] section before reloading systemd and restarting the daos_agent service: Environment=DAOS_AGENT_DISABLE_CACHE=true https://github.com/intel/ipmctl \u21a9 https://github.com/daos-stack/daos/tree/master/utils/config \u21a9 https://www.open-mpi.org/faq/?category=running#mpirun-hostfile \u21a9 https://github.com/daos-stack/daos/tree/master/src/control/README.md \u21a9 https://github.com/pmem/ndctl/issues/130 \u21a9","title":"Disable Agent Cache (Optional)"},{"location":"admin/env_variables/","text":"DAOS Environment Variables \u00b6 This section lists the environment variables used by DAOS. Warning Many of these variables are used for development purposes only, and may be removed or changed in the future. The description of each variable follows the following format: Short description Type The default behavior if not set. A longer description if necessary Type is defined by this table: Type Values BOOL 0 means false; any other value means true BOOL2 no means false; any other value means true BOOL3 set to empty, or any value means true; unset means false INTEGER Non-negative decimal integer STRING String Server environment variables \u00b6 Environment variables in this section only apply to the server side. Variable Description RDB_ELECTION_TIMEOUT Raft election timeout used by RDBs in milliseconds. INTEGER. Default to 7000 ms. RDB_REQUEST_TIMEOUT Raft request timeout used by RDBs in milliseconds. INTEGER. Default to 3000 ms. DAOS_REBUILD Determines whether to start rebuilds when excluding targets. BOOL2. Default to true. DAOS_MD_CAP Size of a metadata pmem pool/file in MBs. INTEGER. Default to 128 MB. DAOS_START_POOL_SVC Determines whether to start existing pool services when starting a daos_server. BOOL. Default to true. CRT_DISABLE_MEM_PIN Disable memory pinning workaround on a server side. BOOL. Default to 0. DAOS_SCHED_PRIO_DISABLED Disable server ULT prioritizing. BOOL. Default to 0. DAOS_SCHED_RELAX_MODE The mode of CPU relaxing on idle. \"disabled\":disable relaxing; \"net\":wait on network request for INTVL; \"sleep\":sleep for INTVL. STRING. Default to \"net\" DAOS_SCHED_RELAX_INTVL CPU relax interval in milliseconds. INTEGER. Default to 1 ms. Server and Client environment variables \u00b6 Environment variables in this section apply to both the server side and the client side. Variable Description FI_OFI_RXM_USE_SRX Enable shared receive buffers for RXM-based providers (verbs, tcp). BOOL. Auto-defaults to 1. FI_UNIVERSE_SIZE Sets expected universe size in OFI layer to be more than expected number of clients. INTEGER. Auto-defaults to 2048. Client environment variables \u00b6 Environment variables in this section only apply to the client side. Variable Description FI_MR_CACHE_MAX_COUNT Enable MR caching in OFI layer. Recommended to be set to 0 (disable) when CRT_DISABLE_MEM_PIN is NOT set to 1. INTEGER. Default to unset. Debug System (Client & Server) \u00b6 Variable Description D_LOG_FILE DAOS debug logs (both server and client) are written to stdout by default. The debug location can be modified by setting this environment variable (\"D_LOG_FILE=/tmp/daos_debug.log\"). D_LOG_FILE_APPEND_PID If set and not 0, causes the main PID to be appended at the end of D_LOG_FILE path name (both server and client). D_LOG_STDERR_IN_LOG If set and not 0, causes stderr messages to be merged in D_LOG_FILE. D_LOG_SIZE DAOS debug logs (both server and client) have a 1GB file size limit by default. When this limit is reached, the current log file is closed and renamed with a .old suffix, and a new one is opened. This mechanism will repeat each time the limit is reached, meaning that available saved log records could be found in both ${D_LOG_FILE} and last generation of ${D_LOG_FILE}.old files, to a maximum of the most recent 2*D_LOG_SIZE records. This can be modified by setting this environment variable (\"D_LOG_SIZE=536870912\"). Sizes can also be specified in human-readable form using k , m , g , K , M , and G . The lower-case specifiers are base-10 multipliers and the upper case specifiers are base-2 multipliers. DD_SUBSYS Used to specify which subsystems to enable. DD_SUBSYS can be set to individual subsystems for finer-grained debugging (\"DD_SUBSYS=vos\"), multiple facilities (\"DD_SUBSYS=eio,mgmt,misc,mem\"), or all facilities (\"DD_SUBSYS=all\") which is also the default setting. If a facility is not enabled, then only ERR messages or more severe messages will print. DD_STDERR Used to specify the priority level to output to stderr. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. By default, all CRIT and more severe DAOS messages will log to stderr (\"DD_STDERR=CRIT\"), and the default for CaRT/GURT is FATAL. D_LOG_MASK Used to specify what type/level of logging will be present for either all of the registered subsystems or a select few. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. DEBUG option is used to enable all logging (debug messages as well as all higher priority level messages). Note that if D_LOG_MASK is not set, it will default to logging all messages excluding debug (\"D_LOG_MASK=INFO\"). Example: \"D_LOG_MASK=DEBUG\". This will set the logging level for all facilities to DEBUG, meaning that all debug messages, as well as higher priority messages will be logged (INFO, NOTE, WARN, ERR, CRIT, FATAL). Example 2: \"D_LOG_MASK=DEBUG,MEM=ERR,RPC=ERR\". This will set the logging level to DEBUG for all facilities except MEM & RPC (which will now only log ERR and higher priority level messages, skipping all DEBUG, INFO, NOTE & WARN messages) DD_MASK Used to enable different debug streams for finer-grained debug messages, essentially allowing the user to specify an area of interest to debug (possibly involving many different subsystems) as opposed to parsing through many lines of generic DEBUG messages. All debug streams will be enabled by default (\"DD_MASK=all\"). Single debug masks can be set (\"DD_MASK=trace\") or multiple masks (\"DD_MASK=trace,test,mgmt\"). Note that since these debug streams are strictly related to the debug log messages, DD_LOG_MASK must be set to DEBUG. Priority messages higher than DEBUG will still be logged for all facilities unless otherwise specified by D_LOG_MASK (not affected by enabling debug masks).","title":"Environment Variables"},{"location":"admin/env_variables/#daos-environment-variables","text":"This section lists the environment variables used by DAOS. Warning Many of these variables are used for development purposes only, and may be removed or changed in the future. The description of each variable follows the following format: Short description Type The default behavior if not set. A longer description if necessary Type is defined by this table: Type Values BOOL 0 means false; any other value means true BOOL2 no means false; any other value means true BOOL3 set to empty, or any value means true; unset means false INTEGER Non-negative decimal integer STRING String","title":"DAOS Environment Variables"},{"location":"admin/env_variables/#server-environment-variables","text":"Environment variables in this section only apply to the server side. Variable Description RDB_ELECTION_TIMEOUT Raft election timeout used by RDBs in milliseconds. INTEGER. Default to 7000 ms. RDB_REQUEST_TIMEOUT Raft request timeout used by RDBs in milliseconds. INTEGER. Default to 3000 ms. DAOS_REBUILD Determines whether to start rebuilds when excluding targets. BOOL2. Default to true. DAOS_MD_CAP Size of a metadata pmem pool/file in MBs. INTEGER. Default to 128 MB. DAOS_START_POOL_SVC Determines whether to start existing pool services when starting a daos_server. BOOL. Default to true. CRT_DISABLE_MEM_PIN Disable memory pinning workaround on a server side. BOOL. Default to 0. DAOS_SCHED_PRIO_DISABLED Disable server ULT prioritizing. BOOL. Default to 0. DAOS_SCHED_RELAX_MODE The mode of CPU relaxing on idle. \"disabled\":disable relaxing; \"net\":wait on network request for INTVL; \"sleep\":sleep for INTVL. STRING. Default to \"net\" DAOS_SCHED_RELAX_INTVL CPU relax interval in milliseconds. INTEGER. Default to 1 ms.","title":"Server environment variables"},{"location":"admin/env_variables/#server-and-client-environment-variables","text":"Environment variables in this section apply to both the server side and the client side. Variable Description FI_OFI_RXM_USE_SRX Enable shared receive buffers for RXM-based providers (verbs, tcp). BOOL. Auto-defaults to 1. FI_UNIVERSE_SIZE Sets expected universe size in OFI layer to be more than expected number of clients. INTEGER. Auto-defaults to 2048.","title":"Server and Client environment variables"},{"location":"admin/env_variables/#client-environment-variables","text":"Environment variables in this section only apply to the client side. Variable Description FI_MR_CACHE_MAX_COUNT Enable MR caching in OFI layer. Recommended to be set to 0 (disable) when CRT_DISABLE_MEM_PIN is NOT set to 1. INTEGER. Default to unset.","title":"Client environment variables"},{"location":"admin/env_variables/#debug-system-client-server","text":"Variable Description D_LOG_FILE DAOS debug logs (both server and client) are written to stdout by default. The debug location can be modified by setting this environment variable (\"D_LOG_FILE=/tmp/daos_debug.log\"). D_LOG_FILE_APPEND_PID If set and not 0, causes the main PID to be appended at the end of D_LOG_FILE path name (both server and client). D_LOG_STDERR_IN_LOG If set and not 0, causes stderr messages to be merged in D_LOG_FILE. D_LOG_SIZE DAOS debug logs (both server and client) have a 1GB file size limit by default. When this limit is reached, the current log file is closed and renamed with a .old suffix, and a new one is opened. This mechanism will repeat each time the limit is reached, meaning that available saved log records could be found in both ${D_LOG_FILE} and last generation of ${D_LOG_FILE}.old files, to a maximum of the most recent 2*D_LOG_SIZE records. This can be modified by setting this environment variable (\"D_LOG_SIZE=536870912\"). Sizes can also be specified in human-readable form using k , m , g , K , M , and G . The lower-case specifiers are base-10 multipliers and the upper case specifiers are base-2 multipliers. DD_SUBSYS Used to specify which subsystems to enable. DD_SUBSYS can be set to individual subsystems for finer-grained debugging (\"DD_SUBSYS=vos\"), multiple facilities (\"DD_SUBSYS=eio,mgmt,misc,mem\"), or all facilities (\"DD_SUBSYS=all\") which is also the default setting. If a facility is not enabled, then only ERR messages or more severe messages will print. DD_STDERR Used to specify the priority level to output to stderr. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. By default, all CRIT and more severe DAOS messages will log to stderr (\"DD_STDERR=CRIT\"), and the default for CaRT/GURT is FATAL. D_LOG_MASK Used to specify what type/level of logging will be present for either all of the registered subsystems or a select few. Options in decreasing priority level order: FATAL, CRIT, ERR, WARN, NOTE, INFO, DEBUG. DEBUG option is used to enable all logging (debug messages as well as all higher priority level messages). Note that if D_LOG_MASK is not set, it will default to logging all messages excluding debug (\"D_LOG_MASK=INFO\"). Example: \"D_LOG_MASK=DEBUG\". This will set the logging level for all facilities to DEBUG, meaning that all debug messages, as well as higher priority messages will be logged (INFO, NOTE, WARN, ERR, CRIT, FATAL). Example 2: \"D_LOG_MASK=DEBUG,MEM=ERR,RPC=ERR\". This will set the logging level to DEBUG for all facilities except MEM & RPC (which will now only log ERR and higher priority level messages, skipping all DEBUG, INFO, NOTE & WARN messages) DD_MASK Used to enable different debug streams for finer-grained debug messages, essentially allowing the user to specify an area of interest to debug (possibly involving many different subsystems) as opposed to parsing through many lines of generic DEBUG messages. All debug streams will be enabled by default (\"DD_MASK=all\"). Single debug masks can be set (\"DD_MASK=trace\") or multiple masks (\"DD_MASK=trace,test,mgmt\"). Note that since these debug streams are strictly related to the debug log messages, DD_LOG_MASK must be set to DEBUG. Priority messages higher than DEBUG will still be logged for all facilities unless otherwise specified by D_LOG_MASK (not affected by enabling debug masks).","title":"Debug System (Client &amp; Server)"},{"location":"admin/hardware/","text":"Hardware Requirements \u00b6 The purpose of this section is to describe processor, storage, and network requirements to deploy a DAOS system. Deployment Options \u00b6 As illustrated in the figure below, a DAOS storage system can be deployed in two different ways: Pooled Storage Model : The DAOS servers can run on dedicated storage nodes in separate racks. This is a traditional pool model where storage is uniformly accessed by all compute nodes. In order to minimize the number of I/O racks and to optimize floor space, this approach usually requires high-density storage servers. Hyper-converged Storage Model : In this model, the storage nodes are integrated into compute racks and can be either dedicated or shared nodes. The DAOS servers are thus massively distributed, and storage access is non-uniform and must take locality into account. This model is common in hyper-converged infrastructure. While DAOS is mostly deployed following the pooled model, active research is conducted to efficiently support the hyper-converged model as well. Processor Requirements \u00b6 DAOS requires a 64-bit processor architecture and is primarily developed on Intel x86_64 architecture. The DAOS software and the libraries it depends on (e.g., ISA-L, SPDK, PMDK, and DPDK) can take advantage of Intel SSE and AVX extensions. Some success was also reported by the community on running the DAOS client on 64-bit ARM processors configured in Little Endian mode. That being said, ARM testing is not part of the current DAOS CI pipeline and is thus not validated on a regular basis. Network Requirements \u00b6 The DAOS network layer relies on libfabrics and supports OFI providers for Ethernet/sockets, InfiniBand/verbs and RoCE. An RDMA-capable fabric is preferred for better performance. DAOS can support multiple network interfaces by binding different engines and different client processes to individual network cards. The DAOS control plane provides methods for administering and managing the DAOS servers using a secure socket layer interface. An additional out-of-band network connecting the nodes in the DAOS service cluster is required for DAOS administration. Management traffic between clients and servers uses IP over Fabric. Storage Requirements \u00b6 DAOS requires each storage node to have direct access to storage-class memory (SCM). While DAOS is primarily tested and tuned for Optane Persistent Memory, the DAOS software stack is built over the Persistent Memory Development Kit (PMDK) and the DAX feature of the Linux operating systems as described in the SNIA NVM Programming Model 1 . As a result, the open-source DAOS software stack should be able to run transparently over any storage-class memory supported by the PMDK. The storage node can optionally be equipped with NVMe (non-volatile memory express) SSDs to provide capacity. HDDs, as well as SATA and SAS SSDs, are not supported by DAOS. Both NVMe 3D-NAND and Optane SSDs are supported. Optane SSDs are preferred for DAOS installation that targets a very high IOPS rate. NVMe-oF devices are also supported by the userspace storage stack but have never been tested. A minimum 6% ratio of SCM to SSD capacity will guarantee that DAOS has enough space in SCM to store its internal metadata (e.g., pool metadata, SSD block allocation tracking). For testing purposes, SCM can be emulated with DRAM by mounting a tmpfs filesystem, and NVMe SSDs can be also emulated with DRAM or a loopback file. Storage Server Design \u00b6 The hardware design of a DAOS storage server balances the network bandwidth of the fabric with the aggregate storage bandwidth of the NVMe storage devices. This relationship sets the number of NVMe drives. For example, 8 PCIe gen4 x4 NVMe SSDs balance two 200Gbps PCIe gen4 x16 network adapters. The capacity of the SSDs will determine the minimum capacity of the Optane PMem DIMMs needed to provide the 6% ratio for DAOS metadata. CPU Affinity \u00b6 Recent Intel Xeon data center platforms use two processor CPUs connected together with the Ultra Path Interconnect (UPI). PCIe lanes in these servers have a natural affinity to one CPU. Although globally accessible from any of the system cores, NVMe SSDs and network interface cards connected through the PCIe bus may provide different performance characteristics (e.g., higher latency, lower bandwidth) to each CPU. Accessing non-local PCIe devices may involve traffic over the UPI link that might become a point of congestion. Similarly, persistent memory is non-uniformly accessible (NUMA), and CPU affinity must be respected for maximal performance. Therefore, when running in a multi-socket and multi-rail environment, the DAOS service must be able to detect the CPU to PCIe device and persistent memory affinity and minimize as much as possible non-local access. This can be achieved by spawning one instance of the I/O Engine per CPU, then accessing only the persistent memory and PCI devices local to that CPU from that server instance. The DAOS control plane is responsible for detecting the storage and network affinity and starting the I/O Engines accordingly. Fault Domains \u00b6 DAOS relies on single-ported storage massively distributed across different storage nodes. Each storage node is thus a single point of failure. DAOS achieves fault tolerance by providing data redundancy across storage nodes in different fault domains. DAOS assumes that fault domains are hierarchical and do not overlap. For instance, the first level of a fault domain could be the racks and the second one, the storage nodes. For efficient placement and optimal data resilience, more fault domains are better. As a result, it is preferable to distribute storage nodes across as many racks as possible. https://www.snia.org/sites/default/files/ technical_work/final/NVMProgrammingModel_v1.2.pdf \u21a9","title":"Hardware Requirements"},{"location":"admin/hardware/#hardware-requirements","text":"The purpose of this section is to describe processor, storage, and network requirements to deploy a DAOS system.","title":"Hardware Requirements"},{"location":"admin/hardware/#deployment-options","text":"As illustrated in the figure below, a DAOS storage system can be deployed in two different ways: Pooled Storage Model : The DAOS servers can run on dedicated storage nodes in separate racks. This is a traditional pool model where storage is uniformly accessed by all compute nodes. In order to minimize the number of I/O racks and to optimize floor space, this approach usually requires high-density storage servers. Hyper-converged Storage Model : In this model, the storage nodes are integrated into compute racks and can be either dedicated or shared nodes. The DAOS servers are thus massively distributed, and storage access is non-uniform and must take locality into account. This model is common in hyper-converged infrastructure. While DAOS is mostly deployed following the pooled model, active research is conducted to efficiently support the hyper-converged model as well.","title":"Deployment Options"},{"location":"admin/hardware/#processor-requirements","text":"DAOS requires a 64-bit processor architecture and is primarily developed on Intel x86_64 architecture. The DAOS software and the libraries it depends on (e.g., ISA-L, SPDK, PMDK, and DPDK) can take advantage of Intel SSE and AVX extensions. Some success was also reported by the community on running the DAOS client on 64-bit ARM processors configured in Little Endian mode. That being said, ARM testing is not part of the current DAOS CI pipeline and is thus not validated on a regular basis.","title":"Processor Requirements"},{"location":"admin/hardware/#network-requirements","text":"The DAOS network layer relies on libfabrics and supports OFI providers for Ethernet/sockets, InfiniBand/verbs and RoCE. An RDMA-capable fabric is preferred for better performance. DAOS can support multiple network interfaces by binding different engines and different client processes to individual network cards. The DAOS control plane provides methods for administering and managing the DAOS servers using a secure socket layer interface. An additional out-of-band network connecting the nodes in the DAOS service cluster is required for DAOS administration. Management traffic between clients and servers uses IP over Fabric.","title":"Network Requirements"},{"location":"admin/hardware/#storage-requirements","text":"DAOS requires each storage node to have direct access to storage-class memory (SCM). While DAOS is primarily tested and tuned for Optane Persistent Memory, the DAOS software stack is built over the Persistent Memory Development Kit (PMDK) and the DAX feature of the Linux operating systems as described in the SNIA NVM Programming Model 1 . As a result, the open-source DAOS software stack should be able to run transparently over any storage-class memory supported by the PMDK. The storage node can optionally be equipped with NVMe (non-volatile memory express) SSDs to provide capacity. HDDs, as well as SATA and SAS SSDs, are not supported by DAOS. Both NVMe 3D-NAND and Optane SSDs are supported. Optane SSDs are preferred for DAOS installation that targets a very high IOPS rate. NVMe-oF devices are also supported by the userspace storage stack but have never been tested. A minimum 6% ratio of SCM to SSD capacity will guarantee that DAOS has enough space in SCM to store its internal metadata (e.g., pool metadata, SSD block allocation tracking). For testing purposes, SCM can be emulated with DRAM by mounting a tmpfs filesystem, and NVMe SSDs can be also emulated with DRAM or a loopback file.","title":"Storage Requirements"},{"location":"admin/hardware/#storage-server-design","text":"The hardware design of a DAOS storage server balances the network bandwidth of the fabric with the aggregate storage bandwidth of the NVMe storage devices. This relationship sets the number of NVMe drives. For example, 8 PCIe gen4 x4 NVMe SSDs balance two 200Gbps PCIe gen4 x16 network adapters. The capacity of the SSDs will determine the minimum capacity of the Optane PMem DIMMs needed to provide the 6% ratio for DAOS metadata.","title":"Storage Server Design"},{"location":"admin/hardware/#cpu-affinity","text":"Recent Intel Xeon data center platforms use two processor CPUs connected together with the Ultra Path Interconnect (UPI). PCIe lanes in these servers have a natural affinity to one CPU. Although globally accessible from any of the system cores, NVMe SSDs and network interface cards connected through the PCIe bus may provide different performance characteristics (e.g., higher latency, lower bandwidth) to each CPU. Accessing non-local PCIe devices may involve traffic over the UPI link that might become a point of congestion. Similarly, persistent memory is non-uniformly accessible (NUMA), and CPU affinity must be respected for maximal performance. Therefore, when running in a multi-socket and multi-rail environment, the DAOS service must be able to detect the CPU to PCIe device and persistent memory affinity and minimize as much as possible non-local access. This can be achieved by spawning one instance of the I/O Engine per CPU, then accessing only the persistent memory and PCI devices local to that CPU from that server instance. The DAOS control plane is responsible for detecting the storage and network affinity and starting the I/O Engines accordingly.","title":"CPU Affinity"},{"location":"admin/hardware/#fault-domains","text":"DAOS relies on single-ported storage massively distributed across different storage nodes. Each storage node is thus a single point of failure. DAOS achieves fault tolerance by providing data redundancy across storage nodes in different fault domains. DAOS assumes that fault domains are hierarchical and do not overlap. For instance, the first level of a fault domain could be the racks and the second one, the storage nodes. For efficient placement and optimal data resilience, more fault domains are better. As a result, it is preferable to distribute storage nodes across as many racks as possible. https://www.snia.org/sites/default/files/ technical_work/final/NVMProgrammingModel_v1.2.pdf \u21a9","title":"Fault Domains"},{"location":"admin/installation/","text":"Software Installation \u00b6 Please check the support matrix to select the appropriate software combination. Distribution Packages \u00b6 DAOS packages will be available for CentOS7, CentOS8 and openSUSE Leap 15 when DAOS 2.0 is released (planned for September 2021). DAOS from Scratch \u00b6 The following instructions have been verified with CentOS. Installations on other Linux distributions might be similar with some variations. Developers of DAOS may want to review the additional sections below before beginning, for suggestions related specifically to development. Contact us in our forum for further help with any issues. Build Prerequisites \u00b6 To build DAOS and its dependencies, several software packages must be installed on the system. This includes scons, libuuid, cmocka, ipmctl, and several other packages usually available on all the Linux distributions. Moreover, a Go version of at least 1.10 is required. An exhaustive list of packages for each supported Linux distribution is maintained in the Docker files (please click on the link): CentOS 7 openSUSE Leap 15 Ubuntu 20.04 The command lines to install the required packages can be extracted from the Docker files by removing the \"RUN\" command, which is specific to Docker. Check the utils/docker directory for different Linux distribution versions. Some DAOS tests use MPI. The DAOS build process uses the environment modules package to detect the presence of MPI. If none is found, the build will skip building those tests. DAOS Source Code \u00b6 The DAOS repository is hosted on GitHub . To checkout the latest development version, simply run: $ git clone --recurse-submodules https://github.com/daos-stack/daos.git This command clones the DAOS git repository (path referred as ${daospath} below) and initializes all the submodules automatically. Building DAOS & Dependencies \u00b6 If all the software dependencies listed previously are already satisfied, then type the following command in the top source directory to build the DAOS stack: scons-3 --config=force install If you are a developer of DAOS, we recommend following the instructions in the DAOS for Development section. Otherwise, the missing dependencies can be built automatically by invoking scons with the following parameters: scons-3 --config=force --build-deps=yes install By default, DAOS and its dependencies are installed under ${daospath}/install. The installation path can be modified by adding the PREFIX= option to the above command line (e.g., PREFIX=/usr/local). Note Several parameters can be set (e.g., COMPILER=clang or COMPILER=icc) on the scons command line. Please see scons-3 --help for all the possible options. Those options are also saved for future compilations. Environment setup \u00b6 Once built, the environment must be modified to search for binaries and header files in the installation path. This step is not required if standard locations (e.g. /bin, /sbin, /usr/lib, ...) are used. CPATH=${daospath}/install/include/:$CPATH PATH=${daospath}/install/bin/:${daospath}/install/sbin:$PATH export CPATH PATH If using bash, PATH can be set up for you after a build by sourcing the script utils/sl/setup_local.sh from the daos root. This script utilizes a file generated by the build to determine the location of daos and its dependencies. If required, ${daospath}/install must be replaced with the alternative path specified through PREFIX. DAOS in Docker \u00b6 This section describes how to build and run the DAOS service in a Docker container. A minimum of 5GB of DRAM and 16GB of disk space will be required. On Mac, please make sure that the Docker settings under \"Preferences/{Disk, Memory}\" are configured accordingly. Building a Docker Image \u00b6 To build the Docker image directly from GitHub, run the following command: $ docker build https://github.com/daos-stack/daos.git#master \\ -f utils/docker/Dockerfile.centos.7 -t daos or from a local tree: $ docker build . -f utils/docker/Dockerfile.centos.7 -t daos This creates a CentOS 7 image, fetches the latest DAOS version from GitHub, builds it, and installs it in the image. For Ubuntu and other Linux distributions, replace Dockerfile.centos.7 with Dockerfile.ubuntu.20.04 and the appropriate version of interest. Simple Docker Setup \u00b6 Once the image created, one can start a container that will eventually run the DAOS service: $ docker run -it -d --privileged --cap-add=ALL --name server -v /dev:/dev daos Note If you want to be more selective with the devices that are exported to the container, individual devices should be listed and exported as volume via the -v option. In this case, the hugepages devices should also be added to the command line via -v /dev/hugepages:/dev/hugepages and -v /dev/hugepages-1G:/dev/hugepages-1G Warning If Docker is being run on a non-Linux system (e.g., OSX), -v /dev:/dev should be removed from the command line. The daos_server_local.yml configuration file sets up a simple local DAOS system with a single server instance running in the container. By default, it uses 4GB of DRAM to emulate persistent memory and 16GB of bulk storage under /tmp. The storage size can be changed in the yaml file if necessary. The DAOS service can be started in the docker container as follows: $ docker exec server daos_server start \\ -o /home/daos/daos/utils/config/examples/daos_server_local.yml Note Please make sure that the uio_pci_generic module is loaded on the host. Once started, the DAOS server waits for the administrator to format the system. This can be triggered in a different shell, using the following command: $ docker exec server dmg -i storage format Upon successful completion of the format, the storage engine is started, and pools can be created using the daos admin tool (see next section). For more advanced configurations involving SCM, SSD or a real fabric, please refer to the next section.","title":"Software Installation"},{"location":"admin/installation/#software-installation","text":"Please check the support matrix to select the appropriate software combination.","title":"Software Installation"},{"location":"admin/installation/#distribution-packages","text":"DAOS packages will be available for CentOS7, CentOS8 and openSUSE Leap 15 when DAOS 2.0 is released (planned for September 2021).","title":"Distribution Packages"},{"location":"admin/installation/#daos-from-scratch","text":"The following instructions have been verified with CentOS. Installations on other Linux distributions might be similar with some variations. Developers of DAOS may want to review the additional sections below before beginning, for suggestions related specifically to development. Contact us in our forum for further help with any issues.","title":"DAOS from Scratch"},{"location":"admin/installation/#build-prerequisites","text":"To build DAOS and its dependencies, several software packages must be installed on the system. This includes scons, libuuid, cmocka, ipmctl, and several other packages usually available on all the Linux distributions. Moreover, a Go version of at least 1.10 is required. An exhaustive list of packages for each supported Linux distribution is maintained in the Docker files (please click on the link): CentOS 7 openSUSE Leap 15 Ubuntu 20.04 The command lines to install the required packages can be extracted from the Docker files by removing the \"RUN\" command, which is specific to Docker. Check the utils/docker directory for different Linux distribution versions. Some DAOS tests use MPI. The DAOS build process uses the environment modules package to detect the presence of MPI. If none is found, the build will skip building those tests.","title":"Build Prerequisites"},{"location":"admin/installation/#daos-source-code","text":"The DAOS repository is hosted on GitHub . To checkout the latest development version, simply run: $ git clone --recurse-submodules https://github.com/daos-stack/daos.git This command clones the DAOS git repository (path referred as ${daospath} below) and initializes all the submodules automatically.","title":"DAOS Source Code"},{"location":"admin/installation/#building-daos-dependencies","text":"If all the software dependencies listed previously are already satisfied, then type the following command in the top source directory to build the DAOS stack: scons-3 --config=force install If you are a developer of DAOS, we recommend following the instructions in the DAOS for Development section. Otherwise, the missing dependencies can be built automatically by invoking scons with the following parameters: scons-3 --config=force --build-deps=yes install By default, DAOS and its dependencies are installed under ${daospath}/install. The installation path can be modified by adding the PREFIX= option to the above command line (e.g., PREFIX=/usr/local). Note Several parameters can be set (e.g., COMPILER=clang or COMPILER=icc) on the scons command line. Please see scons-3 --help for all the possible options. Those options are also saved for future compilations.","title":"Building DAOS &amp; Dependencies"},{"location":"admin/installation/#environment-setup","text":"Once built, the environment must be modified to search for binaries and header files in the installation path. This step is not required if standard locations (e.g. /bin, /sbin, /usr/lib, ...) are used. CPATH=${daospath}/install/include/:$CPATH PATH=${daospath}/install/bin/:${daospath}/install/sbin:$PATH export CPATH PATH If using bash, PATH can be set up for you after a build by sourcing the script utils/sl/setup_local.sh from the daos root. This script utilizes a file generated by the build to determine the location of daos and its dependencies. If required, ${daospath}/install must be replaced with the alternative path specified through PREFIX.","title":"Environment setup"},{"location":"admin/installation/#daos-in-docker","text":"This section describes how to build and run the DAOS service in a Docker container. A minimum of 5GB of DRAM and 16GB of disk space will be required. On Mac, please make sure that the Docker settings under \"Preferences/{Disk, Memory}\" are configured accordingly.","title":"DAOS in Docker"},{"location":"admin/installation/#building-a-docker-image","text":"To build the Docker image directly from GitHub, run the following command: $ docker build https://github.com/daos-stack/daos.git#master \\ -f utils/docker/Dockerfile.centos.7 -t daos or from a local tree: $ docker build . -f utils/docker/Dockerfile.centos.7 -t daos This creates a CentOS 7 image, fetches the latest DAOS version from GitHub, builds it, and installs it in the image. For Ubuntu and other Linux distributions, replace Dockerfile.centos.7 with Dockerfile.ubuntu.20.04 and the appropriate version of interest.","title":"Building a Docker Image"},{"location":"admin/installation/#simple-docker-setup","text":"Once the image created, one can start a container that will eventually run the DAOS service: $ docker run -it -d --privileged --cap-add=ALL --name server -v /dev:/dev daos Note If you want to be more selective with the devices that are exported to the container, individual devices should be listed and exported as volume via the -v option. In this case, the hugepages devices should also be added to the command line via -v /dev/hugepages:/dev/hugepages and -v /dev/hugepages-1G:/dev/hugepages-1G Warning If Docker is being run on a non-Linux system (e.g., OSX), -v /dev:/dev should be removed from the command line. The daos_server_local.yml configuration file sets up a simple local DAOS system with a single server instance running in the container. By default, it uses 4GB of DRAM to emulate persistent memory and 16GB of bulk storage under /tmp. The storage size can be changed in the yaml file if necessary. The DAOS service can be started in the docker container as follows: $ docker exec server daos_server start \\ -o /home/daos/daos/utils/config/examples/daos_server_local.yml Note Please make sure that the uio_pci_generic module is loaded on the host. Once started, the DAOS server waits for the administrator to format the system. This can be triggered in a different shell, using the following command: $ docker exec server dmg -i storage format Upon successful completion of the format, the storage engine is started, and pools can be created using the daos admin tool (see next section). For more advanced configurations involving SCM, SSD or a real fabric, please refer to the next section.","title":"Simple Docker Setup"},{"location":"admin/performance_tuning/","text":"DAOS Performance Tuning \u00b6 Network Performance \u00b6 The DAOS CaRT layer can validate and benchmark network communications in the same context as an application and using the same networks/tuning options as regular DAOS. The CaRT self_test can run against the DAOS servers in a production environment in a non-destructive manner. CaRT self_test supports different message sizes, bulk transfers, multiple targets, and the following test scenarios: Selftest client to servers - where self_test issues RPCs directly to a list of servers. Cross-servers - where self_test sends instructions to the different servers that will issue cross-server RPCs. This model supports a many to many communication model. Building CaRT self_test \u00b6 The CaRT self_test and its tests are delivered as part of the daos_client and daos_tests distribution packages . It can also be built from scratch. $ git clone --recurse-submodules https://github.com/daos-stack/daos.git $ cd daos $ scons --build-deps=yes install $ cd install For detailed information, please refer to the DAOS build documentation section. Running CaRT self_test \u00b6 Instructions to run CaRT self_test are as follows. Start DAOS server self_test requires DAOS server to be running before attempt running self_test . For detailed instruction on how to start DAOS server, please refer to the server startup documentation. Dump system attachinfo self_test will use the address information in daos_server.attach_info_tmp file. To create such file, run the following command: ./bin/daos_agent dump-attachinfo -o ./daos_server.attach_info_tmp Prepare hostfile The list of nodes from which self_test will run can be specified in a hostfile (referred to as ${hostfile}). Hostfile used here is the same as the ones used by OpenMPI. For additional details, please refer to the mpirun documentation . Run CaRT self_test The example below uses an Ethernet interface and Sockets provider. In the self_test commands: Selftest client to servers - Replace the argument for --endpoint accordingly. Cross-servers - Replace the argument for --endpoint and --master-endpoint accordingly. For example, if you have 8 servers, you would specify --endpoint 0-7:0 and --master-endpoint 0-7:0 The commands below will run self_test benchmark using the following message sizes: b1048576 1Mb bulk transfer Get and Put b1048576 0 1Mb bulk transfer Get only 0 b1048576 1Mb bulk transfer Put only I2048 2Kb iovec Input and Output i2048 0 2Kb iovec Input only 0 i2048 2Kb iovec Output only For a full description of self_test usage, run: $ ./bin/self_test --help To run self_test in client-to-servers mode: (Assuming sockets provider over eth0) # Specify provider export CRT_PHY_ADDR_STR='ofi+sockets' # Specify interface export OFI_INTERFACE=eth0 # Specify domain; usually only required when running over ofi+verbs;ofi_rxm # For example in such configuration OFI_DOMAIN might be set to mlx5_0 # run fi_info --provider='verbs;ofi_rxm' in order to find an appropriate domain export OFI_DOMAIN=eth0 # Export additional CART-level environment variables as described in README.env # if needed. For example export D_LOG_FILE=/path/to/log will allow dumping of the # log into the file instead of stdout/stderr $ ./bin/self_test --group-name daos_server --endpoint 0-<MAX_SERVER-1>:0 \\ --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" \\ --max-inflight-rpcs 16 --repetitions 100 -p /path/to/attach_info To run self_test in cross-servers mode: $ ./bin/self_test --group-name daos_server --endpoint 0-<MAX_SERVER-1>:0 \\ --master-endpoint 0-<MAX_SERVER-1>:0 \\ --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" \\ --max-inflight-rpcs 16 --repetitions 100 -p /path/to/attach_info Note: Number of repetitions, max inflight rpcs, message sizes can be adjusted based on the particular test/experiment. Benchmarking DAOS \u00b6 DAOS can be benchmarked using several widely used IO benchmarks like IOR, mdtest, and FIO. There are several backends that can be used with those benchmarks. ior \u00b6 IOR ( https://github.com/hpc/ior ) with the following backends: The IOR APIs POSIX, MPIIO and HDF5 can be used with DAOS POSIX containers that are accessed over dfuse. This works without or with the I/O interception library ( libioil ). Performance is significantly better when using libioil . For detailed information on dfuse usage with the IO interception library, please refer to the POSIX DFUSE section . A custom DFS (DAOS File System) plugin for DAOS can be used by building IOR with DAOS support, and selecting API=DFS. This integrates IOR directly with the DAOS File System ( libdfs ), without requiring FUSE or an interception library. Please refer to the DAOS README in the hpc/ior repository for some basic instructions on how to use the DFS driver. When using the IOR API=MPIIO, the ROMIO ADIO driver for DAOS can be used by providing the daos:// prefix to the filename. This ADIO driver bypasses dfuse and directly invkes the libdfs calls to perform I/O to a DAOS POSIX container. The DAOS-enabled MPIIO driver is available in the upstream MPICH repository and included with Intel MPI. Please refer to the MPI-IO documentation . An HDF5 VOL connector for DAOS is under development. This maps the HDF5 data model directly to the DAOS data model, and works in conjunction with DAOS containers of --type=HDF5 (in contrast to DAOS container of --type=POSIX that are used for the other IOR APIs). Please refer the the HDF5 with DAOS documentation . IOR has several parameters to characterize performance. The main parameters to work with include: - transfer size (-t) - block size (-b) - segment size (-s) For more use cases, the IO-500 workloads are a good starting point to measure performance on a system: https://github.com/IO500/io500 mdtest \u00b6 mdtest is released in the same repository as IOR. The corresponding backends that are listed above support mdtest, except for the MPI-IO and HDF5 backends that were only designed to support IOR. The DAOS README in the hpc/ior repository includes some examples to run mdtest with DAOS. The IO-500 workloads for mdtest provide some good criteria for performance measurements. FIO \u00b6 A DAOS engine is integrated into FIO and available upstream. To build it, just run: $ git clone http://git.kernel.dk/fio.git $ cd fio $ ./configure $ make install If DAOS is installed via packages, it should be automatically detected. If not, please specific the path to the DAOS library and headers to configure as follows: $ CFLAGS=\"-I/path/to/daos/install/include\" LDFLAGS=\"-L/path/to/daos/install/lib64\" ./configure Once successfully build, once can run the default example: $ export POOL= # your pool UUID $ export CONT= # your container UUID $ fio ./examples/dfs.fio Please note that DAOS does not transfer data (i.e. zeros) over the network when reading a hole in a sparse POSIX file. Very high read bandwidth can thus be reported if fio reads unallocated extents in a file. It is thus a good practice to start fio with a first write phase. FIO can also be used to benchmark DAOS performance using dfuse and the interception library with all the POSIX based engines like sync and libaio. daos_perf \u00b6 Finally, DAOS provides a tool called daos_perf which allows benchmarking to the DAOS object API directly or to the internal VOS API, which bypasses the client and network stack and reports performance accessing the storage directly using VOS. For a full description of daos_perf usage, run: $ daos_perf --help Client Performance Tuning \u00b6 For best performance, a DAOS client should specifically bind itself to a NUMA node instead of leaving core allocation and memory binding to chance. This allows the DAOS Agent to detect the client's NUMA affinity from its PID and automatically assign a network interface with a matching NUMA node. The network interface provided in the GetAttachInfo response is used to initialize CaRT. To override the automatically assigned interface, the client should set the environment variable OFI_INTERFACE to match the desired network interface. The DAOS Agent scans the client machine on the first GetAttachInfo request to determine the set of network interfaces available that support the DAOS Server's OFI provider. This request occurs as part of the initialization sequence in the libdaos daos_init() performed by each client. Upon receipt, the Agent populates a cache of responses indexed by NUMA affinity. Provided a client application has bound itself to a specific NUMA node and that NUMA node has a network device associated with it, the DAOS Agent will provide a GetAttachInfo response with a network interface corresponding to the client's NUMA node. When more than one appropriate network interface exists per NUMA node, the agent uses a round-robin resource allocation scheme to load balance the responses for that NUMA node. If a client is bound to a NUMA node that has no matching network interface, then a default NUMA node is used for the purpose of selecting a response. Provided that the DAOS Agent can detect any valid network device on any NUMA node, the default response will contain a valid network interface for the client. When a default response is provided, a message in the Agent's log is emitted: No network devices bound to client NUMA node X. Using response from NUMA Y To improve performance, it is worth figuring out if the client bound itself to the wrong NUMA node, or if expected network devices for that NUMA node are missing from the Agent's fabric scan. In some situations, the Agent may detect no network devices and the response cache will be empty. In such a situation, the GetAttachInfo response will contain no interface assignment and the following info message will be found in the Agent's log: No network devices detected in fabric scan; default AttachInfo response may be incorrect In either situation, the admin may execute the command daos_agent net-scan with appropriate debug flags to gain more insight into the configuration problem. Disabling the GetAttachInfo cache: The default configuration enables the Agent GetAttachInfo cache. If it is desired, the cache may be disabled prior to DAOS Agent startup by setting the Agent's environment variable DAOS_AGENT_DISABLE_CACHE=true . The cache is loaded only at Agent startup. The following debug message will be found in the Agent's log: GetAttachInfo agent caching has been disabled If the network configuration changes while the Agent is running, it must be restarted to gain visibility to these changes. For additional information, please refer to the System Deployment: Agent Startup documentation section.","title":"Performance Tuning"},{"location":"admin/performance_tuning/#daos-performance-tuning","text":"","title":"DAOS Performance Tuning"},{"location":"admin/performance_tuning/#network-performance","text":"The DAOS CaRT layer can validate and benchmark network communications in the same context as an application and using the same networks/tuning options as regular DAOS. The CaRT self_test can run against the DAOS servers in a production environment in a non-destructive manner. CaRT self_test supports different message sizes, bulk transfers, multiple targets, and the following test scenarios: Selftest client to servers - where self_test issues RPCs directly to a list of servers. Cross-servers - where self_test sends instructions to the different servers that will issue cross-server RPCs. This model supports a many to many communication model.","title":"Network Performance"},{"location":"admin/performance_tuning/#building-cart-self_test","text":"The CaRT self_test and its tests are delivered as part of the daos_client and daos_tests distribution packages . It can also be built from scratch. $ git clone --recurse-submodules https://github.com/daos-stack/daos.git $ cd daos $ scons --build-deps=yes install $ cd install For detailed information, please refer to the DAOS build documentation section.","title":"Building CaRT self_test"},{"location":"admin/performance_tuning/#running-cart-self_test","text":"Instructions to run CaRT self_test are as follows. Start DAOS server self_test requires DAOS server to be running before attempt running self_test . For detailed instruction on how to start DAOS server, please refer to the server startup documentation. Dump system attachinfo self_test will use the address information in daos_server.attach_info_tmp file. To create such file, run the following command: ./bin/daos_agent dump-attachinfo -o ./daos_server.attach_info_tmp Prepare hostfile The list of nodes from which self_test will run can be specified in a hostfile (referred to as ${hostfile}). Hostfile used here is the same as the ones used by OpenMPI. For additional details, please refer to the mpirun documentation . Run CaRT self_test The example below uses an Ethernet interface and Sockets provider. In the self_test commands: Selftest client to servers - Replace the argument for --endpoint accordingly. Cross-servers - Replace the argument for --endpoint and --master-endpoint accordingly. For example, if you have 8 servers, you would specify --endpoint 0-7:0 and --master-endpoint 0-7:0 The commands below will run self_test benchmark using the following message sizes: b1048576 1Mb bulk transfer Get and Put b1048576 0 1Mb bulk transfer Get only 0 b1048576 1Mb bulk transfer Put only I2048 2Kb iovec Input and Output i2048 0 2Kb iovec Input only 0 i2048 2Kb iovec Output only For a full description of self_test usage, run: $ ./bin/self_test --help To run self_test in client-to-servers mode: (Assuming sockets provider over eth0) # Specify provider export CRT_PHY_ADDR_STR='ofi+sockets' # Specify interface export OFI_INTERFACE=eth0 # Specify domain; usually only required when running over ofi+verbs;ofi_rxm # For example in such configuration OFI_DOMAIN might be set to mlx5_0 # run fi_info --provider='verbs;ofi_rxm' in order to find an appropriate domain export OFI_DOMAIN=eth0 # Export additional CART-level environment variables as described in README.env # if needed. For example export D_LOG_FILE=/path/to/log will allow dumping of the # log into the file instead of stdout/stderr $ ./bin/self_test --group-name daos_server --endpoint 0-<MAX_SERVER-1>:0 \\ --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" \\ --max-inflight-rpcs 16 --repetitions 100 -p /path/to/attach_info To run self_test in cross-servers mode: $ ./bin/self_test --group-name daos_server --endpoint 0-<MAX_SERVER-1>:0 \\ --master-endpoint 0-<MAX_SERVER-1>:0 \\ --message-sizes \"b1048576,b1048576 0,0 b1048576,i2048,i2048 0,0 i2048\" \\ --max-inflight-rpcs 16 --repetitions 100 -p /path/to/attach_info Note: Number of repetitions, max inflight rpcs, message sizes can be adjusted based on the particular test/experiment.","title":"Running CaRT self_test"},{"location":"admin/performance_tuning/#benchmarking-daos","text":"DAOS can be benchmarked using several widely used IO benchmarks like IOR, mdtest, and FIO. There are several backends that can be used with those benchmarks.","title":"Benchmarking DAOS"},{"location":"admin/performance_tuning/#ior","text":"IOR ( https://github.com/hpc/ior ) with the following backends: The IOR APIs POSIX, MPIIO and HDF5 can be used with DAOS POSIX containers that are accessed over dfuse. This works without or with the I/O interception library ( libioil ). Performance is significantly better when using libioil . For detailed information on dfuse usage with the IO interception library, please refer to the POSIX DFUSE section . A custom DFS (DAOS File System) plugin for DAOS can be used by building IOR with DAOS support, and selecting API=DFS. This integrates IOR directly with the DAOS File System ( libdfs ), without requiring FUSE or an interception library. Please refer to the DAOS README in the hpc/ior repository for some basic instructions on how to use the DFS driver. When using the IOR API=MPIIO, the ROMIO ADIO driver for DAOS can be used by providing the daos:// prefix to the filename. This ADIO driver bypasses dfuse and directly invkes the libdfs calls to perform I/O to a DAOS POSIX container. The DAOS-enabled MPIIO driver is available in the upstream MPICH repository and included with Intel MPI. Please refer to the MPI-IO documentation . An HDF5 VOL connector for DAOS is under development. This maps the HDF5 data model directly to the DAOS data model, and works in conjunction with DAOS containers of --type=HDF5 (in contrast to DAOS container of --type=POSIX that are used for the other IOR APIs). Please refer the the HDF5 with DAOS documentation . IOR has several parameters to characterize performance. The main parameters to work with include: - transfer size (-t) - block size (-b) - segment size (-s) For more use cases, the IO-500 workloads are a good starting point to measure performance on a system: https://github.com/IO500/io500","title":"ior"},{"location":"admin/performance_tuning/#mdtest","text":"mdtest is released in the same repository as IOR. The corresponding backends that are listed above support mdtest, except for the MPI-IO and HDF5 backends that were only designed to support IOR. The DAOS README in the hpc/ior repository includes some examples to run mdtest with DAOS. The IO-500 workloads for mdtest provide some good criteria for performance measurements.","title":"mdtest"},{"location":"admin/performance_tuning/#fio","text":"A DAOS engine is integrated into FIO and available upstream. To build it, just run: $ git clone http://git.kernel.dk/fio.git $ cd fio $ ./configure $ make install If DAOS is installed via packages, it should be automatically detected. If not, please specific the path to the DAOS library and headers to configure as follows: $ CFLAGS=\"-I/path/to/daos/install/include\" LDFLAGS=\"-L/path/to/daos/install/lib64\" ./configure Once successfully build, once can run the default example: $ export POOL= # your pool UUID $ export CONT= # your container UUID $ fio ./examples/dfs.fio Please note that DAOS does not transfer data (i.e. zeros) over the network when reading a hole in a sparse POSIX file. Very high read bandwidth can thus be reported if fio reads unallocated extents in a file. It is thus a good practice to start fio with a first write phase. FIO can also be used to benchmark DAOS performance using dfuse and the interception library with all the POSIX based engines like sync and libaio.","title":"FIO"},{"location":"admin/performance_tuning/#daos_perf","text":"Finally, DAOS provides a tool called daos_perf which allows benchmarking to the DAOS object API directly or to the internal VOS API, which bypasses the client and network stack and reports performance accessing the storage directly using VOS. For a full description of daos_perf usage, run: $ daos_perf --help","title":"daos_perf"},{"location":"admin/performance_tuning/#client-performance-tuning","text":"For best performance, a DAOS client should specifically bind itself to a NUMA node instead of leaving core allocation and memory binding to chance. This allows the DAOS Agent to detect the client's NUMA affinity from its PID and automatically assign a network interface with a matching NUMA node. The network interface provided in the GetAttachInfo response is used to initialize CaRT. To override the automatically assigned interface, the client should set the environment variable OFI_INTERFACE to match the desired network interface. The DAOS Agent scans the client machine on the first GetAttachInfo request to determine the set of network interfaces available that support the DAOS Server's OFI provider. This request occurs as part of the initialization sequence in the libdaos daos_init() performed by each client. Upon receipt, the Agent populates a cache of responses indexed by NUMA affinity. Provided a client application has bound itself to a specific NUMA node and that NUMA node has a network device associated with it, the DAOS Agent will provide a GetAttachInfo response with a network interface corresponding to the client's NUMA node. When more than one appropriate network interface exists per NUMA node, the agent uses a round-robin resource allocation scheme to load balance the responses for that NUMA node. If a client is bound to a NUMA node that has no matching network interface, then a default NUMA node is used for the purpose of selecting a response. Provided that the DAOS Agent can detect any valid network device on any NUMA node, the default response will contain a valid network interface for the client. When a default response is provided, a message in the Agent's log is emitted: No network devices bound to client NUMA node X. Using response from NUMA Y To improve performance, it is worth figuring out if the client bound itself to the wrong NUMA node, or if expected network devices for that NUMA node are missing from the Agent's fabric scan. In some situations, the Agent may detect no network devices and the response cache will be empty. In such a situation, the GetAttachInfo response will contain no interface assignment and the following info message will be found in the Agent's log: No network devices detected in fabric scan; default AttachInfo response may be incorrect In either situation, the admin may execute the command daos_agent net-scan with appropriate debug flags to gain more insight into the configuration problem. Disabling the GetAttachInfo cache: The default configuration enables the Agent GetAttachInfo cache. If it is desired, the cache may be disabled prior to DAOS Agent startup by setting the Agent's environment variable DAOS_AGENT_DISABLE_CACHE=true . The cache is loaded only at Agent startup. The following debug message will be found in the Agent's log: GetAttachInfo agent caching has been disabled If the network configuration changes while the Agent is running, it must be restarted to gain visibility to these changes. For additional information, please refer to the System Deployment: Agent Startup documentation section.","title":"Client Performance Tuning"},{"location":"admin/pool_operations/","text":"Pool Operations \u00b6 A DAOS pool is a storage reservation that can span any storage nodes and is managed by the administrator. The amount of space allocated to a pool is decided at creation time and can eventually be expanded through the management interface or the dmg utility. Pool Creation/Destroy \u00b6 A DAOS pool can be created and destroyed through the dmg utility. To create a pool labeled tank : $ dmg pool create --size=NTB tank This command creates a pool labeled tank distributed across the DAOS servers with a target size on each server that is comprised of N TB of NVMe storage and N * 0.06 (i.e. 6% of NVMe) of SCM storage. The default SCM:NVMe ratio may be adjusted at pool creation time as described below. The UUID allocated to the newly created pool is printed to stdout as well as the pool service replica ranks. Note The --scm-size and --nvme-size options still exist, but should be considered deprecated and will likely be removed in a future release. The label must consist of alphanumeric characters, colon (':'), period ('.') or underscore ('_'). The maximum length is set to 127 characters. $ dmg pool create --help ... [create command options] -g, --group= DAOS pool to be owned by given group, format name@domain -u, --user= DAOS pool to be owned by given user, format name@domain -p, --label= Unique label for pool -P, --properties= Pool properties to be set -a, --acl-file= Access Control List file path for DAOS pool -z, --size= Total size of DAOS pool (auto) -t, --scm-ratio= Percentage of SCM:NVMe for pool storage (auto) (default: 6) -k, --nranks= Number of ranks to use (auto) -v, --nsvc= Number of pool service replicas -s, --scm-size= Per-server SCM allocation for DAOS pool (manual) -n, --nvme-size= Per-server NVMe allocation for DAOS pool (manual) -r, --ranks= Storage server unique identifiers (ranks) for DAOS pool The typical output of this command is as follows: $ dmg pool create --size 50GB --label tank Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 6.00% SCM/NVMe ratio ----------------------------------------- UUID : 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 Replica Ranks : [1-3] Target Ranks : [0-15] Size : 50 GB SCM : 3.0 GB (188 MB / rank) NVMe : 50 GB (3.2 GB / rank) This created a pool with UUID 8a05bf3a-a088-4a77-bb9f-df989fce7cc8, with redundancy enabled by default (pool service replicas on ranks 1-3). If no redundancy is desired, use --nsvc=1 in order to specify that only a single pool service replica should be created. To evict handles/connections to a pool: $ dmg pool evict tank Pool-evict command succeeded To see a list of the pools in your DAOS system: $ dmg pool list UUID Label Svc Replicas ---- ----- ------------ a0d17ecc-50d4-4bef-9908-01911e56827e test 0 93f63cea-98ca-4f8e-bdcf-2a9dde11c602 test1 0 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 tank 1-3 This returns a table of pool UUIDs, labels and the ranks of their pool service replicas. To destroy a pool: $ dmg pool destroy tank Pool-destroy command succeeded Pool Properties \u00b6 Properties are predefined parameters that the administrator can tune to control the behavior of a pool. Properties Management \u00b6 Current properties of an existing pool can be retrieved via the dmg pool get-prop command line. $ dmg pool get-prop tank Pool 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 properties: Name Value ---- ----- EC cell size (ec_cell_sz) 1.0 MiB Pool label (label) tank Reclaim strategy (reclaim) lazy Self-healing policy (self_heal) exclude Rebuild space ratio (space_rb) 0% All properties can be specified when creating the pool. $ dmg pool create --size 50GB --label tank2 --properties reclaim:disabled Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ----------------------------------------- UUID : 1f265216-5877-4302-ad29-aa0f90df3f86 Service Ranks : 0 Storage Ranks : [0-1] Total Size : 50 GB SCM : 50 GB (25 GB / rank) NVMe : 0 B (0 B / rank) $ dmg pool get-prop tank2 Pool 1f265216-5877-4302-ad29-aa0f90df3f86 properties: Name Value ---- ----- EC cell size (ec_cell_sz) 1.0 MiB Pool label (label) tank2 Reclaim strategy (reclaim) disabled Self-healing policy (self_heal) exclude Rebuild space ratio (space_rb) 0% Some properties can be modified after pool creation via the set-prop option. $ dmg pool set-prop tank2 reclaim:lazy pool set-prop succeeded $ dmg pool get-prop tank2 reclaim Pool 1f265216-5877-4302-ad29-aa0f90df3f86 properties: Name Value ---- ----- Reclaim strategy (reclaim) lazy Reclaim Strategy (reclaim) \u00b6 DAOS is a versioned object store that tags every I/O with an epoch number. This versioning mechanism is the baseline for multi-version concurrency control and snapshot support. Over time, unused versions need to be reclaimed in order to release storage space and also simplify the metadata index. This process is called aggregation. The reclaim property defines what strategy to use to reclaimed unused version. Three options are supported: \"lazy\" : Trigger aggregation only when there is no IO activities or SCM free space is under pressure (default strategy) \"time\" : Trigger aggregation regularly despite of IO activities. \"disabled\" : Never trigger aggregation. The system will eventually run out of space even if data is being deleted. Self-healing Policy (self_heal) \u00b6 This property defines whether a failing node is automatically evicted from the pool. Once excluded, the self-healing mechasnism will be triggered to restore the pool data redundancy on the surviving storage nodes. Reserve Space (space_rb) \u00b6 This property defines the percentage of total space reserved on each storage node for self-healing purpose. The reserved space cannot be consumed by the applications. EC Cell Size (ec_cell_sz) \u00b6 This property defines the default erasure code cell size inherited to DAOS containers. The value is typically between 32K and 1MB. Access Control Lists \u00b6 Client user and group access for pools are controlled by Access Control Lists (ACLs) . Most pool-related tasks are performed using the DMG administrative tool, which is authenticated by the administrative certificate rather than user-specific credentials. Access-controlled client pool accesses include: Connecting to the pool. Querying the pool. Creating containers in the pool. Deleting containers in the pool. This is reflected in the set of supported pool permissions . A user must be able to connect to the pool in order to access any containers inside, regardless of their permissions on those containers. Ownership \u00b6 Pool ownership conveys no special privileges for access control decisions. All desired privileges of the owner-user ( OWNER@ ) and owner-group ( GROUP@ ) must be explicitly defined by an administrator in the pool ACL. ACL at Pool Creation \u00b6 To create a pool with a custom ACL: $ dmg pool create --size <size> --acl-file <path> <pool_label> The ACL file format is detailed in here . Displaying ACL \u00b6 To view a pool's ACL: $ dmg pool get-acl --acl-file <path> <pool_label> The output is in the same string format used in the ACL file during creation, with one Access Control Entry (i.e., ACE) per line. An example output is presented below: $ dmg pool get-acl tank # Owner: jlombard@ # Owner Group: jlombard@ # Entries: A::OWNER@:rw A::bob@:r A:G:GROUP@:rw Modifying ACL \u00b6 For all of these commands using an ACL file, the ACL file must be in the format noted above for container creation. Overwriting ACL \u00b6 To replace a pool's ACL with a new ACL: $ dmg pool overwrite-acl --pool <UUID> --acl-file <path> Adding and Updating ACEs \u00b6 To add or update multiple entries in an existing pool ACL: $ dmg pool update-acl --acl-file <path> <pool_label> To add or update a single entry in an existing pool ACL: $ dmg pool update-acl --entry <ACE> <pool_label> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one. For instance: $ dmg pool update-acl -e A::kelsey@:r tank Pool-update-ACL command succeeded, UUID: 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 # Owner: jlombard@ # Owner Group: jlombard@ # Entries: A::OWNER@:rw A::bob@:r A::kelsey@:r A:G:GROUP@:rw Removing an ACE \u00b6 To delete an entry for a given principal in an existing pool ACL: $ dmg pool delete-acl --principal <principal> <pool_label> The principal corresponds to the principal portion of an ACE that was set during pool creation or a previous pool ACL operation. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ , GROUP@ , and EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the pool will be decided based on the remaining ACL rules. Pool Query \u00b6 The pool query operation retrieves information (i.e., the number of targets, space usage, rebuild status, property list, and more) about a created pool. It is integrated into the dmg utility. To query a pool: $ dmg pool query tank Below is the output for a pool created with SCM space only. pool=47293abe-aa6f-4147-97f6-42a9f796d64a Pool 47293abe-aa6f-4147-97f6-42a9f796d64a, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 28GB Free: 28GB, min:505MB, max:512MB, mean:512MB - NVMe: Total size: 0 Free: 0, min:0, max:0, mean:0 Rebuild done, 10 objs, 1026 recs The total and free sizes are the sum across all the targets whereas min/max/mean gives information about individual targets. A min value close to 0 means that one target is running out of space. NB: the Versioning Object Store (VOS) may reserve a portion of the SCM and NVMe allocations to mitigate fragmentation and for background operations (e.g., aggregation, garbage collection). The amount of storage set aside depends on the size of the target, and may take up 2+ GB. Therefore, out of space conditions may occur even while pool query may not reveal min approaching zero. The example below shows a rebuild in progress and NVMe space allocated. pool=95886b8b-7eb8-454d-845c-fc0ae0ba5671 Pool 95886b8b-7eb8-454d-845c-fc0ae0ba5671, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 28GB Free: 28GB, min:470MB, max:512MB, mean:509MB - NVMe: Total size: 56GB Free: 28GB, min:470MB, max:512MB, mean:509MB Rebuild busy, 75 objs, 9722 recs Additional status and telemetry data are planned to be exported through management tools and will be documented here once available. Pool Modifications \u00b6 Exclusion & Self-healing \u00b6 An operator can exclude one or more targets from a specific DAOS pool using the rank the target resides on as well as the target idx on that rank. If a target idx list is not provided then all targets on the rank will be excluded. Excluding a target will automatically start the rebuild process. To exclude a target from a pool: $ dmg pool exclude --rank=${rank} --target-idx=${idx1},${idx2},${idx3} <pool_label> The pool target exclude command accepts 2 parameters: The rank of the target(s) to be excluded. The target Indices of the targets to be excluded from that rank (optional). Drain \u00b6 Alternatively when an operator would like to remove one or more pool targets without the system operating in degraded mode Drain can be used. A pool drain operation will initiate rebuild without excluding the designated target until after the rebuild is complete. This allows the target(s) drained to continue to perform I/O while the rebuild operation is ongoing. Drain additionally enables non-replicated data to be rebuilt onto another target whereas in a conventional failure scenario non-replicated data would not be integrated into a rebuild and would be lost. To drain a target from a pool: $ dmg pool drain --rank=${rank} --target-idx=${idx1},${idx2},${idx3} <pool_label> The pool target drain command accepts 2 parameters: The rank of the target(s) to be drained. The target Indices of the targets to be drained from that rank (optional). Reintegration \u00b6 After a target failure an operator can fix the underlying issue and reintegrate the affected targets to restore the pool to its original state. The operator can either reintegrate specific targets for a rank by supplying a target idx list, or reintegrate an entire rank by omitting the list. $ dmg pool reintegrate --pool=${puuid} --rank=${rank} --target-idx=${idx1},${idx2},${idx3} The pool reintegrate command accepts 3 parameters: The pool UUID of the pool that the targets will be reintegrated into. The rank of the affected targets. The target Indices of the targets to be reintegrated on that rank (optional). When rebuild is triggered it will list the operations and their related targets by their rank ID and target index. Target (rank 5 idx 0) is down. Target (rank 5 idx 1) is down. ... (rank 5 idx 0) is excluded. (rank 5 idx 1) is excluded. These should be the same values used when reintegrating the targets. $ dmg pool reintegrate --rank=5 --target-idx=0,1 <pool_label> Pool Extension \u00b6 Addition & Space Rebalancing \u00b6 Full Support for online target addition and automatic space rebalancing is planned for a future release and will be documented here once available. Until then the following command(s) are placeholders and offer limited functionality related to Online Server Addition/Rebalancing operations. An operator can choose to extend a pool to include ranks not currently in the pool. This will automatically trigger a server rebalance operation where objects within the extended pool will be rebalanced across the new storage. $ dmg pool extend --pool=${puuid} --ranks=${rank1},${rank2}... <pool_label> The pool extend command accepts one required parameter which is a comma separated list of server ranks to include in the pool. The pool rebalance operation will work most efficiently when the pool is extended to its desired size in a single operation, as opposed to multiple, small extensions. Resize \u00b6 Support for quiescent pool resize (changing capacity used on each storage node without adding new ones) is currently not supported and is under consideration. Pool Catastrophic Recovery \u00b6 A DAOS pool is instantiated on each target by a set of pmemobj files managed by PMDK and SPDK blobs on SSDs. Tools to verify and repair this persistent data is scheduled for DAOS v2.4 and will be documented here once available. Meanwhile, PMDK provides a recovery tool (i.e., pmempool check) to verify and possibly repair a pmemobj file. As discussed in the previous section, the rebuild status can be consulted via the pool query and will be expanded with more information. Recovering Container Ownership \u00b6 Typically users are expected to manage their containers. However, in the event that a container is orphaned and no users have the privileges to change the ownership, an administrator can transfer ownership of the container to a new user and/or group. To change the owner user: $ dmg cont set-owner --pool <UUID> --cont <UUID> --user <owner-user> To change the owner group: $ dmg cont set-owner --pool <UUID> --cont <UUID> --group <owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals . Because this is an administrative action, it does not require the administrator to have any privileges assigned in the container ACL.","title":"Pool Operations"},{"location":"admin/pool_operations/#pool-operations","text":"A DAOS pool is a storage reservation that can span any storage nodes and is managed by the administrator. The amount of space allocated to a pool is decided at creation time and can eventually be expanded through the management interface or the dmg utility.","title":"Pool Operations"},{"location":"admin/pool_operations/#pool-creationdestroy","text":"A DAOS pool can be created and destroyed through the dmg utility. To create a pool labeled tank : $ dmg pool create --size=NTB tank This command creates a pool labeled tank distributed across the DAOS servers with a target size on each server that is comprised of N TB of NVMe storage and N * 0.06 (i.e. 6% of NVMe) of SCM storage. The default SCM:NVMe ratio may be adjusted at pool creation time as described below. The UUID allocated to the newly created pool is printed to stdout as well as the pool service replica ranks. Note The --scm-size and --nvme-size options still exist, but should be considered deprecated and will likely be removed in a future release. The label must consist of alphanumeric characters, colon (':'), period ('.') or underscore ('_'). The maximum length is set to 127 characters. $ dmg pool create --help ... [create command options] -g, --group= DAOS pool to be owned by given group, format name@domain -u, --user= DAOS pool to be owned by given user, format name@domain -p, --label= Unique label for pool -P, --properties= Pool properties to be set -a, --acl-file= Access Control List file path for DAOS pool -z, --size= Total size of DAOS pool (auto) -t, --scm-ratio= Percentage of SCM:NVMe for pool storage (auto) (default: 6) -k, --nranks= Number of ranks to use (auto) -v, --nsvc= Number of pool service replicas -s, --scm-size= Per-server SCM allocation for DAOS pool (manual) -n, --nvme-size= Per-server NVMe allocation for DAOS pool (manual) -r, --ranks= Storage server unique identifiers (ranks) for DAOS pool The typical output of this command is as follows: $ dmg pool create --size 50GB --label tank Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 6.00% SCM/NVMe ratio ----------------------------------------- UUID : 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 Replica Ranks : [1-3] Target Ranks : [0-15] Size : 50 GB SCM : 3.0 GB (188 MB / rank) NVMe : 50 GB (3.2 GB / rank) This created a pool with UUID 8a05bf3a-a088-4a77-bb9f-df989fce7cc8, with redundancy enabled by default (pool service replicas on ranks 1-3). If no redundancy is desired, use --nsvc=1 in order to specify that only a single pool service replica should be created. To evict handles/connections to a pool: $ dmg pool evict tank Pool-evict command succeeded To see a list of the pools in your DAOS system: $ dmg pool list UUID Label Svc Replicas ---- ----- ------------ a0d17ecc-50d4-4bef-9908-01911e56827e test 0 93f63cea-98ca-4f8e-bdcf-2a9dde11c602 test1 0 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 tank 1-3 This returns a table of pool UUIDs, labels and the ranks of their pool service replicas. To destroy a pool: $ dmg pool destroy tank Pool-destroy command succeeded","title":"Pool Creation/Destroy"},{"location":"admin/pool_operations/#pool-properties","text":"Properties are predefined parameters that the administrator can tune to control the behavior of a pool.","title":"Pool Properties"},{"location":"admin/pool_operations/#properties-management","text":"Current properties of an existing pool can be retrieved via the dmg pool get-prop command line. $ dmg pool get-prop tank Pool 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 properties: Name Value ---- ----- EC cell size (ec_cell_sz) 1.0 MiB Pool label (label) tank Reclaim strategy (reclaim) lazy Self-healing policy (self_heal) exclude Rebuild space ratio (space_rb) 0% All properties can be specified when creating the pool. $ dmg pool create --size 50GB --label tank2 --properties reclaim:disabled Creating DAOS pool with automatic storage allocation: 50 GB NVMe + 6.00% SCM Pool created with 100.00% SCM/NVMe ratio ----------------------------------------- UUID : 1f265216-5877-4302-ad29-aa0f90df3f86 Service Ranks : 0 Storage Ranks : [0-1] Total Size : 50 GB SCM : 50 GB (25 GB / rank) NVMe : 0 B (0 B / rank) $ dmg pool get-prop tank2 Pool 1f265216-5877-4302-ad29-aa0f90df3f86 properties: Name Value ---- ----- EC cell size (ec_cell_sz) 1.0 MiB Pool label (label) tank2 Reclaim strategy (reclaim) disabled Self-healing policy (self_heal) exclude Rebuild space ratio (space_rb) 0% Some properties can be modified after pool creation via the set-prop option. $ dmg pool set-prop tank2 reclaim:lazy pool set-prop succeeded $ dmg pool get-prop tank2 reclaim Pool 1f265216-5877-4302-ad29-aa0f90df3f86 properties: Name Value ---- ----- Reclaim strategy (reclaim) lazy","title":"Properties Management"},{"location":"admin/pool_operations/#reclaim-strategy-reclaim","text":"DAOS is a versioned object store that tags every I/O with an epoch number. This versioning mechanism is the baseline for multi-version concurrency control and snapshot support. Over time, unused versions need to be reclaimed in order to release storage space and also simplify the metadata index. This process is called aggregation. The reclaim property defines what strategy to use to reclaimed unused version. Three options are supported: \"lazy\" : Trigger aggregation only when there is no IO activities or SCM free space is under pressure (default strategy) \"time\" : Trigger aggregation regularly despite of IO activities. \"disabled\" : Never trigger aggregation. The system will eventually run out of space even if data is being deleted.","title":"Reclaim Strategy (reclaim)"},{"location":"admin/pool_operations/#self-healing-policy-self_heal","text":"This property defines whether a failing node is automatically evicted from the pool. Once excluded, the self-healing mechasnism will be triggered to restore the pool data redundancy on the surviving storage nodes.","title":"Self-healing Policy (self_heal)"},{"location":"admin/pool_operations/#reserve-space-space_rb","text":"This property defines the percentage of total space reserved on each storage node for self-healing purpose. The reserved space cannot be consumed by the applications.","title":"Reserve Space (space_rb)"},{"location":"admin/pool_operations/#ec-cell-size-ec_cell_sz","text":"This property defines the default erasure code cell size inherited to DAOS containers. The value is typically between 32K and 1MB.","title":"EC Cell Size (ec_cell_sz)"},{"location":"admin/pool_operations/#access-control-lists","text":"Client user and group access for pools are controlled by Access Control Lists (ACLs) . Most pool-related tasks are performed using the DMG administrative tool, which is authenticated by the administrative certificate rather than user-specific credentials. Access-controlled client pool accesses include: Connecting to the pool. Querying the pool. Creating containers in the pool. Deleting containers in the pool. This is reflected in the set of supported pool permissions . A user must be able to connect to the pool in order to access any containers inside, regardless of their permissions on those containers.","title":"Access Control Lists"},{"location":"admin/pool_operations/#ownership","text":"Pool ownership conveys no special privileges for access control decisions. All desired privileges of the owner-user ( OWNER@ ) and owner-group ( GROUP@ ) must be explicitly defined by an administrator in the pool ACL.","title":"Ownership"},{"location":"admin/pool_operations/#acl-at-pool-creation","text":"To create a pool with a custom ACL: $ dmg pool create --size <size> --acl-file <path> <pool_label> The ACL file format is detailed in here .","title":"ACL at Pool Creation"},{"location":"admin/pool_operations/#displaying-acl","text":"To view a pool's ACL: $ dmg pool get-acl --acl-file <path> <pool_label> The output is in the same string format used in the ACL file during creation, with one Access Control Entry (i.e., ACE) per line. An example output is presented below: $ dmg pool get-acl tank # Owner: jlombard@ # Owner Group: jlombard@ # Entries: A::OWNER@:rw A::bob@:r A:G:GROUP@:rw","title":"Displaying ACL"},{"location":"admin/pool_operations/#modifying-acl","text":"For all of these commands using an ACL file, the ACL file must be in the format noted above for container creation.","title":"Modifying ACL"},{"location":"admin/pool_operations/#overwriting-acl","text":"To replace a pool's ACL with a new ACL: $ dmg pool overwrite-acl --pool <UUID> --acl-file <path>","title":"Overwriting ACL"},{"location":"admin/pool_operations/#adding-and-updating-aces","text":"To add or update multiple entries in an existing pool ACL: $ dmg pool update-acl --acl-file <path> <pool_label> To add or update a single entry in an existing pool ACL: $ dmg pool update-acl --entry <ACE> <pool_label> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one. For instance: $ dmg pool update-acl -e A::kelsey@:r tank Pool-update-ACL command succeeded, UUID: 8a05bf3a-a088-4a77-bb9f-df989fce7cc8 # Owner: jlombard@ # Owner Group: jlombard@ # Entries: A::OWNER@:rw A::bob@:r A::kelsey@:r A:G:GROUP@:rw","title":"Adding and Updating ACEs"},{"location":"admin/pool_operations/#removing-an-ace","text":"To delete an entry for a given principal in an existing pool ACL: $ dmg pool delete-acl --principal <principal> <pool_label> The principal corresponds to the principal portion of an ACE that was set during pool creation or a previous pool ACL operation. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ , GROUP@ , and EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the pool will be decided based on the remaining ACL rules.","title":"Removing an ACE"},{"location":"admin/pool_operations/#pool-query","text":"The pool query operation retrieves information (i.e., the number of targets, space usage, rebuild status, property list, and more) about a created pool. It is integrated into the dmg utility. To query a pool: $ dmg pool query tank Below is the output for a pool created with SCM space only. pool=47293abe-aa6f-4147-97f6-42a9f796d64a Pool 47293abe-aa6f-4147-97f6-42a9f796d64a, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 28GB Free: 28GB, min:505MB, max:512MB, mean:512MB - NVMe: Total size: 0 Free: 0, min:0, max:0, mean:0 Rebuild done, 10 objs, 1026 recs The total and free sizes are the sum across all the targets whereas min/max/mean gives information about individual targets. A min value close to 0 means that one target is running out of space. NB: the Versioning Object Store (VOS) may reserve a portion of the SCM and NVMe allocations to mitigate fragmentation and for background operations (e.g., aggregation, garbage collection). The amount of storage set aside depends on the size of the target, and may take up 2+ GB. Therefore, out of space conditions may occur even while pool query may not reveal min approaching zero. The example below shows a rebuild in progress and NVMe space allocated. pool=95886b8b-7eb8-454d-845c-fc0ae0ba5671 Pool 95886b8b-7eb8-454d-845c-fc0ae0ba5671, ntarget=64, disabled=8 Pool space info: - Target(VOS) count:56 - SCM: Total size: 28GB Free: 28GB, min:470MB, max:512MB, mean:509MB - NVMe: Total size: 56GB Free: 28GB, min:470MB, max:512MB, mean:509MB Rebuild busy, 75 objs, 9722 recs Additional status and telemetry data are planned to be exported through management tools and will be documented here once available.","title":"Pool Query"},{"location":"admin/pool_operations/#pool-modifications","text":"","title":"Pool Modifications"},{"location":"admin/pool_operations/#exclusion-self-healing","text":"An operator can exclude one or more targets from a specific DAOS pool using the rank the target resides on as well as the target idx on that rank. If a target idx list is not provided then all targets on the rank will be excluded. Excluding a target will automatically start the rebuild process. To exclude a target from a pool: $ dmg pool exclude --rank=${rank} --target-idx=${idx1},${idx2},${idx3} <pool_label> The pool target exclude command accepts 2 parameters: The rank of the target(s) to be excluded. The target Indices of the targets to be excluded from that rank (optional).","title":"Exclusion &amp; Self-healing"},{"location":"admin/pool_operations/#drain","text":"Alternatively when an operator would like to remove one or more pool targets without the system operating in degraded mode Drain can be used. A pool drain operation will initiate rebuild without excluding the designated target until after the rebuild is complete. This allows the target(s) drained to continue to perform I/O while the rebuild operation is ongoing. Drain additionally enables non-replicated data to be rebuilt onto another target whereas in a conventional failure scenario non-replicated data would not be integrated into a rebuild and would be lost. To drain a target from a pool: $ dmg pool drain --rank=${rank} --target-idx=${idx1},${idx2},${idx3} <pool_label> The pool target drain command accepts 2 parameters: The rank of the target(s) to be drained. The target Indices of the targets to be drained from that rank (optional).","title":"Drain"},{"location":"admin/pool_operations/#reintegration","text":"After a target failure an operator can fix the underlying issue and reintegrate the affected targets to restore the pool to its original state. The operator can either reintegrate specific targets for a rank by supplying a target idx list, or reintegrate an entire rank by omitting the list. $ dmg pool reintegrate --pool=${puuid} --rank=${rank} --target-idx=${idx1},${idx2},${idx3} The pool reintegrate command accepts 3 parameters: The pool UUID of the pool that the targets will be reintegrated into. The rank of the affected targets. The target Indices of the targets to be reintegrated on that rank (optional). When rebuild is triggered it will list the operations and their related targets by their rank ID and target index. Target (rank 5 idx 0) is down. Target (rank 5 idx 1) is down. ... (rank 5 idx 0) is excluded. (rank 5 idx 1) is excluded. These should be the same values used when reintegrating the targets. $ dmg pool reintegrate --rank=5 --target-idx=0,1 <pool_label>","title":"Reintegration"},{"location":"admin/pool_operations/#pool-extension","text":"","title":"Pool Extension"},{"location":"admin/pool_operations/#addition-space-rebalancing","text":"Full Support for online target addition and automatic space rebalancing is planned for a future release and will be documented here once available. Until then the following command(s) are placeholders and offer limited functionality related to Online Server Addition/Rebalancing operations. An operator can choose to extend a pool to include ranks not currently in the pool. This will automatically trigger a server rebalance operation where objects within the extended pool will be rebalanced across the new storage. $ dmg pool extend --pool=${puuid} --ranks=${rank1},${rank2}... <pool_label> The pool extend command accepts one required parameter which is a comma separated list of server ranks to include in the pool. The pool rebalance operation will work most efficiently when the pool is extended to its desired size in a single operation, as opposed to multiple, small extensions.","title":"Addition &amp; Space Rebalancing"},{"location":"admin/pool_operations/#resize","text":"Support for quiescent pool resize (changing capacity used on each storage node without adding new ones) is currently not supported and is under consideration.","title":"Resize"},{"location":"admin/pool_operations/#pool-catastrophic-recovery","text":"A DAOS pool is instantiated on each target by a set of pmemobj files managed by PMDK and SPDK blobs on SSDs. Tools to verify and repair this persistent data is scheduled for DAOS v2.4 and will be documented here once available. Meanwhile, PMDK provides a recovery tool (i.e., pmempool check) to verify and possibly repair a pmemobj file. As discussed in the previous section, the rebuild status can be consulted via the pool query and will be expanded with more information.","title":"Pool Catastrophic Recovery"},{"location":"admin/pool_operations/#recovering-container-ownership","text":"Typically users are expected to manage their containers. However, in the event that a container is orphaned and no users have the privileges to change the ownership, an administrator can transfer ownership of the container to a new user and/or group. To change the owner user: $ dmg cont set-owner --pool <UUID> --cont <UUID> --user <owner-user> To change the owner group: $ dmg cont set-owner --pool <UUID> --cont <UUID> --group <owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals . Because this is an administrative action, it does not require the administrator to have any privileges assigned in the container ACL.","title":"Recovering Container Ownership"},{"location":"admin/predeployment_check/","text":"Pre-deployment Checklist \u00b6 This section covers the preliminary setup required on the compute and storage nodes before deploying DAOS. Enable IOMMU \u00b6 In order to run the DAOS server as a non-root user with NVMe devices, the hardware must support virtualized device access, and it must be enabled in the system BIOS. On Intel\u00ae systems, this capability is named Intel\u00ae Virtualization Technology for Directed I/O (VT-d). Once enabled in BIOS, IOMMU support must also be enabled in the Linux kernel. Exact details depend on the distribution, but the following example should be illustrative: # Enable IOMMU on CentOS 7 # All commands must be run as root/sudo! $ sudo vi /etc/default/grub # add the following line: GRUB_CMDLINE_LINUX_DEFAULT=\"intel_iommu=on\" # after saving the file, run the following to reconfigure # the bootloader: $ sudo grub2-mkconfig --output=/boot/grub2/grub.cfg # if the command completed with no errors, reboot the system # in order to make the changes take effect $ sudo reboot Note To force SPDK to use UIO rather than VFIO at daos_server runtime, set 'disable_vfio' in the server config file , but note that this will require running daos_server as root. Time Synchronization \u00b6 The DAOS transaction model relies on timestamps and requires time to be synchronized across all the storage and client nodes. This can be done using NTP or any other equivalent protocol. User/Group Synchronization \u00b6 DAOS ACLs store the actual user and group names (instead of numeric IDs), and therefore the servers do not need access to a synchronized user/group database. The DAOS Agent (running on the client nodes) is responsible for resolving UID/GID to user/group names which are added to a signed credential and sent to the DAOS storage nodes. Multi-rail/NIC Setup \u00b6 Storage nodes can be configured with multiple network interfaces to run multiple engine instances. Some special configuration is required to use librdmacm with multiple interfaces. First, the accept_local feature must be enabled on the network interfaces to be used by DAOS. This can be done using the following command ( must be replaced with the interface names): $ sudo sysctl -w net.ipv4.conf.all.accept_local=1 Second, Linux must be configured to only send ARP replies on the interface targeted in the ARP request. This is configured via the arp_ignore parameter. This should be set to 2 if all the IPoIB interfaces on the client and storage nodes are in the same logical subnet (e.g. ib0 == 10.0.0.27, ib1 == 10.0.1.27, prefix=16). $ sysctl -w net.ipv4.conf.all.arp_ignore=2 If separate logical subnets are used (e.g. prefix = 24), then the value must be set to 1. $ sysctl -w net.ipv4.conf.all.arp_ignore=1 Finally, the rp_filter is set to 1 by default on several distributions (e.g. on CentOS 7) and should be set to either 0 or 2, with 2 being more secure. This is true even if the configuration uses a single logical subnet. $ sysctl -w net.ipv4.conf.<ifaces>.rp_filter=2 All those parameters can be made persistent in /etc/sysctl.conf by adding a new sysctl file under /etc/sysctl.d (e.g. /etc/sysctl.d/95-daos-net.conf) with all the relevant settings. For more information, please refer to the librdmacm documentation Subnet \u00b6 Since all engines need to be able to communicate, the different network interfaces need to be on the same subnet or routing capabilities across the different subnet must be configured. Runtime Directory Setup \u00b6 DAOS uses a series of Unix Domain Sockets to communicate between its various components. On modern Linux systems, Unix Domain Sockets are typically stored under /run or /var/run (usually a symlink to /run) and are a mounted tmpfs file system. There are several methods for ensuring the necessary directories are setup. A sign that this step may have been missed is when starting daos_server or daos_agent, you may see the message: $ mkdir /var/run/daos_server: permission denied Unable to create socket directory: /var/run/daos_server Non-default Directory \u00b6 By default, daos_server and daos_agent will use the directories /var/run/daos_server and /var/run/daos_agent respectively. To change the default location that daos_server uses for its runtime directory, either uncomment and set the socket_dir configuration value in install/etc/daos_server.yml, or pass the location to daos_server on the command line using the -d flag ( daos_server start -d /tmp/daos_server ). For the daos_agent, an alternate location can be passed on the command line using the --runtime_dir flag ( daos_agent -d /tmp/daos_agent ). Default Directory (non-persistent) \u00b6 Files and directories created in /run and /var/run only survive until the next reboot. These directories are required for subsequent runs; therefore, if reboots are infrequent, an easy solution while still utilizing the default locations is to create the required directories manually. To do this execute the following commands. daos_server: $ mkdir /var/run/daos_server $ chmod 0755 /var/run/daos_server $ chown user:user /var/run/daos_server (where user is the user you will run daos_server as) daos_agent: $ mkdir /var/run/daos_agent $ chmod 0755 /var/run/daos_agent $ chown user:user /var/run/daos_agent (where user is the user you will run daos_agent as) Default Directory (persistent) \u00b6 If the server hosting daos_server or daos_agent will be rebooted often, systemd provides a persistent mechanism for creating the required directories called tmpfiles.d. This mechanism will be required every time the system is provisioned and requires a reboot to take effect. To tell systemd to create the necessary directories for DAOS: Copy the file utils/systemd/daosfiles.conf to /etc/tmpfiles.d\\ cp utils/systemd/daosfiles.conf /etc/tmpfiles.d Modify the copied file to change the user and group fields (currently daos) to the user daos will be run as Reboot the system, and the directories will be created automatically on all subsequent reboots. Elevated Privileges \u00b6 DAOS employs a privileged helper binary ( daos_admin ) to perform tasks that require elevated privileges on behalf of daos_server . Privileged Helper Configuration \u00b6 When DAOS is installed from RPM, the daos_admin helper is automatically installed to the correct location with the correct permissions. The RPM creates a \"daos_server\" system group and configures permissions such that daos_admin may only be invoked from daos_server . For non-RPM installations, there are two supported scenarios: daos_server is run as root, which means that daos_admin is also invoked as root, and therefore no additional setup is necessary. daos_server is run as a non-root user, which means that daos_admin must be manually installed and configured. The steps to enable the second scenario are as follows (steps are assumed to be running out of a DAOS source tree which may be on a NFS share): $ chmod -x $daospath/bin/daos_admin # prevent this copy from being executed $ sudo cp $daospath/bin/daos_admin /usr/bin/daos_admin $ sudo chmod 4755 /usr/bin/daos_admin # make this copy setuid root $ sudo mkdir -p /usr/share/daos/control # create symlinks to SPDK scripts $ sudo ln -sf $daospath/share/daos/control/setup_spdk.sh \\ /usr/share/daos/control $ sudo mkdir -p /usr/share/spdk/scripts $ sudo ln -sf $daospath/share/spdk/scripts/setup.sh \\ /usr/share/spdk/scripts $ sudo ln -sf $daospath/share/spdk/scripts/common.sh \\ /usr/share/spdk/scripts $ sudo ln -s $daospath/include \\ /usr/share/spdk/include Note The RPM installation is preferred for production scenarios. Manual installation is most appropriate for development and predeployment proof-of-concept scenarios. Memory Lock Limits \u00b6 Low ulimit for memlock can cause SPDK to fail and emit the following error: daos_engine:1 EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) The memlock limit only needs to be manually adjusted when daos_server is not running as a systemd service. Default ulimit settings vary between OSes. For RPM installations, the service will typically be launched by systemd and the limit is pre-set to unlimited in the daos_server.service unit file: https://github.com/daos-stack/daos/blob/master/utils/systemd/daos_server.service#L16. Note that values set in /etc/security/limits.conf are ignored by services launched by systemd. For non-RPM installations where daos_server is launched directly from the commandline, limits should be adjusted in /etc/security/limits.conf as per https://software.intel.com/content/www/us/en/develop/blogs/best-known-methods-for-setting-locked-memory-size.html.","title":"Pre-deployment Checklist"},{"location":"admin/predeployment_check/#pre-deployment-checklist","text":"This section covers the preliminary setup required on the compute and storage nodes before deploying DAOS.","title":"Pre-deployment Checklist"},{"location":"admin/predeployment_check/#enable-iommu","text":"In order to run the DAOS server as a non-root user with NVMe devices, the hardware must support virtualized device access, and it must be enabled in the system BIOS. On Intel\u00ae systems, this capability is named Intel\u00ae Virtualization Technology for Directed I/O (VT-d). Once enabled in BIOS, IOMMU support must also be enabled in the Linux kernel. Exact details depend on the distribution, but the following example should be illustrative: # Enable IOMMU on CentOS 7 # All commands must be run as root/sudo! $ sudo vi /etc/default/grub # add the following line: GRUB_CMDLINE_LINUX_DEFAULT=\"intel_iommu=on\" # after saving the file, run the following to reconfigure # the bootloader: $ sudo grub2-mkconfig --output=/boot/grub2/grub.cfg # if the command completed with no errors, reboot the system # in order to make the changes take effect $ sudo reboot Note To force SPDK to use UIO rather than VFIO at daos_server runtime, set 'disable_vfio' in the server config file , but note that this will require running daos_server as root.","title":"Enable IOMMU"},{"location":"admin/predeployment_check/#time-synchronization","text":"The DAOS transaction model relies on timestamps and requires time to be synchronized across all the storage and client nodes. This can be done using NTP or any other equivalent protocol.","title":"Time Synchronization"},{"location":"admin/predeployment_check/#usergroup-synchronization","text":"DAOS ACLs store the actual user and group names (instead of numeric IDs), and therefore the servers do not need access to a synchronized user/group database. The DAOS Agent (running on the client nodes) is responsible for resolving UID/GID to user/group names which are added to a signed credential and sent to the DAOS storage nodes.","title":"User/Group Synchronization"},{"location":"admin/predeployment_check/#multi-railnic-setup","text":"Storage nodes can be configured with multiple network interfaces to run multiple engine instances. Some special configuration is required to use librdmacm with multiple interfaces. First, the accept_local feature must be enabled on the network interfaces to be used by DAOS. This can be done using the following command ( must be replaced with the interface names): $ sudo sysctl -w net.ipv4.conf.all.accept_local=1 Second, Linux must be configured to only send ARP replies on the interface targeted in the ARP request. This is configured via the arp_ignore parameter. This should be set to 2 if all the IPoIB interfaces on the client and storage nodes are in the same logical subnet (e.g. ib0 == 10.0.0.27, ib1 == 10.0.1.27, prefix=16). $ sysctl -w net.ipv4.conf.all.arp_ignore=2 If separate logical subnets are used (e.g. prefix = 24), then the value must be set to 1. $ sysctl -w net.ipv4.conf.all.arp_ignore=1 Finally, the rp_filter is set to 1 by default on several distributions (e.g. on CentOS 7) and should be set to either 0 or 2, with 2 being more secure. This is true even if the configuration uses a single logical subnet. $ sysctl -w net.ipv4.conf.<ifaces>.rp_filter=2 All those parameters can be made persistent in /etc/sysctl.conf by adding a new sysctl file under /etc/sysctl.d (e.g. /etc/sysctl.d/95-daos-net.conf) with all the relevant settings. For more information, please refer to the librdmacm documentation","title":"Multi-rail/NIC Setup"},{"location":"admin/predeployment_check/#subnet","text":"Since all engines need to be able to communicate, the different network interfaces need to be on the same subnet or routing capabilities across the different subnet must be configured.","title":"Subnet"},{"location":"admin/predeployment_check/#runtime-directory-setup","text":"DAOS uses a series of Unix Domain Sockets to communicate between its various components. On modern Linux systems, Unix Domain Sockets are typically stored under /run or /var/run (usually a symlink to /run) and are a mounted tmpfs file system. There are several methods for ensuring the necessary directories are setup. A sign that this step may have been missed is when starting daos_server or daos_agent, you may see the message: $ mkdir /var/run/daos_server: permission denied Unable to create socket directory: /var/run/daos_server","title":"Runtime Directory Setup"},{"location":"admin/predeployment_check/#non-default-directory","text":"By default, daos_server and daos_agent will use the directories /var/run/daos_server and /var/run/daos_agent respectively. To change the default location that daos_server uses for its runtime directory, either uncomment and set the socket_dir configuration value in install/etc/daos_server.yml, or pass the location to daos_server on the command line using the -d flag ( daos_server start -d /tmp/daos_server ). For the daos_agent, an alternate location can be passed on the command line using the --runtime_dir flag ( daos_agent -d /tmp/daos_agent ).","title":"Non-default Directory"},{"location":"admin/predeployment_check/#default-directory-non-persistent","text":"Files and directories created in /run and /var/run only survive until the next reboot. These directories are required for subsequent runs; therefore, if reboots are infrequent, an easy solution while still utilizing the default locations is to create the required directories manually. To do this execute the following commands. daos_server: $ mkdir /var/run/daos_server $ chmod 0755 /var/run/daos_server $ chown user:user /var/run/daos_server (where user is the user you will run daos_server as) daos_agent: $ mkdir /var/run/daos_agent $ chmod 0755 /var/run/daos_agent $ chown user:user /var/run/daos_agent (where user is the user you will run daos_agent as)","title":"Default Directory (non-persistent)"},{"location":"admin/predeployment_check/#default-directory-persistent","text":"If the server hosting daos_server or daos_agent will be rebooted often, systemd provides a persistent mechanism for creating the required directories called tmpfiles.d. This mechanism will be required every time the system is provisioned and requires a reboot to take effect. To tell systemd to create the necessary directories for DAOS: Copy the file utils/systemd/daosfiles.conf to /etc/tmpfiles.d\\ cp utils/systemd/daosfiles.conf /etc/tmpfiles.d Modify the copied file to change the user and group fields (currently daos) to the user daos will be run as Reboot the system, and the directories will be created automatically on all subsequent reboots.","title":"Default Directory (persistent)"},{"location":"admin/predeployment_check/#elevated-privileges","text":"DAOS employs a privileged helper binary ( daos_admin ) to perform tasks that require elevated privileges on behalf of daos_server .","title":"Elevated Privileges"},{"location":"admin/predeployment_check/#privileged-helper-configuration","text":"When DAOS is installed from RPM, the daos_admin helper is automatically installed to the correct location with the correct permissions. The RPM creates a \"daos_server\" system group and configures permissions such that daos_admin may only be invoked from daos_server . For non-RPM installations, there are two supported scenarios: daos_server is run as root, which means that daos_admin is also invoked as root, and therefore no additional setup is necessary. daos_server is run as a non-root user, which means that daos_admin must be manually installed and configured. The steps to enable the second scenario are as follows (steps are assumed to be running out of a DAOS source tree which may be on a NFS share): $ chmod -x $daospath/bin/daos_admin # prevent this copy from being executed $ sudo cp $daospath/bin/daos_admin /usr/bin/daos_admin $ sudo chmod 4755 /usr/bin/daos_admin # make this copy setuid root $ sudo mkdir -p /usr/share/daos/control # create symlinks to SPDK scripts $ sudo ln -sf $daospath/share/daos/control/setup_spdk.sh \\ /usr/share/daos/control $ sudo mkdir -p /usr/share/spdk/scripts $ sudo ln -sf $daospath/share/spdk/scripts/setup.sh \\ /usr/share/spdk/scripts $ sudo ln -sf $daospath/share/spdk/scripts/common.sh \\ /usr/share/spdk/scripts $ sudo ln -s $daospath/include \\ /usr/share/spdk/include Note The RPM installation is preferred for production scenarios. Manual installation is most appropriate for development and predeployment proof-of-concept scenarios.","title":"Privileged Helper Configuration"},{"location":"admin/predeployment_check/#memory-lock-limits","text":"Low ulimit for memlock can cause SPDK to fail and emit the following error: daos_engine:1 EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) The memlock limit only needs to be manually adjusted when daos_server is not running as a systemd service. Default ulimit settings vary between OSes. For RPM installations, the service will typically be launched by systemd and the limit is pre-set to unlimited in the daos_server.service unit file: https://github.com/daos-stack/daos/blob/master/utils/systemd/daos_server.service#L16. Note that values set in /etc/security/limits.conf are ignored by services launched by systemd. For non-RPM installations where daos_server is launched directly from the commandline, limits should be adjusted in /etc/security/limits.conf as per https://software.intel.com/content/www/us/en/develop/blogs/best-known-methods-for-setting-locked-memory-size.html.","title":"Memory Lock Limits"},{"location":"admin/tiering_uns/","text":"Tiering and Unified Namespace \u00b6 Unified Namespace \u00b6 The DAOS tier can be tightly integrated with the Lustre parallel filesystem, in which DAOS containers will be represented through the Lustre namespace. This capability is under development and is scheduled for DAOS v1.2. The current state of work can be summarized as follows : DAOS integration with Lustre uses the Lustre foreign file/dir feature (from LU-11376 and associated patches). Each time a DAOS POSIX container is created using the daos utility and its '--path' UNS option, a Lustre foreign file/dir of 'symlink' type is created with a specific LOV/LMV EA content that will allow the DAOS pool and containers UUIDs to be stored. The Lustre Client patch for LU-12682 adds DAOS specific support to the Lustre foreign file/dir feature. It allows for the foreign file/dir of symlink type to be presented and act as an <absolute-prefix>/<pool-uuid>/<container-uuid> symlink to the Linux Kernel/VFS. The <absolute-prefix> can be specified as the new foreign_symlink=<absolute-prefix> Lustre Client mount option, or also through the new llite.*.foreign_symlink_prefix Lustre dynamic tuneable. Both <pool-uuid> and <container-uuid> are extracted from foreign file/dir LOV/LMV EA. To allow for symlink resolution and transparent access to the DAOS container content, it is expected that a DFuse/DFS instance/mount of DAOS Server root exists on <absolute-prefix> , presenting all served pools/containers as <pool-uuid>/<container-uuid> relative paths. daos foreign support is enabled at mount time with the symlink= option present or dynamically, through the llite.*.daos_enable setting. Building and using a DAOS-aware Lustre version \u00b6 As indicated before, a Lustre Client patch (for LU-12682) has been developed to allow for the application's transparent access to the DAOS container's data from a Lustre foreign file/dir. This patch can be found at https://review.whamcloud.com/35856 and has been landed onto master but is still not integrated with an official Lustre version. This patch must be applied on top of the selected Lustre version's source tree. After any conflicts are resolved, Lustre must be built and the generated RPMs installed on client nodes by following the instructions at https://wiki.whamcloud.com/display/PUB/Building+Lustre+from+Source. The Lustre client mount command must use the new foreign_symlink=<absolute_path> option to set the prefix to be used in front of the <pool-UUID>/<cont-UUID> relative path, based on pool/container information being extracted from the LOV/LMV foreign symlink EAs. This can be configured by dynamically modifying both foreign_symlink_[enable,prefix] parameters for each Lustre client mount, using the lctl set_param llite/*/foreign_symlink_[enable,prefix]=[0|1,<path>] command. The Dfuse instance will then use this prefix to mount/expose all DAOS pools, or use <prefix>/<pool-UUID>[/<cont-UUID>] to mount a single pool/container. To allow non-root/admin users to use the llapi_set_dirstripe() API (like the daos cont create command with --path option), or the lfs setdirstripe command, the Lustre MDS servers configuration must be modified accordingly by running the lctl set_param mdt/*/enable_remote_dir_gid=-1 command. Additionally, there is a feature available to provide a customized format of LOV/LMV EAs, apart from the default <pool-UUID>/<cont-UUID> , through the llite/*/foreign_symlink_upcall tunable. This provides the path of a user-land upcall, that will indicate where to extract <pool-UUID> and <cont-UUID> in the LOV/LMV EAs, using a series of [pos, len] tuples and constant strings. lustre/utils/l_foreign_symlink.c is a helper example in the Lustre source code. Data Migration \u00b6 Migration to/from a POSIX filesystem \u00b6 A dataset mover tool is under development to move a snapshot of a DAOS POSIX container or DAOS HDF5 container to a POSIX filesystem and vice versa. The copy will be performed at the POSIX or HDF5 level. (The MPI-IO ROMIO ADIO driver for DAOS also uses DAOS POSIX containers.) For DAOS HDF5 containers, the resulting HDF5 file in the POSIX filesystem will be accessible through the native HDF5 connector with the POSIX VFD. The first version of the data mover tool is currently scheduled for DAOS v1.4. Container Parking \u00b6 The mover tool will also eventually support the ability to serialize and deserialize a DAOS container to a set of POSIX files that can be stored or \"parked\" in an external POSIX filesystem. This transformation is agnostic to the data model and container type and will retain all DAOS internal metadata.","title":"Tiering and Unified Namespace"},{"location":"admin/tiering_uns/#tiering-and-unified-namespace","text":"","title":"Tiering and Unified Namespace"},{"location":"admin/tiering_uns/#unified-namespace","text":"The DAOS tier can be tightly integrated with the Lustre parallel filesystem, in which DAOS containers will be represented through the Lustre namespace. This capability is under development and is scheduled for DAOS v1.2. The current state of work can be summarized as follows : DAOS integration with Lustre uses the Lustre foreign file/dir feature (from LU-11376 and associated patches). Each time a DAOS POSIX container is created using the daos utility and its '--path' UNS option, a Lustre foreign file/dir of 'symlink' type is created with a specific LOV/LMV EA content that will allow the DAOS pool and containers UUIDs to be stored. The Lustre Client patch for LU-12682 adds DAOS specific support to the Lustre foreign file/dir feature. It allows for the foreign file/dir of symlink type to be presented and act as an <absolute-prefix>/<pool-uuid>/<container-uuid> symlink to the Linux Kernel/VFS. The <absolute-prefix> can be specified as the new foreign_symlink=<absolute-prefix> Lustre Client mount option, or also through the new llite.*.foreign_symlink_prefix Lustre dynamic tuneable. Both <pool-uuid> and <container-uuid> are extracted from foreign file/dir LOV/LMV EA. To allow for symlink resolution and transparent access to the DAOS container content, it is expected that a DFuse/DFS instance/mount of DAOS Server root exists on <absolute-prefix> , presenting all served pools/containers as <pool-uuid>/<container-uuid> relative paths. daos foreign support is enabled at mount time with the symlink= option present or dynamically, through the llite.*.daos_enable setting.","title":"Unified Namespace"},{"location":"admin/tiering_uns/#building-and-using-a-daos-aware-lustre-version","text":"As indicated before, a Lustre Client patch (for LU-12682) has been developed to allow for the application's transparent access to the DAOS container's data from a Lustre foreign file/dir. This patch can be found at https://review.whamcloud.com/35856 and has been landed onto master but is still not integrated with an official Lustre version. This patch must be applied on top of the selected Lustre version's source tree. After any conflicts are resolved, Lustre must be built and the generated RPMs installed on client nodes by following the instructions at https://wiki.whamcloud.com/display/PUB/Building+Lustre+from+Source. The Lustre client mount command must use the new foreign_symlink=<absolute_path> option to set the prefix to be used in front of the <pool-UUID>/<cont-UUID> relative path, based on pool/container information being extracted from the LOV/LMV foreign symlink EAs. This can be configured by dynamically modifying both foreign_symlink_[enable,prefix] parameters for each Lustre client mount, using the lctl set_param llite/*/foreign_symlink_[enable,prefix]=[0|1,<path>] command. The Dfuse instance will then use this prefix to mount/expose all DAOS pools, or use <prefix>/<pool-UUID>[/<cont-UUID>] to mount a single pool/container. To allow non-root/admin users to use the llapi_set_dirstripe() API (like the daos cont create command with --path option), or the lfs setdirstripe command, the Lustre MDS servers configuration must be modified accordingly by running the lctl set_param mdt/*/enable_remote_dir_gid=-1 command. Additionally, there is a feature available to provide a customized format of LOV/LMV EAs, apart from the default <pool-UUID>/<cont-UUID> , through the llite/*/foreign_symlink_upcall tunable. This provides the path of a user-land upcall, that will indicate where to extract <pool-UUID> and <cont-UUID> in the LOV/LMV EAs, using a series of [pos, len] tuples and constant strings. lustre/utils/l_foreign_symlink.c is a helper example in the Lustre source code.","title":"Building and using a DAOS-aware Lustre version"},{"location":"admin/tiering_uns/#data-migration","text":"","title":"Data Migration"},{"location":"admin/tiering_uns/#migration-tofrom-a-posix-filesystem","text":"A dataset mover tool is under development to move a snapshot of a DAOS POSIX container or DAOS HDF5 container to a POSIX filesystem and vice versa. The copy will be performed at the POSIX or HDF5 level. (The MPI-IO ROMIO ADIO driver for DAOS also uses DAOS POSIX containers.) For DAOS HDF5 containers, the resulting HDF5 file in the POSIX filesystem will be accessible through the native HDF5 connector with the POSIX VFD. The first version of the data mover tool is currently scheduled for DAOS v1.4.","title":"Migration to/from a POSIX filesystem"},{"location":"admin/tiering_uns/#container-parking","text":"The mover tool will also eventually support the ability to serialize and deserialize a DAOS container to a set of POSIX files that can be stored or \"parked\" in an external POSIX filesystem. This transformation is agnostic to the data model and container type and will retain all DAOS internal metadata.","title":"Container Parking"},{"location":"admin/troubleshooting/","text":"Troubleshooting \u00b6 DAOS Errors \u00b6 DAOS error numbering starts at 1000. The most common errors are documented in the table below. DAOS Error Value Description DER_NO_PERM 1001 No permission DER_NO_HDL 1002 Invalid handle DER_INVAL 1003 Invalid parameters DER_EXIST 1004 Entity already exists DER_NONEXIST 1005 The specified entity does not exist DER_UNREACH 1006 Unreachable node DER_NOSPACE 1007 No space left on storage target DER_ALREADY 1008 Operation already performed DER_NOMEM 1009 Out of memory DER_NOSYS 1010 Function not implemented DER_TIMEDOUT 1011 Time out DER_BUSY 1012 Device or resource busy DER_AGAIN 1013 Try again DER_PROTO 1014 Incompatible protocol DER_UNINIT 1015 Device or resource not initialized DER_TRUNC 1016 Buffer too short DER_OVERFLOW 1017 Data too long for defined data type or buffer size DER_CANCELED 1018 Operation canceled DER_OOG 1019 Out of group or member list DER_HG 1020 Transport layer mercury error DER_MISC 1025 Miscellaneous error DER_BADPATH 1026 Bad path name DER_NOTDIR 1027 Not a directory DER_EVICTED 1032 Rank has been evicted DER_DOS 1034 Denial of service DER_BAD_TARGET 1035 Incorrect target for the RPC DER_HLC_SYNC 1037 HLC synchronization error DER_IO 2001 Generic I/O error DER_ENOENT 2003 Entry not found DER_NOTYPE 2004 Unknown object type DER_NOSCHEMA 2005 Unknown object schema DER_KEY2BIG 2012 Key is too large DER_REC2BIG 2013 Record is too large DER_IO_INVAL 2014 IO buffers can't match object extents DER_EQ_BUSY 2015 Event queue is busy DER_SHUTDOWN 2017 Service should shut down DER_INPROGRESS 2018 Operation now in progress DER_NOTREPLICA 2020 Not a service replica DER_CSUM 2021 Checksum error DER_REC_SIZE 2024 Record size error DER_TX_RESTART 2025 Transaction should restart DER_DATA_LOSS 2026 Data lost or not recoverable DER_TX_BUSY 2028 TX is not committed DER_AGENT_INCOMPAT 2029 Agent is incompatible with libdaos When an operation fails, DAOS returns a negative DER error. For a full list of errors, please check https://github.com/daos-stack/daos/blob/master/src/include/daos_errno.h ( DER_ERR_GURT_BASE is equal to 1000, and DER_ERR_DAOS_BASE is equal to 2000). The function d_errstr() is provided in the API to convert an error number to an error message. Log Files \u00b6 On the server side, there are three log files created as part of normal server operations: Component Config Parameter Example Config Value Control Plane control_log_file /tmp/daos_server.log Data Plane log_file /tmp/daos_engine.*.log Privileged Helper helper_log_file /tmp/daos_admin.log agent log_file /tmp/daos_agent.log Control Plane Log \u00b6 The default log level for the control plane is INFO. The following levels may be set using the control_log_mask config parameter: DEBUG INFO ERROR Data Plane Log \u00b6 Data Plane ( daos_engine ) logging is configured on a per-instance basis. In other words, each section under the servers: section must have its own logging configuration. The log_file config parameter is converted to a D_LOG_FILE environment variable value. For more detail, please see the Debugging System section of this document. Privileged Helper Log \u00b6 By default, the privileged helper only emits ERROR-level logging which is captured by the control plane and included in that log. If the helper_log_file parameter is set in the server config, then DEBUG-level logging will be sent to the specified file. Daos Agent Log \u00b6 If the log_file config parameter is set in the agent config, then DEBUG-level logging will be sent to the specified file. Debugging System \u00b6 DAOS uses the debug system defined in CaRT , specifically the GURT library. Both server and client default log is stdout , unless otherwise set by D_LOG_FILE environment variable (client) or log_file config parameter (server). Registered Subsystems/Facilities \u00b6 The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging. By default all subsystems are enabled (\"DD_SUBSYS=all\"). DAOS Facilities: common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, bio, tests Common Facilities (GURT): MISC, MEM CaRT Facilities: RPC, BULK, CORPC, GRP, LM, HG, ST, IV Priority Logging \u00b6 The priority level that outputs to stderr is set with DD_STDERR. By default in DAOS (specific to the project), this is set to CRIT (\"DD_STDERR=CRIT\") meaning that all CRIT and more severe log messages will dump to stderr. However, this is separate from the priority of logging to \"/tmp/daos.log\". The priority level of logging can be set with D_LOG_MASK, which by default is set to INFO (\"D_LOG_MASK=INFO\"), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well (\"D_LOG_MASK=DEBUG,MEM=ERR\"). Debug Masks/Streams: \u00b6 DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). To accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can be defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default (\"DD_MASK=all\"). DAOS Debug Masks: md = metadata operations pl = placement operations mgmt = pool management epc = epoch system df = durable format rebuild = rebuild process daos_default = (group mask) io, md, pl, and rebuild operations Common Debug Masks (GURT): any = generic messages, no classification trace = function trace, tree/hash/lru operations mem = memory operations net = network operations io = object I/Otest = test programs Common Use Cases \u00b6 Generic setup for all messages (default settings) D_LOG_MASK=DEBUG DD_SUBSYS=all DD_MASK=all Disable all logs for performance tuning D_LOG_MASK=ERR -> will only log error messages from all facilities D_LOG_MASK=FATAL -> will only log system fatal messages Disable a noisy debug logging subsystem D_LOG_MASK=DEBUG,MEM=ERR -> disables MEM facility by restricting all logs from that facility to ERROR or higher priority only Enable a subset of facilities of interest DD_SUBSYS=rpc,tests D_LOG_MASK=DEBUG -> required to see logs for RPC and TESTS less severe than INFO (the majority of log messages) Fine-tune the debug messages by setting a debug mask D_LOG_MASK=DEBUG DD_MASK=mgmt -> only logs DEBUG messages related to pool management Refer to the DAOS Environment Variables document for more information about the debug system environment. Common DAOS Problems \u00b6 Incompatible Agent \u00b6 When DER_AGENT_INCOMPAT is received, it means that the client library libdaos.so is likely mismatched with the DAOS Agent. The libdaos.so, DAOS Agent and DAOS Server must be built from compatible sources so that the GetAttachInfo protocol is the same between each component. Depending on your situation, you will need to either update the DAOS Agent or the libdaos.so to the newer version in order to maintain compatibility with each other. HLC Sync \u00b6 When DER_HLC_SYNC is received, it means that sender and receiver HLC timestamps are off by more than maximum allowed system clock offset (1 second by default). In order to correct this situation synchronize all server clocks to the same reference time, using services like NTP. Shared Memory Errors \u00b6 When DER_SHMEM_PERMS is received it means that this I/O Engine lacked the permissions to access the shared memory megment left behind by a previous run of the I/O Engine on the same machine. This happens when the I/O Engine fails to remove the shared memory segment upon shutdown, and, there is a mismatch between the user/group used to launch the I/O Engine between these successive runs. To remedy the problem, manually identify the shared memory segment and remove it. Issue ipcs to view the Shared Memory Segments. The output will show a list of segments organized by key . ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xffffffff 49938432 root 666 40 0 0x10242048 98598913 jbrosenz 660 1048576 0 0x10242049 98631682 jbrosenz 660 1048576 0 ------ Semaphore Arrays -------- key semid owner perms nsems Shared Memory Segments with keys [0x10242048 .. (0x10242048 + number of I/O Engines running)] are the segments that must be removed. Use ipcrm to remove the segment. For example, to remove the shared memory segment left behind by I/O Engine instance 0, issue: sudo ipcrm -M 0x10242048 To remove the shared memory segment left behind by I/O Engine instance 1, issue: sudo ipcrm -M 0x10242049 Server Start Issues \u00b6 Read the log located in the control_log_file . Verify that the daos_server process is not currently running. Check the SCM device path in /dev. Verify the PCI addresses using dmg storage scan . Note A server must be started with minimum setup. You can also obtain the addresses with daos_server storage scan . Format the SCMs defined in the config file. Generate the config file using dmg config generate . The various requirements will be populated without a syntax error. Try starting with allow_insecure: true . This will rule out the credential certificate issue. Verify that the access_points host is accessible and the port is not used. Check the provider entry. See the \"Network Scan and Configuration\" section of the admin guide for determining the right provider to use. Check fabric_iface in engines . They should be available and enabled. Check that socket_dir is writeable by the daos_server. Errors creating a Pool \u00b6 Check which engine rank you want to create a pool in with dmg system query --verbose and verify their State is Joined. DER_NOSPACE(-1007) appears: Check the size of the NVMe and PMEM. Next, check the size of the existing pool. Then check that this new pool being created will fit into the remaining disk space. Problems creating a container \u00b6 Check that the path to daos is your intended binary. It's usually /usr/bin/daos . When the server configuration is changed, it's necessary to restart the agent. DER_UNREACH(-1006) : Check the socket ID consistency between PMEM and NVMe. First, determine which socket you're using with daos_server network scan -p all . e.g., if the interface you're using in the engine section is eth0, find which NUMA Socket it belongs to. Next, determine the disks you can use with this socket by calling daos_server storage scan or dmg storage scan . e.g., if eth0 belongs to NUMA Socket 0, use only the disks with 0 in the Socket ID column. Check the interface used in the server config ( fabric_iface ) also exists in the client and can communicate with the server. Check the access_points of the agent config points to the correct server host. Call daos pool query and check that the pool exists and has free space. Applications run slow \u00b6 Verify if you're using Infiniband for fabric_iface : in the server config. The IO will be significantly slower with Ethernet. Bug Report \u00b6 Bugs should be reported through our issue tracker 1 with a test case to reproduce the issue (when applicable) and debug logs. After creating a ticket, logs should be gathered from the locations described in the Log Files section of this document and attached to the ticket. To avoid problems with attaching large files, please attach the logs in a compressed container format, such as .zip or .tar.bz2. https://jira.hpdd.intel.com \u21a9","title":"Troubleshooting"},{"location":"admin/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"admin/troubleshooting/#daos-errors","text":"DAOS error numbering starts at 1000. The most common errors are documented in the table below. DAOS Error Value Description DER_NO_PERM 1001 No permission DER_NO_HDL 1002 Invalid handle DER_INVAL 1003 Invalid parameters DER_EXIST 1004 Entity already exists DER_NONEXIST 1005 The specified entity does not exist DER_UNREACH 1006 Unreachable node DER_NOSPACE 1007 No space left on storage target DER_ALREADY 1008 Operation already performed DER_NOMEM 1009 Out of memory DER_NOSYS 1010 Function not implemented DER_TIMEDOUT 1011 Time out DER_BUSY 1012 Device or resource busy DER_AGAIN 1013 Try again DER_PROTO 1014 Incompatible protocol DER_UNINIT 1015 Device or resource not initialized DER_TRUNC 1016 Buffer too short DER_OVERFLOW 1017 Data too long for defined data type or buffer size DER_CANCELED 1018 Operation canceled DER_OOG 1019 Out of group or member list DER_HG 1020 Transport layer mercury error DER_MISC 1025 Miscellaneous error DER_BADPATH 1026 Bad path name DER_NOTDIR 1027 Not a directory DER_EVICTED 1032 Rank has been evicted DER_DOS 1034 Denial of service DER_BAD_TARGET 1035 Incorrect target for the RPC DER_HLC_SYNC 1037 HLC synchronization error DER_IO 2001 Generic I/O error DER_ENOENT 2003 Entry not found DER_NOTYPE 2004 Unknown object type DER_NOSCHEMA 2005 Unknown object schema DER_KEY2BIG 2012 Key is too large DER_REC2BIG 2013 Record is too large DER_IO_INVAL 2014 IO buffers can't match object extents DER_EQ_BUSY 2015 Event queue is busy DER_SHUTDOWN 2017 Service should shut down DER_INPROGRESS 2018 Operation now in progress DER_NOTREPLICA 2020 Not a service replica DER_CSUM 2021 Checksum error DER_REC_SIZE 2024 Record size error DER_TX_RESTART 2025 Transaction should restart DER_DATA_LOSS 2026 Data lost or not recoverable DER_TX_BUSY 2028 TX is not committed DER_AGENT_INCOMPAT 2029 Agent is incompatible with libdaos When an operation fails, DAOS returns a negative DER error. For a full list of errors, please check https://github.com/daos-stack/daos/blob/master/src/include/daos_errno.h ( DER_ERR_GURT_BASE is equal to 1000, and DER_ERR_DAOS_BASE is equal to 2000). The function d_errstr() is provided in the API to convert an error number to an error message.","title":"DAOS Errors"},{"location":"admin/troubleshooting/#log-files","text":"On the server side, there are three log files created as part of normal server operations: Component Config Parameter Example Config Value Control Plane control_log_file /tmp/daos_server.log Data Plane log_file /tmp/daos_engine.*.log Privileged Helper helper_log_file /tmp/daos_admin.log agent log_file /tmp/daos_agent.log","title":"Log Files"},{"location":"admin/troubleshooting/#control-plane-log","text":"The default log level for the control plane is INFO. The following levels may be set using the control_log_mask config parameter: DEBUG INFO ERROR","title":"Control Plane Log"},{"location":"admin/troubleshooting/#data-plane-log","text":"Data Plane ( daos_engine ) logging is configured on a per-instance basis. In other words, each section under the servers: section must have its own logging configuration. The log_file config parameter is converted to a D_LOG_FILE environment variable value. For more detail, please see the Debugging System section of this document.","title":"Data Plane Log"},{"location":"admin/troubleshooting/#privileged-helper-log","text":"By default, the privileged helper only emits ERROR-level logging which is captured by the control plane and included in that log. If the helper_log_file parameter is set in the server config, then DEBUG-level logging will be sent to the specified file.","title":"Privileged Helper Log"},{"location":"admin/troubleshooting/#daos-agent-log","text":"If the log_file config parameter is set in the agent config, then DEBUG-level logging will be sent to the specified file.","title":"Daos Agent Log"},{"location":"admin/troubleshooting/#debugging-system","text":"DAOS uses the debug system defined in CaRT , specifically the GURT library. Both server and client default log is stdout , unless otherwise set by D_LOG_FILE environment variable (client) or log_file config parameter (server).","title":"Debugging System"},{"location":"admin/troubleshooting/#registered-subsystemsfacilities","text":"The debug logging system includes a series of subsystems or facilities which define groups for related log messages (defined per source file). There are common facilities which are defined in GURT, as well as other facilities that can be defined on a per-project basis (such as those for CaRT and DAOS). DD_SUBSYS can be used to set which subsystems to enable logging. By default all subsystems are enabled (\"DD_SUBSYS=all\"). DAOS Facilities: common, tree, vos, client, server, rdb, pool, container, object, placement, rebuild, tier, mgmt, bio, tests Common Facilities (GURT): MISC, MEM CaRT Facilities: RPC, BULK, CORPC, GRP, LM, HG, ST, IV","title":"Registered Subsystems/Facilities"},{"location":"admin/troubleshooting/#priority-logging","text":"The priority level that outputs to stderr is set with DD_STDERR. By default in DAOS (specific to the project), this is set to CRIT (\"DD_STDERR=CRIT\") meaning that all CRIT and more severe log messages will dump to stderr. However, this is separate from the priority of logging to \"/tmp/daos.log\". The priority level of logging can be set with D_LOG_MASK, which by default is set to INFO (\"D_LOG_MASK=INFO\"), which will result in all messages excluding DEBUG messages being logged. D_LOG_MASK can also be used to specify the level of logging on a per-subsystem basis as well (\"D_LOG_MASK=DEBUG,MEM=ERR\").","title":"Priority Logging"},{"location":"admin/troubleshooting/#debug-masksstreams","text":"DEBUG messages account for a majority of the log messages, and finer-granularity might be desired. Mask bits are set as the first argument passed in D_DEBUG(mask, ...). To accomplish this, DD_MASK can be set to enable different debug streams. Similar to facilities, there are common debug streams defined in GURT, as well as other streams that can be defined on a per-project basis (CaRT and DAOS). All debug streams are enabled by default (\"DD_MASK=all\"). DAOS Debug Masks: md = metadata operations pl = placement operations mgmt = pool management epc = epoch system df = durable format rebuild = rebuild process daos_default = (group mask) io, md, pl, and rebuild operations Common Debug Masks (GURT): any = generic messages, no classification trace = function trace, tree/hash/lru operations mem = memory operations net = network operations io = object I/Otest = test programs","title":"Debug Masks/Streams:"},{"location":"admin/troubleshooting/#common-use-cases","text":"Generic setup for all messages (default settings) D_LOG_MASK=DEBUG DD_SUBSYS=all DD_MASK=all Disable all logs for performance tuning D_LOG_MASK=ERR -> will only log error messages from all facilities D_LOG_MASK=FATAL -> will only log system fatal messages Disable a noisy debug logging subsystem D_LOG_MASK=DEBUG,MEM=ERR -> disables MEM facility by restricting all logs from that facility to ERROR or higher priority only Enable a subset of facilities of interest DD_SUBSYS=rpc,tests D_LOG_MASK=DEBUG -> required to see logs for RPC and TESTS less severe than INFO (the majority of log messages) Fine-tune the debug messages by setting a debug mask D_LOG_MASK=DEBUG DD_MASK=mgmt -> only logs DEBUG messages related to pool management Refer to the DAOS Environment Variables document for more information about the debug system environment.","title":"Common Use Cases"},{"location":"admin/troubleshooting/#common-daos-problems","text":"","title":"Common DAOS Problems"},{"location":"admin/troubleshooting/#incompatible-agent","text":"When DER_AGENT_INCOMPAT is received, it means that the client library libdaos.so is likely mismatched with the DAOS Agent. The libdaos.so, DAOS Agent and DAOS Server must be built from compatible sources so that the GetAttachInfo protocol is the same between each component. Depending on your situation, you will need to either update the DAOS Agent or the libdaos.so to the newer version in order to maintain compatibility with each other.","title":"Incompatible Agent"},{"location":"admin/troubleshooting/#hlc-sync","text":"When DER_HLC_SYNC is received, it means that sender and receiver HLC timestamps are off by more than maximum allowed system clock offset (1 second by default). In order to correct this situation synchronize all server clocks to the same reference time, using services like NTP.","title":"HLC Sync"},{"location":"admin/troubleshooting/#shared-memory-errors","text":"When DER_SHMEM_PERMS is received it means that this I/O Engine lacked the permissions to access the shared memory megment left behind by a previous run of the I/O Engine on the same machine. This happens when the I/O Engine fails to remove the shared memory segment upon shutdown, and, there is a mismatch between the user/group used to launch the I/O Engine between these successive runs. To remedy the problem, manually identify the shared memory segment and remove it. Issue ipcs to view the Shared Memory Segments. The output will show a list of segments organized by key . ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xffffffff 49938432 root 666 40 0 0x10242048 98598913 jbrosenz 660 1048576 0 0x10242049 98631682 jbrosenz 660 1048576 0 ------ Semaphore Arrays -------- key semid owner perms nsems Shared Memory Segments with keys [0x10242048 .. (0x10242048 + number of I/O Engines running)] are the segments that must be removed. Use ipcrm to remove the segment. For example, to remove the shared memory segment left behind by I/O Engine instance 0, issue: sudo ipcrm -M 0x10242048 To remove the shared memory segment left behind by I/O Engine instance 1, issue: sudo ipcrm -M 0x10242049","title":"Shared Memory Errors"},{"location":"admin/troubleshooting/#server-start-issues","text":"Read the log located in the control_log_file . Verify that the daos_server process is not currently running. Check the SCM device path in /dev. Verify the PCI addresses using dmg storage scan . Note A server must be started with minimum setup. You can also obtain the addresses with daos_server storage scan . Format the SCMs defined in the config file. Generate the config file using dmg config generate . The various requirements will be populated without a syntax error. Try starting with allow_insecure: true . This will rule out the credential certificate issue. Verify that the access_points host is accessible and the port is not used. Check the provider entry. See the \"Network Scan and Configuration\" section of the admin guide for determining the right provider to use. Check fabric_iface in engines . They should be available and enabled. Check that socket_dir is writeable by the daos_server.","title":"Server Start Issues"},{"location":"admin/troubleshooting/#errors-creating-a-pool","text":"Check which engine rank you want to create a pool in with dmg system query --verbose and verify their State is Joined. DER_NOSPACE(-1007) appears: Check the size of the NVMe and PMEM. Next, check the size of the existing pool. Then check that this new pool being created will fit into the remaining disk space.","title":"Errors creating a Pool"},{"location":"admin/troubleshooting/#problems-creating-a-container","text":"Check that the path to daos is your intended binary. It's usually /usr/bin/daos . When the server configuration is changed, it's necessary to restart the agent. DER_UNREACH(-1006) : Check the socket ID consistency between PMEM and NVMe. First, determine which socket you're using with daos_server network scan -p all . e.g., if the interface you're using in the engine section is eth0, find which NUMA Socket it belongs to. Next, determine the disks you can use with this socket by calling daos_server storage scan or dmg storage scan . e.g., if eth0 belongs to NUMA Socket 0, use only the disks with 0 in the Socket ID column. Check the interface used in the server config ( fabric_iface ) also exists in the client and can communicate with the server. Check the access_points of the agent config points to the correct server host. Call daos pool query and check that the pool exists and has free space.","title":"Problems creating a container"},{"location":"admin/troubleshooting/#applications-run-slow","text":"Verify if you're using Infiniband for fabric_iface : in the server config. The IO will be significantly slower with Ethernet.","title":"Applications run slow"},{"location":"admin/troubleshooting/#bug-report","text":"Bugs should be reported through our issue tracker 1 with a test case to reproduce the issue (when applicable) and debug logs. After creating a ticket, logs should be gathered from the locations described in the Log Files section of this document and attached to the ticket. To avoid problems with attaching large files, please attach the logs in a compressed container format, such as .zip or .tar.bz2. https://jira.hpdd.intel.com \u21a9","title":"Bug Report"},{"location":"admin/utilities_examples/","text":"Utilities & Usage Examples \u00b6 This section to be updated in a future revision.","title":"Utilities and Usage Examples"},{"location":"admin/utilities_examples/#utilities-usage-examples","text":"This section to be updated in a future revision.","title":"Utilities &amp; Usage Examples"},{"location":"dev/contributing/","text":"Contributing to DAOS \u00b6 Your contributions are most welcome! There are several good ways to suggest new features, offer to add a feature, or just begin a dialog about DAOS: Open an issue in jira Suggest a feature, ask a question, start a discussion, etc. in our community mailing list Chat with members of the DAOS community real-time on slack . An invitation to join the slack workspace is automatically sent when joining the community mailing list. Coding Rules \u00b6 Please check the coding conventions for code contribution. Commit Comments \u00b6 Commit Message Content \u00b6 Writing good commit comments is critical to ensuring that changes are easily understood, even years after they were originally written. The commit comment should contain enough information about the change to allow the reader to understand the motivation for the change, what parts of the code it is affecting, and any interesting, unusual, or complex parts of the change to draw attention to. The reason for a change may be manyfold: bug, enhancement, feature, code style, etc. so providing information about this sets the stage for understanding the change. If it is a bug, include information about what usage triggers the bug and how it manifests (error messages, assertion failure, etc.). If it is a feature, include information about what improvement is being made, and how it will affect usage. Providing some high-level information about the code path that is being modified is useful for the reader, since the files and patch fragments are not necessarily going to be listed in a sensible order in the patch. Including the important functions being modified provides a starting point for the reader to follow the logic of the change, and makes it easier to search for such changes in the future. If the patch is based on some earlier patch, then including the git commit hash of the original patch, Jira ticket number, etc. is useful for tracking the chain of dependencies. This can be very useful if a patch is landed separately to different maintenance branches, if it is fixing a problem in a previously landed patch, or if it is being imported from an upstream kernel commit. Having long commit comments that describe the change well is a good thing. The commit comments will be tightly associated with the code for a long time into the future, even many of the original commit comments from years earlier are still available through changes of the source code repository. In contrast, bug tracking systems come and go, and cannot be relied upon to track information about a change for extended periods of time. Commit Message Format \u00b6 Unlike the content of the commit message, the format is relatively easy to verify for correctness. Having the same standard format allows Git tools like git shortlog to extract information from the patches more easily. The first line of the commit comment is the commit summary of the change. Changes submitted to the DAOS master branch require a DAOS Jira ticket number at the beginning of the commit summary. A DAOS Jira ticket is one that begins with DAOS and is, therefore, part of the DAOS project within Jira. The commit summary should also have a component: tag immediately following the Jira ticket number that indicates to which DAOS subsystem the commit is related. Example DAOS subsystems relate to modules like client, pool, container, object, vos, rdb; functional components like rebuild; or auxiliary components like build, tests, doc. This subsystem list is not exhaustive but provides a good guideline for consistency. The commit summary line must be 62 characters or less, including the Jira ticket number and component tag, so that git shortlog and git format-patch can fit the summary onto a single line. The summary must be followed by a blank line. The rest of the comments should be wrapped to 70 columns or less. This allows for the first line to be used as a subject in emails, and also for the entire body to be displayed using tools like git log or git shortlog in an 80 column window. DAOS-nnn component: short description of change under 62 columns The \"component:\" should be a lower-case single-word subsystem of the DAOS code that best encompasses the change being made. Examples of components include modules: client, pool, container, object, vos, rdb, cart; functional subsystems: recovery; and auxiliary areas: build, tests, docs. This list is not exhaustive, but is a guideline. The commit comment should contain a detailed explanation of changes being made. This can be as long as you'd like. Please give details of what problem was solved (including error messages or problems that were seen), a good high-level description of how it was solved, and which parts of the code were changed (including important functions that were changed, if this is useful to understand the patch, and for easier searching). Wrap lines at/under 70 columns. Signed-off-by: Your Real Name <your_email@domain.name> The Signed-off-by: line \u00b6 The Signed-off-by: line asserts that you have permission to contribute the code to the project according to the Developer's Certificate of Origin. The -s option to git commit also adds the Signed-off-by: line automatically. Additional commit tags \u00b6 A number of additional commit tags can be used to further explain who has contributed to the patch, and for tracking purposes. These tags are commonly used with Linux kernel patches. These tags should appear before the Signed-off-by: tag. Acked-by: User Name <user@domain.com> Tested-by: User Name <user@domain.com> Reported-by: User Name <user@domain.com> Reviewed-by: User Name <user@domain.com> CC: User Name <user@domain.com> Pull Requests (PR) \u00b6 DAOS uses the common fork & merge workflow used by most GitHub-hosted projects. Please refer to the online GitHub documentation .","title":"Contributing"},{"location":"dev/contributing/#contributing-to-daos","text":"Your contributions are most welcome! There are several good ways to suggest new features, offer to add a feature, or just begin a dialog about DAOS: Open an issue in jira Suggest a feature, ask a question, start a discussion, etc. in our community mailing list Chat with members of the DAOS community real-time on slack . An invitation to join the slack workspace is automatically sent when joining the community mailing list.","title":"Contributing to DAOS"},{"location":"dev/contributing/#coding-rules","text":"Please check the coding conventions for code contribution.","title":"Coding Rules"},{"location":"dev/contributing/#commit-comments","text":"","title":"Commit Comments"},{"location":"dev/contributing/#commit-message-content","text":"Writing good commit comments is critical to ensuring that changes are easily understood, even years after they were originally written. The commit comment should contain enough information about the change to allow the reader to understand the motivation for the change, what parts of the code it is affecting, and any interesting, unusual, or complex parts of the change to draw attention to. The reason for a change may be manyfold: bug, enhancement, feature, code style, etc. so providing information about this sets the stage for understanding the change. If it is a bug, include information about what usage triggers the bug and how it manifests (error messages, assertion failure, etc.). If it is a feature, include information about what improvement is being made, and how it will affect usage. Providing some high-level information about the code path that is being modified is useful for the reader, since the files and patch fragments are not necessarily going to be listed in a sensible order in the patch. Including the important functions being modified provides a starting point for the reader to follow the logic of the change, and makes it easier to search for such changes in the future. If the patch is based on some earlier patch, then including the git commit hash of the original patch, Jira ticket number, etc. is useful for tracking the chain of dependencies. This can be very useful if a patch is landed separately to different maintenance branches, if it is fixing a problem in a previously landed patch, or if it is being imported from an upstream kernel commit. Having long commit comments that describe the change well is a good thing. The commit comments will be tightly associated with the code for a long time into the future, even many of the original commit comments from years earlier are still available through changes of the source code repository. In contrast, bug tracking systems come and go, and cannot be relied upon to track information about a change for extended periods of time.","title":"Commit Message Content"},{"location":"dev/contributing/#commit-message-format","text":"Unlike the content of the commit message, the format is relatively easy to verify for correctness. Having the same standard format allows Git tools like git shortlog to extract information from the patches more easily. The first line of the commit comment is the commit summary of the change. Changes submitted to the DAOS master branch require a DAOS Jira ticket number at the beginning of the commit summary. A DAOS Jira ticket is one that begins with DAOS and is, therefore, part of the DAOS project within Jira. The commit summary should also have a component: tag immediately following the Jira ticket number that indicates to which DAOS subsystem the commit is related. Example DAOS subsystems relate to modules like client, pool, container, object, vos, rdb; functional components like rebuild; or auxiliary components like build, tests, doc. This subsystem list is not exhaustive but provides a good guideline for consistency. The commit summary line must be 62 characters or less, including the Jira ticket number and component tag, so that git shortlog and git format-patch can fit the summary onto a single line. The summary must be followed by a blank line. The rest of the comments should be wrapped to 70 columns or less. This allows for the first line to be used as a subject in emails, and also for the entire body to be displayed using tools like git log or git shortlog in an 80 column window. DAOS-nnn component: short description of change under 62 columns The \"component:\" should be a lower-case single-word subsystem of the DAOS code that best encompasses the change being made. Examples of components include modules: client, pool, container, object, vos, rdb, cart; functional subsystems: recovery; and auxiliary areas: build, tests, docs. This list is not exhaustive, but is a guideline. The commit comment should contain a detailed explanation of changes being made. This can be as long as you'd like. Please give details of what problem was solved (including error messages or problems that were seen), a good high-level description of how it was solved, and which parts of the code were changed (including important functions that were changed, if this is useful to understand the patch, and for easier searching). Wrap lines at/under 70 columns. Signed-off-by: Your Real Name <your_email@domain.name>","title":"Commit Message Format"},{"location":"dev/contributing/#the-signed-off-by-line","text":"The Signed-off-by: line asserts that you have permission to contribute the code to the project according to the Developer's Certificate of Origin. The -s option to git commit also adds the Signed-off-by: line automatically.","title":"The Signed-off-by: line"},{"location":"dev/contributing/#additional-commit-tags","text":"A number of additional commit tags can be used to further explain who has contributed to the patch, and for tracking purposes. These tags are commonly used with Linux kernel patches. These tags should appear before the Signed-off-by: tag. Acked-by: User Name <user@domain.com> Tested-by: User Name <user@domain.com> Reported-by: User Name <user@domain.com> Reviewed-by: User Name <user@domain.com> CC: User Name <user@domain.com>","title":"Additional commit tags"},{"location":"dev/contributing/#pull-requests-pr","text":"DAOS uses the common fork & merge workflow used by most GitHub-hosted projects. Please refer to the online GitHub documentation .","title":"Pull Requests (PR)"},{"location":"dev/development/","text":"Development Environment \u00b6 This section covers specific instructions to create a developer-friendly environment to contribute to the DAOS development. This includes how to regenerate the protobuf files or add new Go package dependencies, which is only required for development purposes. Building DAOS for Development \u00b6 The DAOS repository is hosted on GitHub . To checkout the current development version, simply run: $ git clone --recurse-submodules https://github.com/daos-stack/daos.git For a specific branch or tag (e.g. v1.2), add -b v1.2 to the command line above. Prerequisite when built using --build-deps are installed in component specific directories under PREFIX/prereq/$TARGET_TYPE. Run the following scons command: $ scons PREFIX=${daos_prefix_path} install --build-deps=yes --config=force Installing the components into separate directories allow upgrading the components individually by replacing --build-deps=yes with --update-prereq={component\\_name} . This requires a change to the environment configuration from before. For automated environment setup, source utils/sl/setup_local.sh . The install path should be relocatable with the exception that daos_admin will not be able to find the new location of daos and dependencies. All other libraries and binaries should work without any change due to relative paths. Editing the .build-vars.sh file to replace the old with the new can restore the capability of setup_local.sh to automate path setup. To run daos_server, either the rpath in daos_admin needs to be patched to the new installation location of spdk and isal or LD_LIBRARY_PATH needs to be set. This can be done using SL_SPDK_PREFIX and SL_ISAL_PREFIX set when sourcing setup_local.sh . This can also be done with the following commands: source utils/sl/setup_local.sh sudo -E utils/setup_daos_admin.sh [path to new location of daos] This script is intended only for developer setup of daos_admin . With this approach, DAOS gets built using the prebuilt dependencies in ${daos_prefix_path}/prereq , and required options are saved for future compilations. So, after the first time, during development, only \" scons --config=force \" and \" scons --config=force install \" would suffice for compiling changes to DAOS source code. If you wish to compile DAOS with clang rather than gcc , set COMPILER=clang on the scons command line. This option is also saved for future compilations. Additionally, users can specify BUILD_TYPE=[dev|release|debug] and scons will save the intermediate build for the various BUILD_TYPE , COMPILER , and TARGET_TYPE options so a user can switch between options without a full rebuild and thus with minimal cost. By default, TARGET_TYPE is set to 'default' which means it uses the BUILD_TYPE setting. To avoid rebuilding prerequisites for every BUILD_TYPE setting, TARGET_TYPE can be explicitly set to a BUILD_TYPE setting to always use that set of prerequisites. These settings are stored in daos.conf so setting the values on subsequent builds is not necessary. If needed, ALT_PREFIX can be set to a colon separated prefix path where to look for already built components. If set, the build will check these paths for components before proceeding to build. Custom build targets \u00b6 The DAOS build also supports build targets to customize what parts of DAOS are built. At present, just three such targets are defined, client , server , and test . To build only client libraries and tools, use the following command: $ scons [args] client install To build the server instead, substitute server for client in the above command. Note that such targets need to be specified each time you build as the default is equivalent to specifying client server test on the command line. The test target is, at present, dependent on client and server as well. Building Optional Components \u00b6 There are a few optional components that can be included into the DAOS build. For instance, to include the psm2 provider. Run the following scons command: $ scons PREFIX=${daos_prefix_path} INCLUDE=psm2 install --build-deps=yes --config=force Refer to the built-in scons help command to get a full list of all the optional components under the INCLUDE optional parameter. $ scons -h scons: Reading SConscript files ... INCLUDE: Optional components to build (all|none|comma-separated list of names) allowed names: psm2 psm3 default: none actual: The version of the components can be changed by editing the utils/build.config file. NOTE The support of the optional components is not guarantee and can be removed without further notification. Go dependencies \u00b6 Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses Go Modules to manage these dependencies. As this feature is built in to Go distributions starting with version 1.11, no additional tools are needed to manage dependencies. Among other benefits, one of the major advantages of using Go Modules is that it removes the requirement for builds to be done within the $GOPATH , which simplifies our build system and other internal tooling. While it is possible to use Go Modules without checking a vendor directory into SCM, the DAOS project continues to use vendored dependencies in order to insulate our build system from transient network issues and other problems associated with nonvendored builds. The following is a short list of example workflows. For more details, please refer to one of the many resources available online. # add a new dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # update an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get -u github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # replace/remove an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/other/thing # make sure that github.com/other/thing is imported somewhere in the codebase, # and that github.com/awesome/thing is no longer imported $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod tidy $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. In all cases, after updating the vendor directory, it is a good idea to verify that your changes were applied as expected. In order to do this, a simple workflow is to clear the caches to force a clean build and then run the test script, which is vendor-aware and will not try to download missing modules: $ cd ~/daos/src/control # or wherever your daos clone lives $ go clean -modcache -cache $ ./run_go_tests.sh $ ls ~/go/pkg/mod # ~/go/pkg/mod should either not exist or be empty Protobuf Compiler \u00b6 The DAOS control plane infrastructure uses Protocol Buffers as the data serialization format for its RPC requests. Not all developers will need to compile the \\*.proto files, but if Protobuf changes are needed, the developer must regenerate the corresponding C and Go source files using a Protobuf compiler compatible with proto3 syntax. Recommended Versions \u00b6 The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work, but are not guaranteed. You may encounter installation errors when building from source relating to insufficient permissions. If that occurs, you may try relocating the repo to /var/tmp/ in order to build and install from there. Protocol Buffers v3.11.4. Installation instructions . Protobuf-C v1.3.3. Installation instructions . gRPC plugin: protoc-gen-go is the version specified in go.mod . This plugin is automatically installed by the Makefile in $DAOSREPO/src/proto. Compiling Protobuf Files \u00b6 The source ( .proto ) files live under $DAOSREPO/src/proto . The preferred mechanism for generating compiled C/Go protobuf definitions is to use the Makefile in this directory. Care should be taken to keep the Makefile updated when source files are added or removed, or generated file destinations are updated. Note that the generated files are checked into SCM and are not generated as part of the normal DAOS build process. This allows developers to ensure that the generated files are correct after any changes to the source files are made. $ cd ~/daos/src/proto # or wherever your daos clone lives $ make protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ acl.proto protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ mgmt.proto ... $ git status ... # modified: ../control/common/proto/mgmt/acl.pb.go # modified: ../control/common/proto/mgmt/mgmt.pb.go ... After verifying that the generated C/Go files are correct, add and commit them as you would any other file.","title":"Dev Environment"},{"location":"dev/development/#development-environment","text":"This section covers specific instructions to create a developer-friendly environment to contribute to the DAOS development. This includes how to regenerate the protobuf files or add new Go package dependencies, which is only required for development purposes.","title":"Development Environment"},{"location":"dev/development/#building-daos-for-development","text":"The DAOS repository is hosted on GitHub . To checkout the current development version, simply run: $ git clone --recurse-submodules https://github.com/daos-stack/daos.git For a specific branch or tag (e.g. v1.2), add -b v1.2 to the command line above. Prerequisite when built using --build-deps are installed in component specific directories under PREFIX/prereq/$TARGET_TYPE. Run the following scons command: $ scons PREFIX=${daos_prefix_path} install --build-deps=yes --config=force Installing the components into separate directories allow upgrading the components individually by replacing --build-deps=yes with --update-prereq={component\\_name} . This requires a change to the environment configuration from before. For automated environment setup, source utils/sl/setup_local.sh . The install path should be relocatable with the exception that daos_admin will not be able to find the new location of daos and dependencies. All other libraries and binaries should work without any change due to relative paths. Editing the .build-vars.sh file to replace the old with the new can restore the capability of setup_local.sh to automate path setup. To run daos_server, either the rpath in daos_admin needs to be patched to the new installation location of spdk and isal or LD_LIBRARY_PATH needs to be set. This can be done using SL_SPDK_PREFIX and SL_ISAL_PREFIX set when sourcing setup_local.sh . This can also be done with the following commands: source utils/sl/setup_local.sh sudo -E utils/setup_daos_admin.sh [path to new location of daos] This script is intended only for developer setup of daos_admin . With this approach, DAOS gets built using the prebuilt dependencies in ${daos_prefix_path}/prereq , and required options are saved for future compilations. So, after the first time, during development, only \" scons --config=force \" and \" scons --config=force install \" would suffice for compiling changes to DAOS source code. If you wish to compile DAOS with clang rather than gcc , set COMPILER=clang on the scons command line. This option is also saved for future compilations. Additionally, users can specify BUILD_TYPE=[dev|release|debug] and scons will save the intermediate build for the various BUILD_TYPE , COMPILER , and TARGET_TYPE options so a user can switch between options without a full rebuild and thus with minimal cost. By default, TARGET_TYPE is set to 'default' which means it uses the BUILD_TYPE setting. To avoid rebuilding prerequisites for every BUILD_TYPE setting, TARGET_TYPE can be explicitly set to a BUILD_TYPE setting to always use that set of prerequisites. These settings are stored in daos.conf so setting the values on subsequent builds is not necessary. If needed, ALT_PREFIX can be set to a colon separated prefix path where to look for already built components. If set, the build will check these paths for components before proceeding to build.","title":"Building DAOS for Development"},{"location":"dev/development/#custom-build-targets","text":"The DAOS build also supports build targets to customize what parts of DAOS are built. At present, just three such targets are defined, client , server , and test . To build only client libraries and tools, use the following command: $ scons [args] client install To build the server instead, substitute server for client in the above command. Note that such targets need to be specified each time you build as the default is equivalent to specifying client server test on the command line. The test target is, at present, dependent on client and server as well.","title":"Custom build targets"},{"location":"dev/development/#building-optional-components","text":"There are a few optional components that can be included into the DAOS build. For instance, to include the psm2 provider. Run the following scons command: $ scons PREFIX=${daos_prefix_path} INCLUDE=psm2 install --build-deps=yes --config=force Refer to the built-in scons help command to get a full list of all the optional components under the INCLUDE optional parameter. $ scons -h scons: Reading SConscript files ... INCLUDE: Optional components to build (all|none|comma-separated list of names) allowed names: psm2 psm3 default: none actual: The version of the components can be changed by editing the utils/build.config file. NOTE The support of the optional components is not guarantee and can be removed without further notification.","title":"Building Optional Components"},{"location":"dev/development/#go-dependencies","text":"Developers contributing Go code may need to change the external dependencies located in the src/control/vendor directory. The DAOS codebase uses Go Modules to manage these dependencies. As this feature is built in to Go distributions starting with version 1.11, no additional tools are needed to manage dependencies. Among other benefits, one of the major advantages of using Go Modules is that it removes the requirement for builds to be done within the $GOPATH , which simplifies our build system and other internal tooling. While it is possible to use Go Modules without checking a vendor directory into SCM, the DAOS project continues to use vendored dependencies in order to insulate our build system from transient network issues and other problems associated with nonvendored builds. The following is a short list of example workflows. For more details, please refer to one of the many resources available online. # add a new dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # update an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get -u github.com/awesome/thing # make sure that github.com/awesome/thing is imported somewhere in the codebase $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. # replace/remove an existing dependency $ cd ~/daos/src/control # or wherever your daos clone lives $ go get github.com/other/thing # make sure that github.com/other/thing is imported somewhere in the codebase, # and that github.com/awesome/thing is no longer imported $ ./run_go_tests.sh # note that go.mod and go.sum have been updated automatically # # when ready to commit and push for review: $ go mod tidy $ go mod vendor $ git commit -a # should pick up go.mod, go.sum, vendor/*, etc. In all cases, after updating the vendor directory, it is a good idea to verify that your changes were applied as expected. In order to do this, a simple workflow is to clear the caches to force a clean build and then run the test script, which is vendor-aware and will not try to download missing modules: $ cd ~/daos/src/control # or wherever your daos clone lives $ go clean -modcache -cache $ ./run_go_tests.sh $ ls ~/go/pkg/mod # ~/go/pkg/mod should either not exist or be empty","title":"Go dependencies"},{"location":"dev/development/#protobuf-compiler","text":"The DAOS control plane infrastructure uses Protocol Buffers as the data serialization format for its RPC requests. Not all developers will need to compile the \\*.proto files, but if Protobuf changes are needed, the developer must regenerate the corresponding C and Go source files using a Protobuf compiler compatible with proto3 syntax.","title":"Protobuf Compiler"},{"location":"dev/development/#recommended-versions","text":"The recommended installation method is to clone the git repositories, check out the tagged releases noted below, and install from source. Later versions may work, but are not guaranteed. You may encounter installation errors when building from source relating to insufficient permissions. If that occurs, you may try relocating the repo to /var/tmp/ in order to build and install from there. Protocol Buffers v3.11.4. Installation instructions . Protobuf-C v1.3.3. Installation instructions . gRPC plugin: protoc-gen-go is the version specified in go.mod . This plugin is automatically installed by the Makefile in $DAOSREPO/src/proto.","title":"Recommended Versions"},{"location":"dev/development/#compiling-protobuf-files","text":"The source ( .proto ) files live under $DAOSREPO/src/proto . The preferred mechanism for generating compiled C/Go protobuf definitions is to use the Makefile in this directory. Care should be taken to keep the Makefile updated when source files are added or removed, or generated file destinations are updated. Note that the generated files are checked into SCM and are not generated as part of the normal DAOS build process. This allows developers to ensure that the generated files are correct after any changes to the source files are made. $ cd ~/daos/src/proto # or wherever your daos clone lives $ make protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ acl.proto protoc -I /home/foo/daos/src/proto/mgmt/ --go_out=plugins=grpc:/home/foo/daos/src/control/common/proto/mgmt/ mgmt.proto ... $ git status ... # modified: ../control/common/proto/mgmt/acl.pb.go # modified: ../control/common/proto/mgmt/mgmt.pb.go ... After verifying that the generated C/Go files are correct, add and commit them as you would any other file.","title":"Compiling Protobuf Files"},{"location":"overview/architecture/","text":"Architecture \u00b6 DAOS is an open-source software-defined scale-out object store that provides high bandwidth and high IOPS storage containers to applications and enables next-generation data-centric workflows combining simulation, data analytics, and machine learning. Unlike the traditional storage stacks that were primarily designed for rotating media, DAOS is architected from the ground up to exploit new NVM technologies and is extremely lightweight since it operates End-to-End (E2E) in user space with full OS bypass. DAOS offers a shift away from an I/O model designed for block-based and high-latency storage to one that inherently supports fine-grained data access and unlocks the performance of the next-generation storage technologies. DAOS is a high-performant independent, fault-tolerant storage tier that does not rely on a third-party tier to manage metadata and data resilience. DAOS Features \u00b6 DAOS relies on Open Fabric Interface (OFI) for low-latency communications and stores data on both storage-class memory (SCM) and NVMe storage. DAOS presents a native key-array-value storage interface that offers a unified storage model over which domain-specific data models are ported, such as HDF5 , MPI-IO , and Apache Hadoop . A POSIX I/O emulation layer implementing files and directories over the native DAOS API is also available. DAOS I/O operations are logged and then inserted into a persistent index maintained in SCM. Each I/O is tagged with a particular timestamp called epoch and is associated with a particular version of the dataset. No read-modify-write operations are performed internally. Write operations are non-destructive and not sensitive to alignment. Upon read request, the DAOS service walks through the persistent index and creates a complex scatter-gather Remote Direct Memory Access (RDMA) descriptor to reconstruct the data at the requested version directly in the buffer provided by the application. The SCM storage is memory-mapped directly into the address space of the DAOS service that manages the persistent index via direct load/store. Depending on the I/O characteristics, the DAOS service can decide to store the I/O in either SCM or NVMe storage. As represented in Figure 2-1, latency-sensitive I/Os, like application metadata and byte-granular data, will typically be stored in the former, whereas checkpoints and bulk data will be stored in the latter. This approach allows DAOS to deliver the raw NVMe bandwidth for bulk data by streaming the data to NVMe storage and maintaining internal metadata index in SCM. The Persistent Memory Development Kit (PMDK) allows managing transactional access to SCM and the Storage Performance Development Kit (SPDK) enables user-space I/O to NVMe devices. Figure 2-1. DAOS Storage DAOS aims at delivering: High throughput and IOPS at arbitrary alignment and size Fine-grained I/O operations with true zero-copy I/O to SCM Support for massively distributed NVM storage via scalable collective communications across the storage servers Non-blocking data and metadata operations to allow I/O and computation to overlap Advanced data placement taking into account fault domains Software-managed redundancy supporting both replication and erasure code with an online rebuild End-to-end data integrity Scalable distributed transactions with guaranteed data consistency and automated recovery Dataset snapshot Security framework to manage access control to storage pools Software-defined storage management to provision, configure, modify and monitor storage pools over COTS hardware Native support for Hierarchical Data Format (HDF)5, MPI-IO and POSIX namespace over the DAOS data model Tools for disaster recovery Seamless integration with the Lustre parallel filesystem Mover agent to migrate datasets among DAOS pools and from parallel filesystems to DAOS and vice versa DAOS System \u00b6 A data center may have hundreds of thousands of compute instances interconnected via a scalable high-performance network, where all, or a subset of the instances called storage nodes, have direct access to NVM storage. A DAOS installation involves several components that can be either collocated or distributed. A DAOS system is identified by a system name, and consists of a set of DAOS storage nodes connected to the same network. The DAOS storage nodes run one DAOS server instance per node, which in turn starts one DAOS Engine process per physical socket. Membership of the DAOS servers is recorded into the system map, that assigns a unique integer rank to each Engine process. Two different DAOS systems comprise two disjoint sets of DAOS servers, and do not coordinate with each other. The DAOS server is a multi-tenant daemon running on a Linux instance (either natively on the physical node or in a VM or container) of each storage node . Its Engine sub-processes export the locally-attached SCM and NVM storage through the network. It listens to a management port (addressed by an IP address and a TCP port number), plus one or more fabric endpoints (addressed by network URIs). The DAOS server is configured through a YAML file in /etc/daos, including the configuration of its Engine sub-processes. The DAOS server startup can be integrated with different daemon management or orchestration frameworks (for example a systemd script, a Kubernetes service, or even via a parallel launcher like pdsh or srun). Inside a DAOS Engine, the storage is statically partitioned across multiple targets to optimize concurrency. To avoid contention, each target has its private storage, its own pool of service threads, and its dedicated network context that can be directly addressed over the fabric independently of the other targets hosted on the same storage node. The SCM modules are typically configured in AppDirect interleaved mode. They are thus presented to the operating system as a single PMEM namespace per socket (in fsdax mode). When N targets per engine are configured, each target is using 1/N of the capacity of the fsdax SCM capacity of that socket, independently of the other targets. Each target is also using a fraction of the NVMe capacity of the NVMe drives that are attached to this socket. A target does not implement any internal data protection mechanism against storage media failure. As a result, a target is a single point of failure and the unit of fault. A dynamic state is associated with each target: Its state can be either \"up and running\", or \"down and not available\". A target is the unit of performance. Hardware components associated with the target, such as the backend storage medium, the CPU core(s), and the network, have limited capability and capacity. The number of targets exported by a DAOS Engine instance is configurable, and depends on the underlying hardware (in particular, the number of SCM modules and the number of NVMe SSDs that are served by this engine instance). As a best practice, the number of targets of an engine should be an integer multiple of the number of NVMe drives that are served by this engine. SDK and Tools \u00b6 Applications, users, and administrators can interact with a DAOS system through two different client APIs. The management API offers the ability to administrate a DAOS system. It is intended to be integrated with different vendor-specific storage management or open-source orchestration frameworks. The dmg CLI tool is built over the DAOS management API. On the other hand, the DAOS library ( libdaos ) implements the DAOS storage model. It is primarily targeted at application and I/O middleware developers who want to store datasets in a DAOS system. User utilities like the daos command are also built over the API to allow users to manage datasets from a CLI. Applications can access datasets stored in DAOS either directly through the native DAOS API, through an I/O middleware library (e.g. POSIX emulation, MPI-IO, HDF5) or through frameworks like Spark or TensorFlow that have already been integrated with the native DAOS storage model. Agent \u00b6 The DAOS agent is a daemon residing on the client nodes that interacts with the DAOS library to authenticate the application processes. It is a trusted entity that can sign the DAOS library credentials using certificates. The agent can support different authentication frameworks, and uses a Unix Domain Socket to communicate with the DAOS library.","title":"Architecture"},{"location":"overview/architecture/#architecture","text":"DAOS is an open-source software-defined scale-out object store that provides high bandwidth and high IOPS storage containers to applications and enables next-generation data-centric workflows combining simulation, data analytics, and machine learning. Unlike the traditional storage stacks that were primarily designed for rotating media, DAOS is architected from the ground up to exploit new NVM technologies and is extremely lightweight since it operates End-to-End (E2E) in user space with full OS bypass. DAOS offers a shift away from an I/O model designed for block-based and high-latency storage to one that inherently supports fine-grained data access and unlocks the performance of the next-generation storage technologies. DAOS is a high-performant independent, fault-tolerant storage tier that does not rely on a third-party tier to manage metadata and data resilience.","title":"Architecture"},{"location":"overview/architecture/#daos-features","text":"DAOS relies on Open Fabric Interface (OFI) for low-latency communications and stores data on both storage-class memory (SCM) and NVMe storage. DAOS presents a native key-array-value storage interface that offers a unified storage model over which domain-specific data models are ported, such as HDF5 , MPI-IO , and Apache Hadoop . A POSIX I/O emulation layer implementing files and directories over the native DAOS API is also available. DAOS I/O operations are logged and then inserted into a persistent index maintained in SCM. Each I/O is tagged with a particular timestamp called epoch and is associated with a particular version of the dataset. No read-modify-write operations are performed internally. Write operations are non-destructive and not sensitive to alignment. Upon read request, the DAOS service walks through the persistent index and creates a complex scatter-gather Remote Direct Memory Access (RDMA) descriptor to reconstruct the data at the requested version directly in the buffer provided by the application. The SCM storage is memory-mapped directly into the address space of the DAOS service that manages the persistent index via direct load/store. Depending on the I/O characteristics, the DAOS service can decide to store the I/O in either SCM or NVMe storage. As represented in Figure 2-1, latency-sensitive I/Os, like application metadata and byte-granular data, will typically be stored in the former, whereas checkpoints and bulk data will be stored in the latter. This approach allows DAOS to deliver the raw NVMe bandwidth for bulk data by streaming the data to NVMe storage and maintaining internal metadata index in SCM. The Persistent Memory Development Kit (PMDK) allows managing transactional access to SCM and the Storage Performance Development Kit (SPDK) enables user-space I/O to NVMe devices. Figure 2-1. DAOS Storage DAOS aims at delivering: High throughput and IOPS at arbitrary alignment and size Fine-grained I/O operations with true zero-copy I/O to SCM Support for massively distributed NVM storage via scalable collective communications across the storage servers Non-blocking data and metadata operations to allow I/O and computation to overlap Advanced data placement taking into account fault domains Software-managed redundancy supporting both replication and erasure code with an online rebuild End-to-end data integrity Scalable distributed transactions with guaranteed data consistency and automated recovery Dataset snapshot Security framework to manage access control to storage pools Software-defined storage management to provision, configure, modify and monitor storage pools over COTS hardware Native support for Hierarchical Data Format (HDF)5, MPI-IO and POSIX namespace over the DAOS data model Tools for disaster recovery Seamless integration with the Lustre parallel filesystem Mover agent to migrate datasets among DAOS pools and from parallel filesystems to DAOS and vice versa","title":"DAOS Features"},{"location":"overview/architecture/#daos-system","text":"A data center may have hundreds of thousands of compute instances interconnected via a scalable high-performance network, where all, or a subset of the instances called storage nodes, have direct access to NVM storage. A DAOS installation involves several components that can be either collocated or distributed. A DAOS system is identified by a system name, and consists of a set of DAOS storage nodes connected to the same network. The DAOS storage nodes run one DAOS server instance per node, which in turn starts one DAOS Engine process per physical socket. Membership of the DAOS servers is recorded into the system map, that assigns a unique integer rank to each Engine process. Two different DAOS systems comprise two disjoint sets of DAOS servers, and do not coordinate with each other. The DAOS server is a multi-tenant daemon running on a Linux instance (either natively on the physical node or in a VM or container) of each storage node . Its Engine sub-processes export the locally-attached SCM and NVM storage through the network. It listens to a management port (addressed by an IP address and a TCP port number), plus one or more fabric endpoints (addressed by network URIs). The DAOS server is configured through a YAML file in /etc/daos, including the configuration of its Engine sub-processes. The DAOS server startup can be integrated with different daemon management or orchestration frameworks (for example a systemd script, a Kubernetes service, or even via a parallel launcher like pdsh or srun). Inside a DAOS Engine, the storage is statically partitioned across multiple targets to optimize concurrency. To avoid contention, each target has its private storage, its own pool of service threads, and its dedicated network context that can be directly addressed over the fabric independently of the other targets hosted on the same storage node. The SCM modules are typically configured in AppDirect interleaved mode. They are thus presented to the operating system as a single PMEM namespace per socket (in fsdax mode). When N targets per engine are configured, each target is using 1/N of the capacity of the fsdax SCM capacity of that socket, independently of the other targets. Each target is also using a fraction of the NVMe capacity of the NVMe drives that are attached to this socket. A target does not implement any internal data protection mechanism against storage media failure. As a result, a target is a single point of failure and the unit of fault. A dynamic state is associated with each target: Its state can be either \"up and running\", or \"down and not available\". A target is the unit of performance. Hardware components associated with the target, such as the backend storage medium, the CPU core(s), and the network, have limited capability and capacity. The number of targets exported by a DAOS Engine instance is configurable, and depends on the underlying hardware (in particular, the number of SCM modules and the number of NVMe SSDs that are served by this engine instance). As a best practice, the number of targets of an engine should be an integer multiple of the number of NVMe drives that are served by this engine.","title":"DAOS System"},{"location":"overview/architecture/#sdk-and-tools","text":"Applications, users, and administrators can interact with a DAOS system through two different client APIs. The management API offers the ability to administrate a DAOS system. It is intended to be integrated with different vendor-specific storage management or open-source orchestration frameworks. The dmg CLI tool is built over the DAOS management API. On the other hand, the DAOS library ( libdaos ) implements the DAOS storage model. It is primarily targeted at application and I/O middleware developers who want to store datasets in a DAOS system. User utilities like the daos command are also built over the API to allow users to manage datasets from a CLI. Applications can access datasets stored in DAOS either directly through the native DAOS API, through an I/O middleware library (e.g. POSIX emulation, MPI-IO, HDF5) or through frameworks like Spark or TensorFlow that have already been integrated with the native DAOS storage model.","title":"SDK and Tools"},{"location":"overview/architecture/#agent","text":"The DAOS agent is a daemon residing on the client nodes that interacts with the DAOS library to authenticate the application processes. It is a trusted entity that can sign the DAOS library credentials using certificates. The agent can support different authentication frameworks, and uses a Unix Domain Socket to communicate with the DAOS library.","title":"Agent"},{"location":"overview/data_integrity/","text":"Data Integrity \u00b6 Arguably, one of the worst things a data storage system can do is to return incorrect data without the requester knowing. While each component in the system (network layer, storage devices) may offer protection against silent data corruption, DAOS provides end-to-end data integrity using checksums to better ensure that user data is not corrupted silently. For DAOS, end-to-end means that the client will calculate and verify checksums, providing protection for data through the entire I/O stack. During a write or update, the DAOS Client library (libdaos.so) calculates a checksum and appends it to the RPC message before transferred over the network. Depending on the configuration, the DAOS Server may or may not calculate checksums to verify the data on receipt. On a fetch, the DAOS Server will send a known good checksum with the requested data to the DAOS Client, which will calculate checksums on the data received and verify. Requirements \u00b6 Key Requirements \u00b6 There are two key requirements that DAOS will support. Detect silent data corruption - Corruption will be detected on the distribution and attribute keys and records within a DAOS object. At a minimum, when corruption is detected, an error will be reported. Correct data corruption - When data corruption is detected, an attempt will be made to recover the data using data redundancy mechanisms. Supportive/Additional Requirements \u00b6 Additionally, DAOS will support: End to End Data Integrity as a Quality of Service Attribute - Container properties are used to enable/disable the use of checksums for data integrity as well as define specific attributes of data integrity feature. Refer to Data Integrity Readme for details on configuring a container with checksums enabled. Minimize Performance Impact - When there is no data corruption, the End to End Data Integrity feature should have minimal performance impacted. If data corruption is detected, performance can be impacted to correct the data. Work is ongoing to minimize performance impact. Inject Errors - The ability to corrupt data within a specific record, key, or checksum will be necessary for testing purposes. Fault injection is used to simulate corruption over the network and on disk. The DAOS_CSUM_CORRUPT_* flags used for data corruption are defined in src/include/daos/common.h . Logging - When data corruption is detected, error logs are captured in the client and server logs. Features not yet supported: Event Logging - When silent data corruption is discovered, an event should be logged in such a way that it can be retrieved with other system health and diagnostic information. Proactive background service task - A background task on the server which scans for and detects (audits checksums) silent data corruption and corrects. Keys and Value Objects \u00b6 Because DAOS is a key/value store, the data for both keys and values is protected, however, the approach for both is slightly different. For the two different value types, single and array, the approach is also slightly different. Keys \u00b6 On an update and fetch, the client calculates a checksum for the data used as the distribution and attribute keys and will send it to the server within the RPC. The server verifies the keys with the checksum. While enumerating keys, the server will calculate checksums for the keys and pack within the RPC message to the client. The client will verify the keys received. Note Checksums for keys are not stored on the server. A hash of the key is calculated and used to index the key in the server tree of the keys (see VOS Key Array Stores ). It is also expected that keys are stored only in Storage Class Memory which has reliable data integrity protection. Values \u00b6 On an update, the client will calculate a checksum for the data of the value and will send it to the server within the RPC. If \"server verify\" is enabled, the server will calculate a new checksum for the value and compare with the checksum received from the client to verify the integrity of the value. If the checksums don't match, then data corruption has occurred and an error is returned to the client indicating that the client should try the update again. Whether \"server verify\" is enabled or not, the server will store the checksum. See VOS for more info about checksum management and storage in VOS. On a fetch, the server will return the stored checksum to the client with the values fetched so the client can verify the values received. If the checksums don't match, then the client will fetch from another replica if available in an attempt to get uncorrupted data. There are some slight variations to this approach for the two different types of values. The following diagram illustrates a basic example. (See Storage Model for more details about the single value and array value types) Single Value \u00b6 A Single Value is an atomic value, meaning that writes to a single value will update the entire value and reads retrieve the entire value. Other DAOS features such as Erasure Codes might split a Single Value into multiple shards to be distributed among multiple storage nodes. Either the whole Single Value (if going to a single node) or each shard (if distributed) will have a checksum calculated, sent to the server, and stored on the server. Note that it is possible for a single value, or shard of a single value, to be smaller than the checksum derived from it. It is advised that if an application needs many small single values to use an Array Type instead. Array Values \u00b6 Unlike Single Values, Array Values can be updated and fetched at any part of an array. In addition, updates to an array are versioned, so a fetch can include parts from multiple versions of the array. Each of these versioned parts of an array are called extents. The following diagrams illustrate a couple examples (also see VOS Key Array Stores for more information): A single extent update (blue line) from index 2-13. A fetched extent (orange line) from index 2-6. The fetch is only part of the original extent written. Many extent updates and different epochs. A fetch from index 2-13 requires parts from each extent. The nature of the array type requires that a more sophisticated approach to creating checksums is used. DAOS uses a \"chunking\" approach where each extent will be broken up into \"chunks\" with a predetermined \"chunk size.\" Checksums will be derived from these chunks. Chunks are aligned with an absolute offset (starting at 0), not an I/O offset. The following diagram illustrates a chunk size configured to be 4 (units is arbitrary in this example). Though not all chunks have a full size of 4, an absolute offset alignment is maintained. The gray boxes around the extents represent the chunks. (See Object Layer for more details about the checksum process on object update and fetch) Checksum calculations \u00b6 The actual checksum calculations are done by the isa-l and isa-l_crypto libraries. However, these libraries are abstracted away from much of DAOS and a common checksum library is used with appropriate adapters to the actual isa-l implementations. common checksum library Performance Impact \u00b6 Calculating checksums can be CPU intensive and will impact performance. To mitigate performance impact, checksum types with hardware acceleration should be chosen. For example, CRC32C is supported by recent Intel CPUs, and many are accelerated via SIMD. Quality \u00b6 Unit and functional testing is performed at many layers. Test executable What's tested Key test files common_test daos_csummer, utility functions to help with chunk alignment src/common/tests/checksum_tests.c vos_test vos_obj_update/fetch apis with checksum params to ensure updating and fetching checksums src/vos/tests/vts_checksum.c srv_checksum_tests Server side logic for adding fetched checksums to an array request. Checksums are appropriately copied or created depending on extent layout. src/object/tests/srv_checksum_tests.c daos_test daos_obj_update/fetch with checksums enabled. The -z flag can be used for specific checksum tests. Also --csum_type flag can be used to enable checksums with any of the other daos_tests src/tests/suite/daos_checksum.c Running Tests \u00b6 With daos_server not running ./commont_test ./vos_test -z ./srv_checksum_tests With daos_server running export DAOS_CSUM_TEST_ALL_TYPE=1 ./daos_server -z ./daos_server -i --csum_type crc64 Life of a checksum (WIP) \u00b6 Rebuild \u00b6 In order for rebuild/migrate process to get checksums so it doesn't have to recalculate them, the object list and object fetch task api's provide a checksum iov parameter. If memory is allocated for the iov, then the daos client will pack the checksums into the it. If insufficient memory is allocated in the buffer, the iov_len will be set to the required capacity and the checksums packed into the buffer is truncated. Client Task API Touch Points \u00b6 dc_obj_fetch_task_create : sets csum iov to daos_obj_fetch_t args. These args are set to the rw_cb_args.shard_args.api_args and accessed through an accessor function (rw_args2csum_iov) in cli_shard.c so that rw_args_store_csum can easily access it. This function, called from dc_rw_cb_csum_verify, will pack the data checksums received from the server into the iov. dc_obj_list_obj_task_create : sets csum iov to daos_obj_list_obj_t args. args.csum is then copied to obj_enum_args.csum in dc_obj_shard_list(). On enum callback (dc_enumerate_cb()) the packed csum buffer is copied from the rpc args to obj_enum_args.csum (which points to the same buffer as the caller's) Rebuild Touch Points \u00b6 migrate_fetch_update_(inline|single|bulk) - the rebuild/migrate functions that write to vos locally must ensure that the checksum is also written. These must use the csum iov param for fetch to get the checksum, then unpack the csums into iod_csum. obj_enum.c is relied on for enumerating the objects to be rebuilt. Because the fetch_update functions will unpack the csums from fetch, it will also unpack the csums for enum, so the unpacking process in obj_enum.c will simply copy the csum_iov to the io (dss_enum_unpack_io) structure in enum_unpack_recxs() and then deep copy to the mrone (migrate_one) structure in migrate_one_insert() . Packing/unpacking checksums \u00b6 When checksums are packed (either for fetch or object list) only the data checksums are included. For object list, only checksums for data that is inlined is included. During a rebuild, if the data is not inlined, then the rebuild process will fetch the rest of the data and also get the checksums. ci_serialize() - \"packs\" checksums by appending the struct to an iov and then appending the checksum info buffer to the iov. This puts the actual checksum just after the checksum structure that describes the checksum. ci_cast() - \"unpacks\" the checksum and describing structure. It does this by casting an iov's buffer to a dcs_csum_info struct and setting the csum_info's checksum pointer to point to the memory just after the structure. It does not copy anything, but really just \"casts\". To get all dcs_csum_infos, a caller would cast the iov, copy the csum_info to a destination, then move to the next csum_info(ci_move_next_iov) in the iov. Because this process modifies the iov structure it is best to use a copy of the iov as a temp structure. VOS \u00b6 akey_update_begin - determines how much extra space needs to be allocated in SCM to account for the checksum Arrays \u00b6 evt_root_activate - evtree root is activated. If has a csum them the root csum properties are set (csum_len, csum_type, csum_chunk_size) evt_desc_csum_fill - if root was activated with a punched record then it won't have had the csum fields set correctly so set them here. Main purpose is to copy the csum to the end of persistent evt record (evt_desc). Enough SCM should have been reserved in akey_update_begin. evt_entry_csum_fill - Copy the csum from the persistent memory to the evt_entry returned. Also copy the csum fields from the evtree root to complete the csum_info structure in the evt_entry. akey_fetch_recx - checksums are saved to the ioc for each found extent. Will be used to be added to to the result later. Update/Fetch (copied from vos/README.md) \u00b6 SV Update: vos_update_end -> akey_update_single -> svt_rec_store Sv Fetch: vos_fetch_begin -> akey_fetch_single -> svt_rec_load EV Update: vos_update_end -> akey_update_recx -> evt_insert EV Fetch: vos_fetch_begin -> akey_fetch_recx -> evt_fill_entry Enumeration \u00b6 For enumeration the csums for the keys and values are packed into an iov dedicated to csums. - fill_key_csum - Checksum is calcuated for the key and packed into the iov - fill_data_csum - pack/serialize the csum_info structure into the iov. Aggregation \u00b6 srv_csum_recalc.c - the checksum verification and calculations occur here Checksum Scrubbing (In Development) \u00b6 A background task will scan (when the storage server is idle to limit performance impact) the Version Object Store (VOS) trees to verify the data integrity with the checksums. Corrective actions can be taken when corruption is detected. See Corrective Actions Scanner \u00b6 Goals/Requirements \u00b6 Detect Silent Data Corruption Proactively - The whole point of the scrubber is to detect silent data corruption before it is fetched. Minimize CPU and I/O Bandwidth - Checksum scrubbing scanner will impact CPU and the I/O bandwidth because it must iterate the VOS tree (I/O to SCM) fetch data (I/O to SSD) and calculate checksums (CPU intensive). To minimize both of these impacts, the server scheduler must be able to throttled the scrubber's I/O and CPU usage. Minimize Media Wear - The background task will minimize media wear by preventing objects from being scrubbed too frequently. A container config/tunable will be used by an operator to define the minimum number of days that should pass before an object is scanned again. Continuous - The background task will be a continuous processes instead of running on a schedule. Once complete immediately start over. Throttling approaches should prevent from scrubbing same objects too frequently. High Level Design \u00b6 Per Pool ULT (I/O xstream) that will iterate containers. If checksums and scrubber is enabled then iterate the object tree. If a record value (SV or array) is not marked corrupted then scan. Fetch the data. Create new ULTs (helper xstream) to calculate checksum for data Compare calculated checksum with stored checksum. After every checksum is calculated, determine if need to sleep or yield . If checksums don't match confirm record is still there (not deleted by aggregation) then update record as corrupted After each object scanned yield to allow the server scheduler to reschedule the next appropriate I/O. Sleep or Yield \u00b6 Sleep for sufficient amount of time to ensure that scanning completes no sooner than configured interval (i.e. once a week or month). For example, if the interval is 1 week and there are 70 checksums that need to be calculated, then at a maximum 10 checksums are calculated a day, spaced roughly every 2.4 hours. If it doesn't need to sleep, then it will yield to allow the server scheduler to prioritize other jobs. Corrective Actions \u00b6 There are two main options for corrective actions when data corruption is discovered, in place data repair and SSD eviction. In Place Data Repair \u00b6 If enabled, when corruption is detected, the value identifier (dkey, akey, recx) will be placed in a queue. When there are available cycles, the value identifier will be used to request the data from a replica if exists and rewrite the data locally. This will continue until the SSD Eviction threshold is reached, in which case, the SSD is assumed to be bad enough that it isn't worth fixing locally and it will be requested to be evicted. SSD Eviction \u00b6 If enabled, when the SSD Eviction Threshold is reached the SSD will be evicted. Current eviction methods are pool and target based so there will need to be a mapping and mechanism in place to evict an SSD. When an SSD is evicted, the rebuild protocol will be invoked. Also, once the SSD Eviction Threshold is reached, the scanner should quit scanning anything on that SSD. Additional Checksum Properties > doc/user/container.md / doc/user/pool.md? \u00b6 These properties are provided when a container or pool is created, but should also be able to update them. When updated, they should be active right away. Scanner Interval - Minimum number of days scanning will take. Could take longer, but if only a few records will pad so takes longer. (Pool property) Disable scrubbing - at container level & pool level Threshold for when to evict SSD (number of corruption events) In Place Correction - If the number checksum errors is below the Eviction Threshold, DAOS will attempt to repair the corrupted data using replicas if they exist. Design Details & Implementation \u00b6 Pool ULT \u00b6 The code for the pool ULT is found in srv_pool_scrub.c . It can be a bit difficult to follow because there are several layers of callback functions due to the nature of how ULTs and the vos_iterator work, but the file is organized such that functions typically call the function above it (either directly or indirectly as a callback). For example (~> is an indirect call, -> is a direct call): ds_start_scrubbing_ult ~> scrubbing_ult -> scrub_pool ~> cont_iter_scrub_cb -> scrub_cont ~> obj_iter_scrub_cb ... Silent Data Corruption Detection (TODO) \u00b6 ::Still todo:: obj_iter_scrub(coh, epr, csummer, pool_uuid, event_handlers, entry, type) { build_iod vos_obj_fetch(coh, oid, epoch, dkey, iod, sgl); // for single value csum = calc_checksum(type, csummer, iod, sgl) compare(csum, entry.csum) // for recx for each chunk calc csum and compare } VOS Layer \u00b6 In order to mark data as corrupted a flag field is added to bio_addr_t which includes a CORRUPTED bit. The vos update api already accepts a flag, so a CORRUPTED flag is added and handled during an update so that, if set, the bio address will be updated to be corrupted. On fetch, if a value is already marked corrupted, return -DER_CSUM Object Layer \u00b6 When corruption is detected on the server during a fetch, aggregation, or rebuild the server calls VOS to update value as corrupted. (TBD) Add Server Side Verifying on fetch so can know if media or network corruption (note: need something so extents aren't double verified?) Debugging \u00b6 In the server.yml configuration file set the following env_vars - D_LOG_MASK=DEBUG - DD_SUBSYS=pool - DD_MASK=csum","title":"Data Integrity"},{"location":"overview/data_integrity/#data-integrity","text":"Arguably, one of the worst things a data storage system can do is to return incorrect data without the requester knowing. While each component in the system (network layer, storage devices) may offer protection against silent data corruption, DAOS provides end-to-end data integrity using checksums to better ensure that user data is not corrupted silently. For DAOS, end-to-end means that the client will calculate and verify checksums, providing protection for data through the entire I/O stack. During a write or update, the DAOS Client library (libdaos.so) calculates a checksum and appends it to the RPC message before transferred over the network. Depending on the configuration, the DAOS Server may or may not calculate checksums to verify the data on receipt. On a fetch, the DAOS Server will send a known good checksum with the requested data to the DAOS Client, which will calculate checksums on the data received and verify.","title":"Data Integrity"},{"location":"overview/data_integrity/#requirements","text":"","title":"Requirements"},{"location":"overview/data_integrity/#key-requirements","text":"There are two key requirements that DAOS will support. Detect silent data corruption - Corruption will be detected on the distribution and attribute keys and records within a DAOS object. At a minimum, when corruption is detected, an error will be reported. Correct data corruption - When data corruption is detected, an attempt will be made to recover the data using data redundancy mechanisms.","title":"Key Requirements"},{"location":"overview/data_integrity/#supportiveadditional-requirements","text":"Additionally, DAOS will support: End to End Data Integrity as a Quality of Service Attribute - Container properties are used to enable/disable the use of checksums for data integrity as well as define specific attributes of data integrity feature. Refer to Data Integrity Readme for details on configuring a container with checksums enabled. Minimize Performance Impact - When there is no data corruption, the End to End Data Integrity feature should have minimal performance impacted. If data corruption is detected, performance can be impacted to correct the data. Work is ongoing to minimize performance impact. Inject Errors - The ability to corrupt data within a specific record, key, or checksum will be necessary for testing purposes. Fault injection is used to simulate corruption over the network and on disk. The DAOS_CSUM_CORRUPT_* flags used for data corruption are defined in src/include/daos/common.h . Logging - When data corruption is detected, error logs are captured in the client and server logs. Features not yet supported: Event Logging - When silent data corruption is discovered, an event should be logged in such a way that it can be retrieved with other system health and diagnostic information. Proactive background service task - A background task on the server which scans for and detects (audits checksums) silent data corruption and corrects.","title":"Supportive/Additional Requirements"},{"location":"overview/data_integrity/#keys-and-value-objects","text":"Because DAOS is a key/value store, the data for both keys and values is protected, however, the approach for both is slightly different. For the two different value types, single and array, the approach is also slightly different.","title":"Keys and Value Objects"},{"location":"overview/data_integrity/#keys","text":"On an update and fetch, the client calculates a checksum for the data used as the distribution and attribute keys and will send it to the server within the RPC. The server verifies the keys with the checksum. While enumerating keys, the server will calculate checksums for the keys and pack within the RPC message to the client. The client will verify the keys received. Note Checksums for keys are not stored on the server. A hash of the key is calculated and used to index the key in the server tree of the keys (see VOS Key Array Stores ). It is also expected that keys are stored only in Storage Class Memory which has reliable data integrity protection.","title":"Keys"},{"location":"overview/data_integrity/#values","text":"On an update, the client will calculate a checksum for the data of the value and will send it to the server within the RPC. If \"server verify\" is enabled, the server will calculate a new checksum for the value and compare with the checksum received from the client to verify the integrity of the value. If the checksums don't match, then data corruption has occurred and an error is returned to the client indicating that the client should try the update again. Whether \"server verify\" is enabled or not, the server will store the checksum. See VOS for more info about checksum management and storage in VOS. On a fetch, the server will return the stored checksum to the client with the values fetched so the client can verify the values received. If the checksums don't match, then the client will fetch from another replica if available in an attempt to get uncorrupted data. There are some slight variations to this approach for the two different types of values. The following diagram illustrates a basic example. (See Storage Model for more details about the single value and array value types)","title":"Values"},{"location":"overview/data_integrity/#single-value","text":"A Single Value is an atomic value, meaning that writes to a single value will update the entire value and reads retrieve the entire value. Other DAOS features such as Erasure Codes might split a Single Value into multiple shards to be distributed among multiple storage nodes. Either the whole Single Value (if going to a single node) or each shard (if distributed) will have a checksum calculated, sent to the server, and stored on the server. Note that it is possible for a single value, or shard of a single value, to be smaller than the checksum derived from it. It is advised that if an application needs many small single values to use an Array Type instead.","title":"Single Value"},{"location":"overview/data_integrity/#array-values","text":"Unlike Single Values, Array Values can be updated and fetched at any part of an array. In addition, updates to an array are versioned, so a fetch can include parts from multiple versions of the array. Each of these versioned parts of an array are called extents. The following diagrams illustrate a couple examples (also see VOS Key Array Stores for more information): A single extent update (blue line) from index 2-13. A fetched extent (orange line) from index 2-6. The fetch is only part of the original extent written. Many extent updates and different epochs. A fetch from index 2-13 requires parts from each extent. The nature of the array type requires that a more sophisticated approach to creating checksums is used. DAOS uses a \"chunking\" approach where each extent will be broken up into \"chunks\" with a predetermined \"chunk size.\" Checksums will be derived from these chunks. Chunks are aligned with an absolute offset (starting at 0), not an I/O offset. The following diagram illustrates a chunk size configured to be 4 (units is arbitrary in this example). Though not all chunks have a full size of 4, an absolute offset alignment is maintained. The gray boxes around the extents represent the chunks. (See Object Layer for more details about the checksum process on object update and fetch)","title":"Array Values"},{"location":"overview/data_integrity/#checksum-calculations","text":"The actual checksum calculations are done by the isa-l and isa-l_crypto libraries. However, these libraries are abstracted away from much of DAOS and a common checksum library is used with appropriate adapters to the actual isa-l implementations. common checksum library","title":"Checksum calculations"},{"location":"overview/data_integrity/#performance-impact","text":"Calculating checksums can be CPU intensive and will impact performance. To mitigate performance impact, checksum types with hardware acceleration should be chosen. For example, CRC32C is supported by recent Intel CPUs, and many are accelerated via SIMD.","title":"Performance Impact"},{"location":"overview/data_integrity/#quality","text":"Unit and functional testing is performed at many layers. Test executable What's tested Key test files common_test daos_csummer, utility functions to help with chunk alignment src/common/tests/checksum_tests.c vos_test vos_obj_update/fetch apis with checksum params to ensure updating and fetching checksums src/vos/tests/vts_checksum.c srv_checksum_tests Server side logic for adding fetched checksums to an array request. Checksums are appropriately copied or created depending on extent layout. src/object/tests/srv_checksum_tests.c daos_test daos_obj_update/fetch with checksums enabled. The -z flag can be used for specific checksum tests. Also --csum_type flag can be used to enable checksums with any of the other daos_tests src/tests/suite/daos_checksum.c","title":"Quality"},{"location":"overview/data_integrity/#running-tests","text":"With daos_server not running ./commont_test ./vos_test -z ./srv_checksum_tests With daos_server running export DAOS_CSUM_TEST_ALL_TYPE=1 ./daos_server -z ./daos_server -i --csum_type crc64","title":"Running Tests"},{"location":"overview/data_integrity/#life-of-a-checksum-wip","text":"","title":"Life of a checksum (WIP)"},{"location":"overview/data_integrity/#rebuild","text":"In order for rebuild/migrate process to get checksums so it doesn't have to recalculate them, the object list and object fetch task api's provide a checksum iov parameter. If memory is allocated for the iov, then the daos client will pack the checksums into the it. If insufficient memory is allocated in the buffer, the iov_len will be set to the required capacity and the checksums packed into the buffer is truncated.","title":"Rebuild"},{"location":"overview/data_integrity/#client-task-api-touch-points","text":"dc_obj_fetch_task_create : sets csum iov to daos_obj_fetch_t args. These args are set to the rw_cb_args.shard_args.api_args and accessed through an accessor function (rw_args2csum_iov) in cli_shard.c so that rw_args_store_csum can easily access it. This function, called from dc_rw_cb_csum_verify, will pack the data checksums received from the server into the iov. dc_obj_list_obj_task_create : sets csum iov to daos_obj_list_obj_t args. args.csum is then copied to obj_enum_args.csum in dc_obj_shard_list(). On enum callback (dc_enumerate_cb()) the packed csum buffer is copied from the rpc args to obj_enum_args.csum (which points to the same buffer as the caller's)","title":"Client Task API Touch Points"},{"location":"overview/data_integrity/#rebuild-touch-points","text":"migrate_fetch_update_(inline|single|bulk) - the rebuild/migrate functions that write to vos locally must ensure that the checksum is also written. These must use the csum iov param for fetch to get the checksum, then unpack the csums into iod_csum. obj_enum.c is relied on for enumerating the objects to be rebuilt. Because the fetch_update functions will unpack the csums from fetch, it will also unpack the csums for enum, so the unpacking process in obj_enum.c will simply copy the csum_iov to the io (dss_enum_unpack_io) structure in enum_unpack_recxs() and then deep copy to the mrone (migrate_one) structure in migrate_one_insert() .","title":"Rebuild Touch Points"},{"location":"overview/data_integrity/#packingunpacking-checksums","text":"When checksums are packed (either for fetch or object list) only the data checksums are included. For object list, only checksums for data that is inlined is included. During a rebuild, if the data is not inlined, then the rebuild process will fetch the rest of the data and also get the checksums. ci_serialize() - \"packs\" checksums by appending the struct to an iov and then appending the checksum info buffer to the iov. This puts the actual checksum just after the checksum structure that describes the checksum. ci_cast() - \"unpacks\" the checksum and describing structure. It does this by casting an iov's buffer to a dcs_csum_info struct and setting the csum_info's checksum pointer to point to the memory just after the structure. It does not copy anything, but really just \"casts\". To get all dcs_csum_infos, a caller would cast the iov, copy the csum_info to a destination, then move to the next csum_info(ci_move_next_iov) in the iov. Because this process modifies the iov structure it is best to use a copy of the iov as a temp structure.","title":"Packing/unpacking checksums"},{"location":"overview/data_integrity/#vos","text":"akey_update_begin - determines how much extra space needs to be allocated in SCM to account for the checksum","title":"VOS"},{"location":"overview/data_integrity/#arrays","text":"evt_root_activate - evtree root is activated. If has a csum them the root csum properties are set (csum_len, csum_type, csum_chunk_size) evt_desc_csum_fill - if root was activated with a punched record then it won't have had the csum fields set correctly so set them here. Main purpose is to copy the csum to the end of persistent evt record (evt_desc). Enough SCM should have been reserved in akey_update_begin. evt_entry_csum_fill - Copy the csum from the persistent memory to the evt_entry returned. Also copy the csum fields from the evtree root to complete the csum_info structure in the evt_entry. akey_fetch_recx - checksums are saved to the ioc for each found extent. Will be used to be added to to the result later.","title":"Arrays"},{"location":"overview/data_integrity/#updatefetch-copied-from-vosreadmemd","text":"SV Update: vos_update_end -> akey_update_single -> svt_rec_store Sv Fetch: vos_fetch_begin -> akey_fetch_single -> svt_rec_load EV Update: vos_update_end -> akey_update_recx -> evt_insert EV Fetch: vos_fetch_begin -> akey_fetch_recx -> evt_fill_entry","title":"Update/Fetch (copied from vos/README.md)"},{"location":"overview/data_integrity/#enumeration","text":"For enumeration the csums for the keys and values are packed into an iov dedicated to csums. - fill_key_csum - Checksum is calcuated for the key and packed into the iov - fill_data_csum - pack/serialize the csum_info structure into the iov.","title":"Enumeration"},{"location":"overview/data_integrity/#aggregation","text":"srv_csum_recalc.c - the checksum verification and calculations occur here","title":"Aggregation"},{"location":"overview/data_integrity/#checksum-scrubbing-in-development","text":"A background task will scan (when the storage server is idle to limit performance impact) the Version Object Store (VOS) trees to verify the data integrity with the checksums. Corrective actions can be taken when corruption is detected. See Corrective Actions","title":"Checksum Scrubbing (In Development)"},{"location":"overview/data_integrity/#scanner","text":"","title":"Scanner"},{"location":"overview/data_integrity/#goalsrequirements","text":"Detect Silent Data Corruption Proactively - The whole point of the scrubber is to detect silent data corruption before it is fetched. Minimize CPU and I/O Bandwidth - Checksum scrubbing scanner will impact CPU and the I/O bandwidth because it must iterate the VOS tree (I/O to SCM) fetch data (I/O to SSD) and calculate checksums (CPU intensive). To minimize both of these impacts, the server scheduler must be able to throttled the scrubber's I/O and CPU usage. Minimize Media Wear - The background task will minimize media wear by preventing objects from being scrubbed too frequently. A container config/tunable will be used by an operator to define the minimum number of days that should pass before an object is scanned again. Continuous - The background task will be a continuous processes instead of running on a schedule. Once complete immediately start over. Throttling approaches should prevent from scrubbing same objects too frequently.","title":"Goals/Requirements"},{"location":"overview/data_integrity/#high-level-design","text":"Per Pool ULT (I/O xstream) that will iterate containers. If checksums and scrubber is enabled then iterate the object tree. If a record value (SV or array) is not marked corrupted then scan. Fetch the data. Create new ULTs (helper xstream) to calculate checksum for data Compare calculated checksum with stored checksum. After every checksum is calculated, determine if need to sleep or yield . If checksums don't match confirm record is still there (not deleted by aggregation) then update record as corrupted After each object scanned yield to allow the server scheduler to reschedule the next appropriate I/O.","title":"High Level Design"},{"location":"overview/data_integrity/#sleep-or-yield","text":"Sleep for sufficient amount of time to ensure that scanning completes no sooner than configured interval (i.e. once a week or month). For example, if the interval is 1 week and there are 70 checksums that need to be calculated, then at a maximum 10 checksums are calculated a day, spaced roughly every 2.4 hours. If it doesn't need to sleep, then it will yield to allow the server scheduler to prioritize other jobs.","title":"Sleep or Yield"},{"location":"overview/data_integrity/#corrective-actions","text":"There are two main options for corrective actions when data corruption is discovered, in place data repair and SSD eviction.","title":"Corrective Actions"},{"location":"overview/data_integrity/#in-place-data-repair","text":"If enabled, when corruption is detected, the value identifier (dkey, akey, recx) will be placed in a queue. When there are available cycles, the value identifier will be used to request the data from a replica if exists and rewrite the data locally. This will continue until the SSD Eviction threshold is reached, in which case, the SSD is assumed to be bad enough that it isn't worth fixing locally and it will be requested to be evicted.","title":"In Place Data Repair"},{"location":"overview/data_integrity/#ssd-eviction","text":"If enabled, when the SSD Eviction Threshold is reached the SSD will be evicted. Current eviction methods are pool and target based so there will need to be a mapping and mechanism in place to evict an SSD. When an SSD is evicted, the rebuild protocol will be invoked. Also, once the SSD Eviction Threshold is reached, the scanner should quit scanning anything on that SSD.","title":"SSD Eviction"},{"location":"overview/data_integrity/#additional-checksum-properties-docusercontainermd-docuserpoolmd","text":"These properties are provided when a container or pool is created, but should also be able to update them. When updated, they should be active right away. Scanner Interval - Minimum number of days scanning will take. Could take longer, but if only a few records will pad so takes longer. (Pool property) Disable scrubbing - at container level & pool level Threshold for when to evict SSD (number of corruption events) In Place Correction - If the number checksum errors is below the Eviction Threshold, DAOS will attempt to repair the corrupted data using replicas if they exist.","title":"Additional Checksum Properties &gt; doc/user/container.md / doc/user/pool.md?"},{"location":"overview/data_integrity/#design-details-implementation","text":"","title":"Design Details &amp; Implementation"},{"location":"overview/data_integrity/#pool-ult","text":"The code for the pool ULT is found in srv_pool_scrub.c . It can be a bit difficult to follow because there are several layers of callback functions due to the nature of how ULTs and the vos_iterator work, but the file is organized such that functions typically call the function above it (either directly or indirectly as a callback). For example (~> is an indirect call, -> is a direct call): ds_start_scrubbing_ult ~> scrubbing_ult -> scrub_pool ~> cont_iter_scrub_cb -> scrub_cont ~> obj_iter_scrub_cb ...","title":"Pool ULT"},{"location":"overview/data_integrity/#silent-data-corruption-detection-todo","text":"::Still todo:: obj_iter_scrub(coh, epr, csummer, pool_uuid, event_handlers, entry, type) { build_iod vos_obj_fetch(coh, oid, epoch, dkey, iod, sgl); // for single value csum = calc_checksum(type, csummer, iod, sgl) compare(csum, entry.csum) // for recx for each chunk calc csum and compare }","title":"Silent Data Corruption Detection (TODO)"},{"location":"overview/data_integrity/#vos-layer","text":"In order to mark data as corrupted a flag field is added to bio_addr_t which includes a CORRUPTED bit. The vos update api already accepts a flag, so a CORRUPTED flag is added and handled during an update so that, if set, the bio address will be updated to be corrupted. On fetch, if a value is already marked corrupted, return -DER_CSUM","title":"VOS Layer"},{"location":"overview/data_integrity/#object-layer","text":"When corruption is detected on the server during a fetch, aggregation, or rebuild the server calls VOS to update value as corrupted. (TBD) Add Server Side Verifying on fetch so can know if media or network corruption (note: need something so extents aren't double verified?)","title":"Object Layer"},{"location":"overview/data_integrity/#debugging","text":"In the server.yml configuration file set the following env_vars - D_LOG_MASK=DEBUG - DD_SUBSYS=pool - DD_MASK=csum","title":"Debugging"},{"location":"overview/fault/","text":"Fault Model \u00b6 DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. DAOS internal pool and container metadata are replicated via a robust consensus algorithm. DAOS objects are then safely replicated or erasure-coded by transparently leveraging the DAOS distributed transaction mechanisms internally. The purpose of this section is to provide details on how DAOS achieves fault tolerance and guarantees object resilience. Hierarchical Fault Domains \u00b6 A fault domain is a set of servers sharing the same point of failure and which are thus likely to fail altogether. DAOS assumes that fault domains are hierarchical and do not overlap. The actual hierarchy and fault domain membership must be supplied by an external database used by DAOS to generate the pool map. Pool metadata are replicated on several nodes from different high-level fault domains for high availability, whereas object data is replicated or erasure-coded over a variable number of fault domains depending on the selected object class. Fault Detection \u00b6 DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure will make an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed. Fault Isolation \u00b6 Once detected, the faulty target or servers (effectivelly a set of targets) must be excluded from the pool map. This process is triggered either manually by the administrator or automatically. Upon exclusion, the new version of the pool map is eagerly pushed to all storage targets. At this point, the pool enters a degraded mode that might require extra processing on access (e.g. reconstructing data out of erasure code). Consequently, DAOS client and storage nodes retry RPC indefinitely until they find an alternative replacement target from the new pool map. At this point, all outstanding communications with the evicted target are aborted, and no further messages should be sent to the target until it is explicitly reintegrated (possibly only after maintenance action). All storage targets are promptly notified of pool map changes by the pool service. This is not the case for client nodes, which are lazily informed of pool map invalidation each time they communicate with servers. To do so, clients pack in every RPC their current pool map version. Servers reply not only with the current pool map version. Consequently, when a DAOS client experiences RPC timeout, it regularly communicates with the other DAOS target to guarantee that its pool map is always current. Clients will then eventually be informed of the target exclusion and enter into degraded mode. This mechanism guarantees global node eviction and that all nodes eventually share the same view of target aliveness. Fault Recovery \u00b6 Upon exclusion from the pool map, each target starts the rebuild process automatically to restore data redundancy. First, each target creates a list of local objects impacted by the target exclusion. This is done by scanning a local object table maintained by the underlying storage layer. Then for each impacted object, the location of the new object shard is determined and redundancy of the object restored for the whole history (i.e., snapshots). Once all impacted objects have been rebuilt, the pool map is updated a second time to report the target as failed out. This marks the end of collective rebuild process and the exit from degraded mode for this particular fault. At this point, the pool has fully recovered from the fault and client nodes can now read from the rebuilt object shards. This rebuild process is executed online while applications continue accessing and updating objects.","title":"Fault Model"},{"location":"overview/fault/#fault-model","text":"DAOS relies on massively distributed single-ported storage. Each target is thus effectively a single point of failure. DAOS achieves availability and durability of both data and metadata by providing redundancy across targets in different fault domains. DAOS internal pool and container metadata are replicated via a robust consensus algorithm. DAOS objects are then safely replicated or erasure-coded by transparently leveraging the DAOS distributed transaction mechanisms internally. The purpose of this section is to provide details on how DAOS achieves fault tolerance and guarantees object resilience.","title":"Fault Model"},{"location":"overview/fault/#hierarchical-fault-domains","text":"A fault domain is a set of servers sharing the same point of failure and which are thus likely to fail altogether. DAOS assumes that fault domains are hierarchical and do not overlap. The actual hierarchy and fault domain membership must be supplied by an external database used by DAOS to generate the pool map. Pool metadata are replicated on several nodes from different high-level fault domains for high availability, whereas object data is replicated or erasure-coded over a variable number of fault domains depending on the selected object class.","title":"Hierarchical Fault Domains"},{"location":"overview/fault/#fault-detection","text":"DAOS servers are monitored within a DAOS system through a gossip-based protocol called SWIM that provides accurate, efficient, and scalable server fault detection. Storage attached to each DAOS target is monitored through periodic local health assessment. Whenever a local storage I/O error is returned to the DAOS server, an internal health check procedure will be called automatically. This procedure will make an overall health assessment by analyzing the IO error code and device SMART/Health data. If the result is negative, the target will be marked as faulty, and further I/Os to this target will be rejected and re-routed.","title":"Fault Detection"},{"location":"overview/fault/#fault-isolation","text":"Once detected, the faulty target or servers (effectivelly a set of targets) must be excluded from the pool map. This process is triggered either manually by the administrator or automatically. Upon exclusion, the new version of the pool map is eagerly pushed to all storage targets. At this point, the pool enters a degraded mode that might require extra processing on access (e.g. reconstructing data out of erasure code). Consequently, DAOS client and storage nodes retry RPC indefinitely until they find an alternative replacement target from the new pool map. At this point, all outstanding communications with the evicted target are aborted, and no further messages should be sent to the target until it is explicitly reintegrated (possibly only after maintenance action). All storage targets are promptly notified of pool map changes by the pool service. This is not the case for client nodes, which are lazily informed of pool map invalidation each time they communicate with servers. To do so, clients pack in every RPC their current pool map version. Servers reply not only with the current pool map version. Consequently, when a DAOS client experiences RPC timeout, it regularly communicates with the other DAOS target to guarantee that its pool map is always current. Clients will then eventually be informed of the target exclusion and enter into degraded mode. This mechanism guarantees global node eviction and that all nodes eventually share the same view of target aliveness.","title":"Fault Isolation"},{"location":"overview/fault/#fault-recovery","text":"Upon exclusion from the pool map, each target starts the rebuild process automatically to restore data redundancy. First, each target creates a list of local objects impacted by the target exclusion. This is done by scanning a local object table maintained by the underlying storage layer. Then for each impacted object, the location of the new object shard is determined and redundancy of the object restored for the whole history (i.e., snapshots). Once all impacted objects have been rebuilt, the pool map is updated a second time to report the target as failed out. This marks the end of collective rebuild process and the exit from degraded mode for this particular fault. At this point, the pool has fully recovered from the fault and client nodes can now read from the rebuilt object shards. This rebuild process is executed online while applications continue accessing and updating objects.","title":"Fault Recovery"},{"location":"overview/security/","text":"Security Model \u00b6 DAOS uses a flexible security model that separates authentication from authorization. It is designed to have a minimal impact on the I/O path. DAOS does not provide any transport security for the fabric network used for I/O transfers. When deploying DAOS, the administrator is responsible for secure configuration of their specific fabric network. For RDMA over Ethernet, enabling IPsec is recommended. See the RDMA protocol spec (RFC 5040) for more information. There are two areas where DAOS implements its own layer of security. At the user level, clients must be able to read and modify only pools and containers to which they have been granted access. At the system and administrative levels, only authorized components must be able to access the DAOS management network. Authentication \u00b6 There are different means of authentication, depending on whether the caller is accessing client resources or the DAOS management network. Client Library \u00b6 The client library libdaos is an untrusted component. The daos user-level command that uses the client library is also an untrusted component. A trusted process, the DAOS agent ( daos_agent ), runs on each client node and authenticates the user processes. The DAOS security model is designed to support different authentication methods for client processes. Currently, we support only the AUTH_SYS authentication flavor, as defined for NFS in RFC 2623 . DAOS Management Network \u00b6 The DAOS management components communicate over the network using the gRPC protocol . Each trusted DAOS component ( daos_server , daos_agent , and the dmg administrative tool) is authenticated by means of a certificate generated for that component by the system administrator. All of the component certificates must be generated with the same root certificate and distributed to the appropriate DAOS nodes, as described in the DAOS Administration Guide . DAOS components identify one another over the DAOS management network via gRPC over mutually-authenticated TLS using their respective component certificates. DAOS verifies the certificate chain, as well as the Common Name (CN) in the certificate, to authenticate the component's identity. Authorization \u00b6 Client authorization for resources is controlled by the Access Control List (ACL) on the resource. Authorization on the management network is achieved by settings on the certificates that are generated while setting up the DAOS system. Component Certificates \u00b6 Access to DAOS management RPCs is controlled via the CommonName (CN) set in each management component certificate. A given management RPC may only be invoked by a component which connects with the correct certificate. Access Control Lists \u00b6 Client access to resources like pools and containers is controlled by DAOS Access Control Lists (ACLs). These ACLs are derived in part from NFSv4 ACLs, and adapted for the unique needs of a distributed system. The client may request read-only or read-write access to the resource. If the resource ACL doesn't grant them the requested access level, they won't be able to connect. While connected, their handle to that resource grants permissions for specific actions. The permissions of a handle last for the duration of its existence, similar to an open file descriptor in a POSIX system. A handle cannot currently be revoked. A DAOS ACL is composed of zero or more Access Control Entries (ACEs). The ACEs are the rules used to grant or deny privileges to a user who requests access to a resource. Access Control Entries \u00b6 In the input and output of DAOS tools, an Access Control Entry (ACE) is defined using a colon-separated string with the following format: TYPE:FLAGS:PRINCIPAL:PERMISSIONS The contents of all the fields are case-sensitive. Type \u00b6 The type of ACE entry (mandatory). Only one type of ACE is supported at this time. A (Allow): Allow access to the specified principal for the given permissions. Flags \u00b6 The (optional) flags provide additional information about how the ACE should be interpreted. G (Group): The principal should be interpreted as a group. Principal \u00b6 The principal (also called the identity) is specified in the name@domain format. The domain should be left off if the name is a UNIX user/group on the local domain. Currently, this is the only case supported by DAOS. There are three special principals, OWNER@ , GROUP@ , and EVERYONE@ , which align with User, Group, and Other from traditional POSIX permission bits. When providing them in the ACE string format, they must be spelled exactly as written here, in uppercase with no domain appended. The GROUP@ entry must also have the G (group) flag. Permissions \u00b6 The permissions in a resource's ACE permit a certain type of user access to the resource. The order of the permission \"bits\" (characters) within the PERMISSIONS field of the ACE is not significant. Permission Pool Meaning Container Meaning r (Read) Alias for 't' Read container data and attributes w (Write) Alias for 'c' + 'd' Write container data and attributes c (Create) Create containers N/A d (Delete) Delete any container Delete this container t (Get-Prop) Connect/query Get container properties T (Set-Prop) N/A Set/Change container properties a (Get-ACL) N/A Get container ACL A (Set-ACL) N/A Set/Change container ACL o (Set-Owner) N/A Set/Change container's owner user and group ACEs containing permissions not applicable to the given resource are considered invalid. To allow a user/group to connect to a resource, that principal's permissions must include at least some form of read access (for example, read or get-prop ). A user with write -only permissions will be rejected when requesting RW access to a resource. Denying Access \u00b6 Currently, only \"Allow\" Access Control Entries are supported. However, it is possible to deny access to a specific user by creating an Allow entry for them with no permissions. This is fundamentally different from removing a user's ACE, which allows other ACEs in the ACL to determine their access. It is not possible to deny access to a specific group in this way, due to the way group permissions are enforced . ACE Examples \u00b6 A::daos_user@:rw Allow the UNIX user named daos_user to have read-write access. A:G:project_users@:tc Allow anyone in the UNIX group project_users to access a pool's contents and create containers. A::OWNER@:rwdtTaAo Allow the UNIX user who owns the container to have full control. A:G:GROUP@:rwdtT Allow the UNIX group that owns the container to read and write data, delete the container, and manipulate container properties. A::EVERYONE@:r Allow any user not covered by other rules to have read-only access. A::daos_user@: Deny the UNIX user named daos_user any access to the resource. Enforcement \u00b6 Access Control Entries (ACEs) will be enforced in the following order: Owner-User Named users Owner-Group and named groups Everyone In general, enforcement will be based on the first match, ignoring lower-priority entries. If the user is the owner of the resource and there is an OWNER@ entry, they will receive the owner permissions only. They will not receive any of the permissions in the named user/group entries, even if they would match those other entries. If the user isn't the owner, or there is no OWNER@ entry, but there is an ACE for their user identity, they will receive the permissions for their user identity only. They will not receive the permissions for any of their groups, even if those group entries have broader permissions than the user entry does. The user is expected to match at most one user entry. If no matching user entry is found, but entries match one or more of the user's groups, enforcement will be based on the union of the permissions of all matching groups, including the owner-group GROUP@ . If no matching groups are found, the EVERYONE@ entry's permissions will be used, if it exists. By default, if a user matches no ACEs in the ACL list, access will be denied. ACL File \u00b6 Tools that accept an ACL file expect it to be a simple text file with one ACE on each line. A line may be marked as a comment by using a # as the first non-whitespace character on the line. For example: # ACL for my container # Owner can't touch data - just do admin-type things A::OWNER@:dtTaAo # My project's users can generate and access data A:G:my_great_project@:rw # Bob can use the data to generate a report A::bob@:r The permission bits and the ACEs themselves don't need to be in any specific order. However the order may be different when the resulting ACL is parsed and displayed by DAOS. Limitations \u00b6 The maximum size of the ACE list in a DAOS ACL internal data structure is 64KiB. To calculate the internal data size of an ACL, use the following formula for each ACE: The base size of an ACE is 256 Bytes. If the ACE principal is not one of the special principals: Add the length of the principal string + 1. If that value is not 64-Byte aligned, round up to the nearest 64-Byte boundary.","title":"Security Model"},{"location":"overview/security/#security-model","text":"DAOS uses a flexible security model that separates authentication from authorization. It is designed to have a minimal impact on the I/O path. DAOS does not provide any transport security for the fabric network used for I/O transfers. When deploying DAOS, the administrator is responsible for secure configuration of their specific fabric network. For RDMA over Ethernet, enabling IPsec is recommended. See the RDMA protocol spec (RFC 5040) for more information. There are two areas where DAOS implements its own layer of security. At the user level, clients must be able to read and modify only pools and containers to which they have been granted access. At the system and administrative levels, only authorized components must be able to access the DAOS management network.","title":"Security Model"},{"location":"overview/security/#authentication","text":"There are different means of authentication, depending on whether the caller is accessing client resources or the DAOS management network.","title":"Authentication"},{"location":"overview/security/#client-library","text":"The client library libdaos is an untrusted component. The daos user-level command that uses the client library is also an untrusted component. A trusted process, the DAOS agent ( daos_agent ), runs on each client node and authenticates the user processes. The DAOS security model is designed to support different authentication methods for client processes. Currently, we support only the AUTH_SYS authentication flavor, as defined for NFS in RFC 2623 .","title":"Client Library"},{"location":"overview/security/#daos-management-network","text":"The DAOS management components communicate over the network using the gRPC protocol . Each trusted DAOS component ( daos_server , daos_agent , and the dmg administrative tool) is authenticated by means of a certificate generated for that component by the system administrator. All of the component certificates must be generated with the same root certificate and distributed to the appropriate DAOS nodes, as described in the DAOS Administration Guide . DAOS components identify one another over the DAOS management network via gRPC over mutually-authenticated TLS using their respective component certificates. DAOS verifies the certificate chain, as well as the Common Name (CN) in the certificate, to authenticate the component's identity.","title":"DAOS Management Network"},{"location":"overview/security/#authorization","text":"Client authorization for resources is controlled by the Access Control List (ACL) on the resource. Authorization on the management network is achieved by settings on the certificates that are generated while setting up the DAOS system.","title":"Authorization"},{"location":"overview/security/#component-certificates","text":"Access to DAOS management RPCs is controlled via the CommonName (CN) set in each management component certificate. A given management RPC may only be invoked by a component which connects with the correct certificate.","title":"Component Certificates"},{"location":"overview/security/#access-control-lists","text":"Client access to resources like pools and containers is controlled by DAOS Access Control Lists (ACLs). These ACLs are derived in part from NFSv4 ACLs, and adapted for the unique needs of a distributed system. The client may request read-only or read-write access to the resource. If the resource ACL doesn't grant them the requested access level, they won't be able to connect. While connected, their handle to that resource grants permissions for specific actions. The permissions of a handle last for the duration of its existence, similar to an open file descriptor in a POSIX system. A handle cannot currently be revoked. A DAOS ACL is composed of zero or more Access Control Entries (ACEs). The ACEs are the rules used to grant or deny privileges to a user who requests access to a resource.","title":"Access Control Lists"},{"location":"overview/security/#access-control-entries","text":"In the input and output of DAOS tools, an Access Control Entry (ACE) is defined using a colon-separated string with the following format: TYPE:FLAGS:PRINCIPAL:PERMISSIONS The contents of all the fields are case-sensitive.","title":"Access Control Entries"},{"location":"overview/security/#type","text":"The type of ACE entry (mandatory). Only one type of ACE is supported at this time. A (Allow): Allow access to the specified principal for the given permissions.","title":"Type"},{"location":"overview/security/#flags","text":"The (optional) flags provide additional information about how the ACE should be interpreted. G (Group): The principal should be interpreted as a group.","title":"Flags"},{"location":"overview/security/#principal","text":"The principal (also called the identity) is specified in the name@domain format. The domain should be left off if the name is a UNIX user/group on the local domain. Currently, this is the only case supported by DAOS. There are three special principals, OWNER@ , GROUP@ , and EVERYONE@ , which align with User, Group, and Other from traditional POSIX permission bits. When providing them in the ACE string format, they must be spelled exactly as written here, in uppercase with no domain appended. The GROUP@ entry must also have the G (group) flag.","title":"Principal"},{"location":"overview/security/#permissions","text":"The permissions in a resource's ACE permit a certain type of user access to the resource. The order of the permission \"bits\" (characters) within the PERMISSIONS field of the ACE is not significant. Permission Pool Meaning Container Meaning r (Read) Alias for 't' Read container data and attributes w (Write) Alias for 'c' + 'd' Write container data and attributes c (Create) Create containers N/A d (Delete) Delete any container Delete this container t (Get-Prop) Connect/query Get container properties T (Set-Prop) N/A Set/Change container properties a (Get-ACL) N/A Get container ACL A (Set-ACL) N/A Set/Change container ACL o (Set-Owner) N/A Set/Change container's owner user and group ACEs containing permissions not applicable to the given resource are considered invalid. To allow a user/group to connect to a resource, that principal's permissions must include at least some form of read access (for example, read or get-prop ). A user with write -only permissions will be rejected when requesting RW access to a resource.","title":"Permissions"},{"location":"overview/security/#denying-access","text":"Currently, only \"Allow\" Access Control Entries are supported. However, it is possible to deny access to a specific user by creating an Allow entry for them with no permissions. This is fundamentally different from removing a user's ACE, which allows other ACEs in the ACL to determine their access. It is not possible to deny access to a specific group in this way, due to the way group permissions are enforced .","title":"Denying Access"},{"location":"overview/security/#ace-examples","text":"A::daos_user@:rw Allow the UNIX user named daos_user to have read-write access. A:G:project_users@:tc Allow anyone in the UNIX group project_users to access a pool's contents and create containers. A::OWNER@:rwdtTaAo Allow the UNIX user who owns the container to have full control. A:G:GROUP@:rwdtT Allow the UNIX group that owns the container to read and write data, delete the container, and manipulate container properties. A::EVERYONE@:r Allow any user not covered by other rules to have read-only access. A::daos_user@: Deny the UNIX user named daos_user any access to the resource.","title":"ACE Examples"},{"location":"overview/security/#enforcement","text":"Access Control Entries (ACEs) will be enforced in the following order: Owner-User Named users Owner-Group and named groups Everyone In general, enforcement will be based on the first match, ignoring lower-priority entries. If the user is the owner of the resource and there is an OWNER@ entry, they will receive the owner permissions only. They will not receive any of the permissions in the named user/group entries, even if they would match those other entries. If the user isn't the owner, or there is no OWNER@ entry, but there is an ACE for their user identity, they will receive the permissions for their user identity only. They will not receive the permissions for any of their groups, even if those group entries have broader permissions than the user entry does. The user is expected to match at most one user entry. If no matching user entry is found, but entries match one or more of the user's groups, enforcement will be based on the union of the permissions of all matching groups, including the owner-group GROUP@ . If no matching groups are found, the EVERYONE@ entry's permissions will be used, if it exists. By default, if a user matches no ACEs in the ACL list, access will be denied.","title":"Enforcement"},{"location":"overview/security/#acl-file","text":"Tools that accept an ACL file expect it to be a simple text file with one ACE on each line. A line may be marked as a comment by using a # as the first non-whitespace character on the line. For example: # ACL for my container # Owner can't touch data - just do admin-type things A::OWNER@:dtTaAo # My project's users can generate and access data A:G:my_great_project@:rw # Bob can use the data to generate a report A::bob@:r The permission bits and the ACEs themselves don't need to be in any specific order. However the order may be different when the resulting ACL is parsed and displayed by DAOS.","title":"ACL File"},{"location":"overview/security/#limitations","text":"The maximum size of the ACE list in a DAOS ACL internal data structure is 64KiB. To calculate the internal data size of an ACL, use the following formula for each ACE: The base size of an ACE is 256 Bytes. If the ACE principal is not one of the special principals: Add the length of the principal string + 1. If that value is not 64-Byte aligned, round up to the nearest 64-Byte boundary.","title":"Limitations"},{"location":"overview/storage/","text":"Storage Model \u00b6 Overview \u00b6 The figure below represents the fundamental abstractions of the DAOS storage model. A DAOS pool is a storage reservation distributed across a collection of targets . The actual space allocated to the pool on each target is called a pool shard . The total space allocated to a pool is decided at creation time. It can be expanded over time by resizing all the pool shards (within the limit of the storage capacity dedicated to each target), or by spanning more targets (adding more pool shards). A pool offers storage virtualization and is the unit of provisioning and isolation. DAOS pools cannot span across multiple systems. A pool can host multiple transactional object stores called DAOS containers . Each container is a private object address space, which can be modified transactionally and independently of the other containers stored in the same pool. A container is the unit of snapshot and data management. DAOS objects belonging to a container can be distributed across any target of the pool for both performance and resilience and can be accessed through different APIs to represent structured, semi-structured and unstructured data efficiently The table below shows the targeted level of scalability for each DAOS concept. DAOS Concept Scalability (Order of Magnitude) System 10 5 Servers (hundreds of thousands) and 10 2 Pools (hundreds) Server 10 1 Targets (tens) Pool 10 2 Containers (hundreds) Container 10 9 Objects (billions) DAOS Pool \u00b6 A pool is identified by a unique pool UUID and maintains target memberships in a persistent versioned list called the pool map . The membership is definitive and consistent, and membership changes are sequentially numbered. The pool map not only records the list of active targets, it also contains the storage topology in the form of a tree that is used to identify targets sharing common hardware components. For instance, the first level of the tree can represent targets sharing the same motherboard, and then the second level can represent all motherboards sharing the same rack and finally the third level can represent all racks in the same cage. This framework effectively represents hierarchical fault domains, which are then used to avoid placing redundant data on targets subject to correlated failures. At any point in time, new targets can be added to the pool map, and failed targets can be excluded. Moreover, the pool map is fully versioned, which effectively assigns a unique sequence to each modification of the map, particularly for failed node removal. A pool shard is a reservation of persistent memory optionally combined with a pre-allocated space on NVMe storage on a specific target. It has a fixed capacity and fails operations when full. Current space usage can be queried at any time and reports the total amount of bytes used by any data type stored in the pool shard. Upon target failure and exclusion from the pool map, data redundancy inside the pool is automatically restored online. This self-healing process is known as rebuild . Rebuild progress is recorded regularly in special logs in the pool stored in persistent memory to address cascading failures. When new targets are added, data is automatically migrated to the newly added targets to redistribute space usage equally among all the members. This process is known as space rebalancing and uses dedicated persistent logs as well to support interruption and restart. A pool is a set of targets spread across different storage nodes over which data and metadata are distributed to achieve horizontal scalability, and replicated or erasure-coded to ensure durability and availability. When creating a pool, a set of system properties must be defined to configure the different features supported by the pool. Also, users can define their attributes that will be stored persistently. A pool is only accessible to authenticated and authorized applications. Multiple security frameworks could be supported, from NFSv4 access control lists to third party-based authentication (such as Kerberos). Security is enforced when connecting to the pool. Upon successful connection to the pool, a connection context is returned to the application process. As detailed previously, a pool stores many different sorts of persistent metadata, such as the pool map, authentication and authorization information, user attributes, properties, and rebuild logs. Such metadata is critical and requires the highest level of resiliency. Therefore, the pool metadata is replicated on a few nodes from distinct high-level fault domains. For very large configurations with hundreds of thousands of storage nodes, only a very small fraction of those nodes (in the order of tens) run the pool metadata service . With a limited number of storage nodes, DAOS can afford to rely on a consensus algorithm to reach agreement, to guarantee consistency in the presence of faults, and to avoid the split-brain syndrome. To access a pool, a user process should connect to this pool and pass the security checks. Once granted, a pool connection can be shared (via local2global() and global2local() operations) with any or all of its peer application processes (similar to the openg() POSIX extension). This collective connect mechanism helps to avoid metadata request storm when a massively distributed job is run on the datacenter. A pool connection is revoked when the original process that issued the connection request disconnects from the pool. DAOS Container \u00b6 A container represents an object address space inside a pool and is identified by a container UUID . The diagram below represents how the user (I/O middleware, domain-specific data format, big data or AI framework, ...) could use the container concept to store related datasets. Like pools, containers can store user attributes. A set of properties must be passed at container creation time to configure different features like checksums. To access a container, an application must first connect to the pool and then open the container. If the application is authorized to access the container, a container handle is returned. This includes capabilities that authorize any process in the application to access the container and its contents. The opening process may share this handle with any or all of its peers. Their capabilities are revoked on container close. Objects in a container may have different schemas for data distribution and redundancy over targets. Dynamic or static striping, replication, or erasure code are some parameters required to define the object schema. The object class defines common schema attributes for a set of objects. Each object class is assigned a unique identifier and is associated with a given schema at the pool level. A new object class can be defined at any time with a configurable schema, which is then immutable after creation (or at least until all objects belonging to the class have been destroyed). For convenience, several object classes that are expected to be the most commonly used will be predefined by default when the pool is created, as shown in the table below. Sample of Pre-defined Object Classes Object Class (RW = read/write, RM = read-mostly Redundancy Layout (SC = stripe count, RC = replica count, PC = parity count, TGT = target Small size & RW Replication static SCxRC, e.g. 1x4 Small size & RM Erasure code static SC+PC, e.g. 4+2 Large size & RW Replication static SCxRC over max #targets) Large size & RM Erasure code static SCx(SC+PC) w/ max #TGT) Unknown size & RW Replication SCxRC, e.g. 1x4 initially and grows Unknown size & RM Erasure code SC+PC, e.g. 4+2 initially and grows As shown below, each object is identified in the container by a unique 128-bit object address . The high 32 bits of the object address is reserved for DAOS to encode internal metadata such as the object class. The remaining 96 bits are managed by the user and should be unique inside the container. Those bits can be used by upper layers of the stack to encode their metadata, as long as unicity is guaranteed. A per-container 64-bit scalable object ID allocator is provided in the DAOS API. The object ID to be stored by the application is the full 128-bit address, which is for single use only and can be associated with only a single object schema. DAOS Object ID Structure <---------------------------------- 128 bits ----------------------------------> -------------------------------------------------------------------------------- |DAOS Internal Bits| Unique User Bits | -------------------------------------------------------------------------------- <---- 32 bits ----><------------------------- 96 bits -------------------------> A container is the basic unit of transaction and versioning. All object operations are implicitly tagged by the DAOS library with a timestamp called an epoch . The DAOS transaction API allows to combine multiple object updates into a single atomic transaction, with multi-version concurrency control based on epoch ordering. All the versioned updates may be periodically aggregated , to reclaim space utilized by overlapping writes and to reduce metadata complexity. A snapshot is a permanent reference that can be placed on a specific epoch to prevent aggregation. Container metadata (list of snapshots, container open handles, object class, user attributes, properties, and others) are stored in persistent memory and maintained by a dedicated container metadata service that either uses the same replicated engine as the parent metadata pool service, or has its own engine. This is configurable when creating a container. Like a pool, access to a container is controlled by the container handle. To acquire a valid handle, an application process must open the container and pass the security checks. This container handle may then be shared with other peer application processes via the container local2global() and global2local() operations. DAOS Object \u00b6 To avoid scaling problems and overhead common to a traditional storage system, DAOS objects are intentionally simple. No default object metadata beyond the type and schema is provided. This means that the system does not maintain time, size, owner, permissions or even track openers. To achieve high availability and horizontal scalability, many object schemas (replication/erasure code, static/dynamic striping, and others) are provided. The schema framework is flexible and easily expandable to allow for new custom schema types in the future. The layout is generated algorithmically on object open from the object identifier and the pool map. End-to-end integrity is assured by protecting object data with checksums during network transfer and storage. A DAOS object can be accessed through different APIs: Multi-level key-array API is the native object interface with locality feature. The key is split into a distribution (dkey) and an attribute (akey) key. Both the dkey and akey can be of variable length and type (a string, an integer or even a complex data structure). All entries under the same dkey are guaranteed to be collocated on the same target. The value associated with akey can be either a single variable-length value that cannot be partially overwritten, or an array of fixed-length values. Both the akeys and dkeys support enumeration. Key-value API provides a simple key and variable-length value interface. It supports the traditional put, get, remove and list operations. Array API implements a one-dimensional array of fixed-size elements addressed by a 64-bit offset. A DAOS array supports arbitrary extent read, write and punch operations.","title":"Storage Model"},{"location":"overview/storage/#storage-model","text":"","title":"Storage Model"},{"location":"overview/storage/#overview","text":"The figure below represents the fundamental abstractions of the DAOS storage model. A DAOS pool is a storage reservation distributed across a collection of targets . The actual space allocated to the pool on each target is called a pool shard . The total space allocated to a pool is decided at creation time. It can be expanded over time by resizing all the pool shards (within the limit of the storage capacity dedicated to each target), or by spanning more targets (adding more pool shards). A pool offers storage virtualization and is the unit of provisioning and isolation. DAOS pools cannot span across multiple systems. A pool can host multiple transactional object stores called DAOS containers . Each container is a private object address space, which can be modified transactionally and independently of the other containers stored in the same pool. A container is the unit of snapshot and data management. DAOS objects belonging to a container can be distributed across any target of the pool for both performance and resilience and can be accessed through different APIs to represent structured, semi-structured and unstructured data efficiently The table below shows the targeted level of scalability for each DAOS concept. DAOS Concept Scalability (Order of Magnitude) System 10 5 Servers (hundreds of thousands) and 10 2 Pools (hundreds) Server 10 1 Targets (tens) Pool 10 2 Containers (hundreds) Container 10 9 Objects (billions)","title":"Overview"},{"location":"overview/storage/#daos-pool","text":"A pool is identified by a unique pool UUID and maintains target memberships in a persistent versioned list called the pool map . The membership is definitive and consistent, and membership changes are sequentially numbered. The pool map not only records the list of active targets, it also contains the storage topology in the form of a tree that is used to identify targets sharing common hardware components. For instance, the first level of the tree can represent targets sharing the same motherboard, and then the second level can represent all motherboards sharing the same rack and finally the third level can represent all racks in the same cage. This framework effectively represents hierarchical fault domains, which are then used to avoid placing redundant data on targets subject to correlated failures. At any point in time, new targets can be added to the pool map, and failed targets can be excluded. Moreover, the pool map is fully versioned, which effectively assigns a unique sequence to each modification of the map, particularly for failed node removal. A pool shard is a reservation of persistent memory optionally combined with a pre-allocated space on NVMe storage on a specific target. It has a fixed capacity and fails operations when full. Current space usage can be queried at any time and reports the total amount of bytes used by any data type stored in the pool shard. Upon target failure and exclusion from the pool map, data redundancy inside the pool is automatically restored online. This self-healing process is known as rebuild . Rebuild progress is recorded regularly in special logs in the pool stored in persistent memory to address cascading failures. When new targets are added, data is automatically migrated to the newly added targets to redistribute space usage equally among all the members. This process is known as space rebalancing and uses dedicated persistent logs as well to support interruption and restart. A pool is a set of targets spread across different storage nodes over which data and metadata are distributed to achieve horizontal scalability, and replicated or erasure-coded to ensure durability and availability. When creating a pool, a set of system properties must be defined to configure the different features supported by the pool. Also, users can define their attributes that will be stored persistently. A pool is only accessible to authenticated and authorized applications. Multiple security frameworks could be supported, from NFSv4 access control lists to third party-based authentication (such as Kerberos). Security is enforced when connecting to the pool. Upon successful connection to the pool, a connection context is returned to the application process. As detailed previously, a pool stores many different sorts of persistent metadata, such as the pool map, authentication and authorization information, user attributes, properties, and rebuild logs. Such metadata is critical and requires the highest level of resiliency. Therefore, the pool metadata is replicated on a few nodes from distinct high-level fault domains. For very large configurations with hundreds of thousands of storage nodes, only a very small fraction of those nodes (in the order of tens) run the pool metadata service . With a limited number of storage nodes, DAOS can afford to rely on a consensus algorithm to reach agreement, to guarantee consistency in the presence of faults, and to avoid the split-brain syndrome. To access a pool, a user process should connect to this pool and pass the security checks. Once granted, a pool connection can be shared (via local2global() and global2local() operations) with any or all of its peer application processes (similar to the openg() POSIX extension). This collective connect mechanism helps to avoid metadata request storm when a massively distributed job is run on the datacenter. A pool connection is revoked when the original process that issued the connection request disconnects from the pool.","title":"DAOS Pool"},{"location":"overview/storage/#daos-container","text":"A container represents an object address space inside a pool and is identified by a container UUID . The diagram below represents how the user (I/O middleware, domain-specific data format, big data or AI framework, ...) could use the container concept to store related datasets. Like pools, containers can store user attributes. A set of properties must be passed at container creation time to configure different features like checksums. To access a container, an application must first connect to the pool and then open the container. If the application is authorized to access the container, a container handle is returned. This includes capabilities that authorize any process in the application to access the container and its contents. The opening process may share this handle with any or all of its peers. Their capabilities are revoked on container close. Objects in a container may have different schemas for data distribution and redundancy over targets. Dynamic or static striping, replication, or erasure code are some parameters required to define the object schema. The object class defines common schema attributes for a set of objects. Each object class is assigned a unique identifier and is associated with a given schema at the pool level. A new object class can be defined at any time with a configurable schema, which is then immutable after creation (or at least until all objects belonging to the class have been destroyed). For convenience, several object classes that are expected to be the most commonly used will be predefined by default when the pool is created, as shown in the table below. Sample of Pre-defined Object Classes Object Class (RW = read/write, RM = read-mostly Redundancy Layout (SC = stripe count, RC = replica count, PC = parity count, TGT = target Small size & RW Replication static SCxRC, e.g. 1x4 Small size & RM Erasure code static SC+PC, e.g. 4+2 Large size & RW Replication static SCxRC over max #targets) Large size & RM Erasure code static SCx(SC+PC) w/ max #TGT) Unknown size & RW Replication SCxRC, e.g. 1x4 initially and grows Unknown size & RM Erasure code SC+PC, e.g. 4+2 initially and grows As shown below, each object is identified in the container by a unique 128-bit object address . The high 32 bits of the object address is reserved for DAOS to encode internal metadata such as the object class. The remaining 96 bits are managed by the user and should be unique inside the container. Those bits can be used by upper layers of the stack to encode their metadata, as long as unicity is guaranteed. A per-container 64-bit scalable object ID allocator is provided in the DAOS API. The object ID to be stored by the application is the full 128-bit address, which is for single use only and can be associated with only a single object schema. DAOS Object ID Structure <---------------------------------- 128 bits ----------------------------------> -------------------------------------------------------------------------------- |DAOS Internal Bits| Unique User Bits | -------------------------------------------------------------------------------- <---- 32 bits ----><------------------------- 96 bits -------------------------> A container is the basic unit of transaction and versioning. All object operations are implicitly tagged by the DAOS library with a timestamp called an epoch . The DAOS transaction API allows to combine multiple object updates into a single atomic transaction, with multi-version concurrency control based on epoch ordering. All the versioned updates may be periodically aggregated , to reclaim space utilized by overlapping writes and to reduce metadata complexity. A snapshot is a permanent reference that can be placed on a specific epoch to prevent aggregation. Container metadata (list of snapshots, container open handles, object class, user attributes, properties, and others) are stored in persistent memory and maintained by a dedicated container metadata service that either uses the same replicated engine as the parent metadata pool service, or has its own engine. This is configurable when creating a container. Like a pool, access to a container is controlled by the container handle. To acquire a valid handle, an application process must open the container and pass the security checks. This container handle may then be shared with other peer application processes via the container local2global() and global2local() operations.","title":"DAOS Container"},{"location":"overview/storage/#daos-object","text":"To avoid scaling problems and overhead common to a traditional storage system, DAOS objects are intentionally simple. No default object metadata beyond the type and schema is provided. This means that the system does not maintain time, size, owner, permissions or even track openers. To achieve high availability and horizontal scalability, many object schemas (replication/erasure code, static/dynamic striping, and others) are provided. The schema framework is flexible and easily expandable to allow for new custom schema types in the future. The layout is generated algorithmically on object open from the object identifier and the pool map. End-to-end integrity is assured by protecting object data with checksums during network transfer and storage. A DAOS object can be accessed through different APIs: Multi-level key-array API is the native object interface with locality feature. The key is split into a distribution (dkey) and an attribute (akey) key. Both the dkey and akey can be of variable length and type (a string, an integer or even a complex data structure). All entries under the same dkey are guaranteed to be collocated on the same target. The value associated with akey can be either a single variable-length value that cannot be partially overwritten, or an array of fixed-length values. Both the akeys and dkeys support enumeration. Key-value API provides a simple key and variable-length value interface. It supports the traditional put, get, remove and list operations. Array API implements a one-dimensional array of fixed-size elements addressed by a 64-bit offset. A DAOS array supports arbitrary extent read, write and punch operations.","title":"DAOS Object"},{"location":"overview/terminology/","text":"Terminology \u00b6 Acronym Expansion ABT Argobots ACL Access Control List ACE Access Control Entry ACID Atomicity, consistency, isolation, durability BIO Blob I/O CART Collective and RPC Transport CGO Go tools that enable creation of Go packages that call C code CN Compute Node COTS Commercial off-the-shelf CPU Central Processing Unit Daemon A process offering system-level resources. DAOS Distributed Asynchronous Object Storage PMEM Intel Optane Persistent Memory DPDK Data Plane Development Kit dRPC DAOS Remote Procedure Call gRPC gRPC Remote Procedure Calls GURT A common library of Gurt Useful Routines and Types HLC Hybrid Logical Clock HLD High-level Design ISA-L Intel Storage Acceleration Library I/O Input/Output KV store Key-Value store libfabric Open Fabrics Interface Mercury A user-space RPC library that can use libfabrics as a transport MTBF Mean Time Between Failures NVM Non-Volatile Memory NVMe Non-Volatile Memory express OFI Open Fabrics Interface OS Operating System PM Persistent Memory PMDK Persistent Memory Devevelopment Kit RAFT Raft is a consensus algorithm used to distribute state transitions among DAOS server nodes. RAS Reliability, Availability & Serviceability RDB Replicated Database, containing pool metadata and maintained across DAOS servers using the Raft algorithm. RDMA/RMA Remote (Direct) Memory Access RDG Redundancy Group RPC Remote Procedure Call SCM Storage-Class Memory SWIM Scalable Weakly-consistent Infection-style process group Membership SPDK Storage Performance Development Kit SSD Solid State Drive SWIM Scalable Weakly-consistent Infection-style process group Membership protocol ULT User Level Thread UPI Intel Ultra Path Interconnect UUID Universal Unique Identifier VOS Versioning Object Store","title":"Terminology"},{"location":"overview/terminology/#terminology","text":"Acronym Expansion ABT Argobots ACL Access Control List ACE Access Control Entry ACID Atomicity, consistency, isolation, durability BIO Blob I/O CART Collective and RPC Transport CGO Go tools that enable creation of Go packages that call C code CN Compute Node COTS Commercial off-the-shelf CPU Central Processing Unit Daemon A process offering system-level resources. DAOS Distributed Asynchronous Object Storage PMEM Intel Optane Persistent Memory DPDK Data Plane Development Kit dRPC DAOS Remote Procedure Call gRPC gRPC Remote Procedure Calls GURT A common library of Gurt Useful Routines and Types HLC Hybrid Logical Clock HLD High-level Design ISA-L Intel Storage Acceleration Library I/O Input/Output KV store Key-Value store libfabric Open Fabrics Interface Mercury A user-space RPC library that can use libfabrics as a transport MTBF Mean Time Between Failures NVM Non-Volatile Memory NVMe Non-Volatile Memory express OFI Open Fabrics Interface OS Operating System PM Persistent Memory PMDK Persistent Memory Devevelopment Kit RAFT Raft is a consensus algorithm used to distribute state transitions among DAOS server nodes. RAS Reliability, Availability & Serviceability RDB Replicated Database, containing pool metadata and maintained across DAOS servers using the Raft algorithm. RDMA/RMA Remote (Direct) Memory Access RDG Redundancy Group RPC Remote Procedure Call SCM Storage-Class Memory SWIM Scalable Weakly-consistent Infection-style process group Membership SPDK Storage Performance Development Kit SSD Solid State Drive SWIM Scalable Weakly-consistent Infection-style process group Membership protocol ULT User Level Thread UPI Intel Ultra Path Interconnect UUID Universal Unique Identifier VOS Versioning Object Store","title":"Terminology"},{"location":"overview/transaction/","text":"Transaction Model \u00b6 The DAOS API supports distributed transactions that allow any update operations against objects belonging to the same container to be combined into a single ACID transaction. Distributed consistency is provided via a lockless optimistic concurrency control mechanism based on multi-version timestamp ordering. DAOS transactions are serializable and can be used on an ad-hoc basis for parts of the datasets that need it. The DAOS versioning mechanism allows creating persistent container snapshots which provide point-in-time distributed consistent views of a container which can be used to build producer-consumer pipeline. Epoch and Timestamp Ordering \u00b6 Each DAOS I/O operation is tagged with a timestamp called epoch . An epoch is a 64-bit integer that integrates both logical and physical clocks (see HLC paper ). The DAOS API provides helper functions to convert an epoch to traditional POSIX time (i.e., struct timespec , see clock_gettime(3) ). Container Snapshot \u00b6 As shown in the figure below, the content of a container can be snapshot at any time. DAOS snapshots are very lightweight and are tagged with the epoch associated with the time when the snapshot was created. Once successfully created, a snapshot remains readable until it is explicitly destroyed. The content of a container can be rolled back to a particular snapshot. The container snapshot feature allows supporting native producer/consumer pipelines as represented in the diagram below. The producer will generate a snapshot once a consistent version of the dataset has been successfully written. The consumer applications may subscribe to container snapshot events, so that new updates can be processed as the producer commits them. The immutability of the snapshots guarantees that the consumer sees consistent data, even while the producer continues with new updates. Both the producer and consumer indeed operate on different versions of the container and do not need any serialization. Once the producer generates a new version of the dataset, the consumer may query the differences between the two snapshots and process only the incremental changes. Distributed Transactions \u00b6 Unlike POSIX, the DAOS API does not impose any worst-case concurrency control mechanism to address conflicting I/O operations. Instead, individual I/O operations are tagged with a different epoch and applied in epoch order, regardless of execution order. This baseline model delivers the maximum scalability and performance to data models and applications that do not generate conflicting I/O workload. Typical examples are collective MPI-IO operations, POSIX file read/write or HDF5 dataset read/write. For parts of the data model that require conflict serialization, DAOS provides distributed serializable transaction based on multi-version concurrency control. Transactions are typically needed when different user processes can overwrite the value associated with a dkey/akey pair. Examples are a SQL database over DAOS or a consistent POSIX namespace accessed concurrently by uncoordinated clients. All I/O operations (including reads) submitted in the context of the same operation will use the same epoch. The DAOS transaction mechanism automatically detects the traditional read/write, write/read and write/write conflicts and aborts one of the conflicting transactions (the transaction fails to commit with -DER_RESTART ). The failed transaction then has to be restarted by the user/application. In the initial implementation, the transaction API has the following limitations that will be addressed in future DAOS versions: no support for the array API transactional object update and key-value put operations are not visible via object fetch/list and key-value get/list operations executed in the context of the same transaction.","title":"Transaction Model"},{"location":"overview/transaction/#transaction-model","text":"The DAOS API supports distributed transactions that allow any update operations against objects belonging to the same container to be combined into a single ACID transaction. Distributed consistency is provided via a lockless optimistic concurrency control mechanism based on multi-version timestamp ordering. DAOS transactions are serializable and can be used on an ad-hoc basis for parts of the datasets that need it. The DAOS versioning mechanism allows creating persistent container snapshots which provide point-in-time distributed consistent views of a container which can be used to build producer-consumer pipeline.","title":"Transaction Model"},{"location":"overview/transaction/#epoch-and-timestamp-ordering","text":"Each DAOS I/O operation is tagged with a timestamp called epoch . An epoch is a 64-bit integer that integrates both logical and physical clocks (see HLC paper ). The DAOS API provides helper functions to convert an epoch to traditional POSIX time (i.e., struct timespec , see clock_gettime(3) ).","title":"Epoch and Timestamp Ordering"},{"location":"overview/transaction/#container-snapshot","text":"As shown in the figure below, the content of a container can be snapshot at any time. DAOS snapshots are very lightweight and are tagged with the epoch associated with the time when the snapshot was created. Once successfully created, a snapshot remains readable until it is explicitly destroyed. The content of a container can be rolled back to a particular snapshot. The container snapshot feature allows supporting native producer/consumer pipelines as represented in the diagram below. The producer will generate a snapshot once a consistent version of the dataset has been successfully written. The consumer applications may subscribe to container snapshot events, so that new updates can be processed as the producer commits them. The immutability of the snapshots guarantees that the consumer sees consistent data, even while the producer continues with new updates. Both the producer and consumer indeed operate on different versions of the container and do not need any serialization. Once the producer generates a new version of the dataset, the consumer may query the differences between the two snapshots and process only the incremental changes.","title":"Container Snapshot"},{"location":"overview/transaction/#distributed-transactions","text":"Unlike POSIX, the DAOS API does not impose any worst-case concurrency control mechanism to address conflicting I/O operations. Instead, individual I/O operations are tagged with a different epoch and applied in epoch order, regardless of execution order. This baseline model delivers the maximum scalability and performance to data models and applications that do not generate conflicting I/O workload. Typical examples are collective MPI-IO operations, POSIX file read/write or HDF5 dataset read/write. For parts of the data model that require conflict serialization, DAOS provides distributed serializable transaction based on multi-version concurrency control. Transactions are typically needed when different user processes can overwrite the value associated with a dkey/akey pair. Examples are a SQL database over DAOS or a consistent POSIX namespace accessed concurrently by uncoordinated clients. All I/O operations (including reads) submitted in the context of the same operation will use the same epoch. The DAOS transaction mechanism automatically detects the traditional read/write, write/read and write/write conflicts and aborts one of the conflicting transactions (the transaction fails to commit with -DER_RESTART ). The failed transaction then has to be restarted by the user/application. In the initial implementation, the transaction API has the following limitations that will be addressed in future DAOS versions: no support for the array API transactional object update and key-value put operations are not visible via object fetch/list and key-value get/list operations executed in the context of the same transaction.","title":"Distributed Transactions"},{"location":"overview/use_cases/","text":"Use Cases \u00b6 This section provides a non-exhaustive list of use cases presenting how the DAOS storage model and stack could be used on a real HPC cluster. This document contains the following sections: Storage Management and Workflow Integration Workflow Execution Bulk Synchronous Checkpoint Producer/Consumer Concurrent Producers Storage Node Failure and Resilvering Storage Management & Workflow Integration \u00b6 In this section, we consider two different cluster configurations: Cluster A: All or a majority of the compute nodes have local persistent memory. In other words, each compute node is also a storage node. Cluster B: Storage nodes are dedicated to storage and disseminated across the fabric. They are not used for computation and thus do not run any application code. At boot time, each storage node starts the DAOS server that instantiates service threads. In cluster A, the DAOS threads are bound to the noisy cores and interact with the FWK if mOS is used. In cluster B, the DAOS server can use all the cores of the storage node. The DAOS server then loads the storage management module. This module scans for local storage on the node and reports the result to a designated master DAOS server that aggregates information about the used and available storage across the cluster. The management module also retrieves the fault domain hierarchy (from a database or specific service) and integrates this with the storage information. The resource manager then uses the DAOS management API to query available storage and allocate a certain amount of storage (i.e. persistent memory) for a new workflow that is to be scheduled. In cluster A, this allocation request may list the compute nodes where the workflow is supposed to run, whereas in case B, it may ask for storage nearby some allocated compute nodes. Once successfully allocated, the master server will initialize a DAOS pool covering the allocated storage by formatting the VOS layout (i.e. fallocate(1) a PMEM file & create VOS super block) and starting the pool service which will initiate the Raft engine in charge of the pool membership and metadata. At this point, the DAOS pool is ready to be handed off to the actual workflow. When the workflow starts, one rank connects to the DAOS pool, then uses local2global() to generate a global connection handle and shares it with all the other application ranks that use global2local() to create a local connection handle. At that point, new containers can be created and existing ones opened collectively or individually by the application tasks. Workflow Execution \u00b6 We consider the workflow represented in the figure below. Each green box represents a different container. All containers are stored in the same DAOS pool represented by the gray box. The simulation reads data from the input container and writes raw timesteps to another container. It also regularly dumps checkpoints to a dedicated ckpt container. The down-sample job reads the raw timesteps and generates sampled timesteps to be analyzed by the post-process which stores analysis data into yet another container. Bulk Synchronous Checkpoint \u00b6 Defensive I/O is used to manage a large simulation run over a period of time larger than the platform's mean time between failure (MTBF). The simulation regularly dumps the current computation state to a dedicated container used to guarantee forward progress in the event of failures. This section elaborates on how checkponting could be implemented on top of the DAOS storage stack. We first consider the traditional approach relying on blocking barriers and then a more loosely coupled execution. Blocking Barrier When the simulation job starts, one task opens the checkpoint container and fetches the current global HCE. It thens obtains an epoch hold and shares the data (the container handle, the current LHE and global HCE) with peer tasks. Each task checks for the latest computation state saved to the checkpoint container by reading with an epoch equal to the global HCE and resumes computation from where it was last checkpointed. To checkpoint, each task executes a barrier to synchronize with the other tasks, writes its current computation state to the checkpoint container at epoch LHE, flushes all updates and finally executes another barrier. Once all tasks have completed the last barrier, one designated task (e.g. rank 0) commits the LHE which is then increased by one on successful commit. This process is repeated regularly until the simulation successfully completes. Non-blocking Barrier We now consider another approach to checkpointing where the execution is more loosely coupled. As in the previous case, one task is responsible for opening the checkpoint container, fetching the global HCE, obtaining an epoch hold and sharing the data with the other peer tasks. However, tasks can now checkpoint their computation state at their own pace without waiting for each other. After the computation of N timesteps, each task dumps its state to the checkpoint container at epoch LHE+1, flushes the changes and calls a non-blocking barrier (e.g. MPI_Ibarrier()) once done. Then after another N timesteps, the new checkpoint is written with epoch LHE+2 and so on. For each checkpoint, the epoch number is incremented. Moreover, each task regularly calls MPI_Test() to check for barrier completion which allows them to recycle the MPI_Request. Upon barrier completion, one designated task (typically rank 0) also commits the associated epoch number. All epochs are guaranteed to be committed in sequence and each committed epoch is a new consistent checkpoint to restart from. On failure, checkpointed states that have been written by individual tasks, but not committed, are automatically rolled back. Producer/Consumer \u00b6 In the previous figure , we have two examples of producer/consumer. The down-sample job consumes raw timesteps generated by the simulation job and produces sampled timesteps analyzed by the post-process job. The DAOS stack provides specific mechanisms for producer/consumer workflow which even allows the consumer to dumps the result of its analysis into the same container as the producer. Private Container The down-sample job opens the sampled timesteps container, fetches the current global HCE, obtains an epoch hold and writes new sampled data to this container at epoch LHE. While this is occurring, the post process job opens the container storing analyzed data for write, checks for the latest analyzed timesteps and obtains an epoch hold on this container. It then opens the sampled timesteps container for read, and checks whether the next time-step to be consumed is ready. If not, it waits for a new global HCE to be committed (notified by asynchronous event completion on the event queue) and checks again. When the requested time-step is available, the down-sample job processes input data for this new time-step, dumps the results in its own container and updates the latest analyzed time-step in its metadata. It then commits updates to its output container and waits again for a new epoch to be committed and repeats the same process. Another approach is for the producer job to create explicit snapshots for epochs of interest and have the analysis job waiting and processing snapshots. This avoid processing every single committed epoch. Shared Container We now assume that the container storing the sampled timesteps and the one storing the analyzed data are a single container. In other words, the down-sample job consumes input data and writes output data to the same container. The down-sample job opens the shared container, obtains an hold and dumps new sampled timesteps to the container. As before, the post-process job also opens the container, fetches the latest analyzed timestep, but does not obtain an epoch hold until a new global HCE is ready. Once the post-process job is notified of a new global HCE, it can analyze the new sampled timesteps, obtain an hold and write its analyzed data to the same container. Once this is done, the post-process job flushes its updates, commits the held epoch and releases the held epoch. At that point, it can wait again for a new global HCE to be generated by the down-sample job. Concurrent Producers \u00b6 In the previous section, we consider a producer and a consumer job concurrently reading and writing into the same container, but in disjoint objects. We now consider a workflow composed of concurrent producer jobs modifying the same container in a conflicting and uncoordinated manner. This effectively means that the two producers can update the same key of the same KV object or document store or overlapping extents of the same byte array. This model requires the implementation of a concurrency-control mechanism (not part of DAOS) to coordinate conflicting accesses. This section presents an example of such a mechanism based on locking, but alternative approaches can also be considered. A workflow is composed of two applications using a distributed lock manager to serialize contended accesses to DAOS objects. Each application individually opens the same container and grabs an epoch hold whenever it wants to modify some objects in the container. Prior to modifying an object, an application should acquire a write lock on the object. This lock carries a lock value block (LVB) storing the last epoch number in which this object was last modified and committed. Once the lock is acquired, the writer must: read from an epoch equal to the greatest of the epoch specified in the LVB and the handle LRE. submit new writes with an epoch higher than the one in the LVB and the currently held epoch. After all the I/O operations have been completed, flushed, and committed by the application, the LVB is updated with the committed epoch in which the object was modified, and the lock can finally be released. Storage Node Failure and Resilvering \u00b6 In this section, we consider a workflow connected to a DAOS pool and one storage node that suddenly fails. Both DAOS clients and servers communicating with the failed server experience RPC timeouts and inform the RAS system. Failing RPCs are resent repeatedly until the RAS system or the pool metadata service itself decides to declare the storage node dead and evicts it from the pool map. The pool map update, along with the new version, is propagated to all the storage nodes that lazily (in RPC replies) inform clients that a new pool map version is available. Both clients and servers are thus eventually informed of the failure and enter into recovery mode. Server nodes will cooperate to restore redundancy on different servers for the impacted objects, whereas clients will enter in degraded mode and read from other replicas, or reconstruct data from erasure code. This rebuild process is executed online while the container is still being accessed and modified. Once redundancy has been restored for all objects, the poolmap is updated again to inform everyone that the system has recovered from the fault and the system can exit from degraded mode.","title":"Use Cases"},{"location":"overview/use_cases/#use-cases","text":"This section provides a non-exhaustive list of use cases presenting how the DAOS storage model and stack could be used on a real HPC cluster. This document contains the following sections: Storage Management and Workflow Integration Workflow Execution Bulk Synchronous Checkpoint Producer/Consumer Concurrent Producers Storage Node Failure and Resilvering","title":"Use Cases"},{"location":"overview/use_cases/#storage-management-workflow-integration","text":"In this section, we consider two different cluster configurations: Cluster A: All or a majority of the compute nodes have local persistent memory. In other words, each compute node is also a storage node. Cluster B: Storage nodes are dedicated to storage and disseminated across the fabric. They are not used for computation and thus do not run any application code. At boot time, each storage node starts the DAOS server that instantiates service threads. In cluster A, the DAOS threads are bound to the noisy cores and interact with the FWK if mOS is used. In cluster B, the DAOS server can use all the cores of the storage node. The DAOS server then loads the storage management module. This module scans for local storage on the node and reports the result to a designated master DAOS server that aggregates information about the used and available storage across the cluster. The management module also retrieves the fault domain hierarchy (from a database or specific service) and integrates this with the storage information. The resource manager then uses the DAOS management API to query available storage and allocate a certain amount of storage (i.e. persistent memory) for a new workflow that is to be scheduled. In cluster A, this allocation request may list the compute nodes where the workflow is supposed to run, whereas in case B, it may ask for storage nearby some allocated compute nodes. Once successfully allocated, the master server will initialize a DAOS pool covering the allocated storage by formatting the VOS layout (i.e. fallocate(1) a PMEM file & create VOS super block) and starting the pool service which will initiate the Raft engine in charge of the pool membership and metadata. At this point, the DAOS pool is ready to be handed off to the actual workflow. When the workflow starts, one rank connects to the DAOS pool, then uses local2global() to generate a global connection handle and shares it with all the other application ranks that use global2local() to create a local connection handle. At that point, new containers can be created and existing ones opened collectively or individually by the application tasks.","title":"Storage Management &amp; Workflow Integration"},{"location":"overview/use_cases/#workflow-execution","text":"We consider the workflow represented in the figure below. Each green box represents a different container. All containers are stored in the same DAOS pool represented by the gray box. The simulation reads data from the input container and writes raw timesteps to another container. It also regularly dumps checkpoints to a dedicated ckpt container. The down-sample job reads the raw timesteps and generates sampled timesteps to be analyzed by the post-process which stores analysis data into yet another container.","title":"Workflow Execution"},{"location":"overview/use_cases/#bulk-synchronous-checkpoint","text":"Defensive I/O is used to manage a large simulation run over a period of time larger than the platform's mean time between failure (MTBF). The simulation regularly dumps the current computation state to a dedicated container used to guarantee forward progress in the event of failures. This section elaborates on how checkponting could be implemented on top of the DAOS storage stack. We first consider the traditional approach relying on blocking barriers and then a more loosely coupled execution. Blocking Barrier When the simulation job starts, one task opens the checkpoint container and fetches the current global HCE. It thens obtains an epoch hold and shares the data (the container handle, the current LHE and global HCE) with peer tasks. Each task checks for the latest computation state saved to the checkpoint container by reading with an epoch equal to the global HCE and resumes computation from where it was last checkpointed. To checkpoint, each task executes a barrier to synchronize with the other tasks, writes its current computation state to the checkpoint container at epoch LHE, flushes all updates and finally executes another barrier. Once all tasks have completed the last barrier, one designated task (e.g. rank 0) commits the LHE which is then increased by one on successful commit. This process is repeated regularly until the simulation successfully completes. Non-blocking Barrier We now consider another approach to checkpointing where the execution is more loosely coupled. As in the previous case, one task is responsible for opening the checkpoint container, fetching the global HCE, obtaining an epoch hold and sharing the data with the other peer tasks. However, tasks can now checkpoint their computation state at their own pace without waiting for each other. After the computation of N timesteps, each task dumps its state to the checkpoint container at epoch LHE+1, flushes the changes and calls a non-blocking barrier (e.g. MPI_Ibarrier()) once done. Then after another N timesteps, the new checkpoint is written with epoch LHE+2 and so on. For each checkpoint, the epoch number is incremented. Moreover, each task regularly calls MPI_Test() to check for barrier completion which allows them to recycle the MPI_Request. Upon barrier completion, one designated task (typically rank 0) also commits the associated epoch number. All epochs are guaranteed to be committed in sequence and each committed epoch is a new consistent checkpoint to restart from. On failure, checkpointed states that have been written by individual tasks, but not committed, are automatically rolled back.","title":"Bulk Synchronous Checkpoint"},{"location":"overview/use_cases/#producerconsumer","text":"In the previous figure , we have two examples of producer/consumer. The down-sample job consumes raw timesteps generated by the simulation job and produces sampled timesteps analyzed by the post-process job. The DAOS stack provides specific mechanisms for producer/consumer workflow which even allows the consumer to dumps the result of its analysis into the same container as the producer. Private Container The down-sample job opens the sampled timesteps container, fetches the current global HCE, obtains an epoch hold and writes new sampled data to this container at epoch LHE. While this is occurring, the post process job opens the container storing analyzed data for write, checks for the latest analyzed timesteps and obtains an epoch hold on this container. It then opens the sampled timesteps container for read, and checks whether the next time-step to be consumed is ready. If not, it waits for a new global HCE to be committed (notified by asynchronous event completion on the event queue) and checks again. When the requested time-step is available, the down-sample job processes input data for this new time-step, dumps the results in its own container and updates the latest analyzed time-step in its metadata. It then commits updates to its output container and waits again for a new epoch to be committed and repeats the same process. Another approach is for the producer job to create explicit snapshots for epochs of interest and have the analysis job waiting and processing snapshots. This avoid processing every single committed epoch. Shared Container We now assume that the container storing the sampled timesteps and the one storing the analyzed data are a single container. In other words, the down-sample job consumes input data and writes output data to the same container. The down-sample job opens the shared container, obtains an hold and dumps new sampled timesteps to the container. As before, the post-process job also opens the container, fetches the latest analyzed timestep, but does not obtain an epoch hold until a new global HCE is ready. Once the post-process job is notified of a new global HCE, it can analyze the new sampled timesteps, obtain an hold and write its analyzed data to the same container. Once this is done, the post-process job flushes its updates, commits the held epoch and releases the held epoch. At that point, it can wait again for a new global HCE to be generated by the down-sample job.","title":"Producer/Consumer"},{"location":"overview/use_cases/#concurrent-producers","text":"In the previous section, we consider a producer and a consumer job concurrently reading and writing into the same container, but in disjoint objects. We now consider a workflow composed of concurrent producer jobs modifying the same container in a conflicting and uncoordinated manner. This effectively means that the two producers can update the same key of the same KV object or document store or overlapping extents of the same byte array. This model requires the implementation of a concurrency-control mechanism (not part of DAOS) to coordinate conflicting accesses. This section presents an example of such a mechanism based on locking, but alternative approaches can also be considered. A workflow is composed of two applications using a distributed lock manager to serialize contended accesses to DAOS objects. Each application individually opens the same container and grabs an epoch hold whenever it wants to modify some objects in the container. Prior to modifying an object, an application should acquire a write lock on the object. This lock carries a lock value block (LVB) storing the last epoch number in which this object was last modified and committed. Once the lock is acquired, the writer must: read from an epoch equal to the greatest of the epoch specified in the LVB and the handle LRE. submit new writes with an epoch higher than the one in the LVB and the currently held epoch. After all the I/O operations have been completed, flushed, and committed by the application, the LVB is updated with the committed epoch in which the object was modified, and the lock can finally be released.","title":"Concurrent Producers"},{"location":"overview/use_cases/#storage-node-failure-and-resilvering","text":"In this section, we consider a workflow connected to a DAOS pool and one storage node that suddenly fails. Both DAOS clients and servers communicating with the failed server experience RPC timeouts and inform the RAS system. Failing RPCs are resent repeatedly until the RAS system or the pool metadata service itself decides to declare the storage node dead and evicts it from the pool map. The pool map update, along with the new version, is propagated to all the storage nodes that lazily (in RPC replies) inform clients that a new pool map version is available. Both clients and servers are thus eventually informed of the failure and enter into recovery mode. Server nodes will cooperate to restore redundancy on different servers for the impacted objects, whereas clients will enter in degraded mode and read from other replicas, or reconstruct data from erasure code. This rebuild process is executed online while the container is still being accessed and modified. Once redundancy has been restored for all objects, the poolmap is updated again to inform everyone that the system has recovered from the fault and the system can exit from degraded mode.","title":"Storage Node Failure and Resilvering"},{"location":"release/release_notes_v2_0/","text":"DAOS Version 2.0 Release Notes \u00b6 DAOS 2.0 is under active development and is planned for September 2021.","title":"Release Notes v2.0"},{"location":"release/release_notes_v2_0/#daos-version-20-release-notes","text":"DAOS 2.0 is under active development and is planned for September 2021.","title":"DAOS Version 2.0 Release Notes"},{"location":"release/support_matrix/","text":"Support Matrix \u00b6 Please find below the recommended Linux distribution and third-party software that have been tested with the different tags. Information for future tags/releases is indicative only and may change. Tag CentOS openSUSE Ubuntu MOFED v1.0.1 7.7 No No 5.0.1 v1.2 7.9 No 20.04 5.1.x v2.0 7.9/8.3 15.2 20.04 5.1.x DAOS is primarily validated on Intel x86_64 architecture. Some users have reported successful compilation and basic testing on ARM64 platforms.","title":"Support Matrix"},{"location":"release/support_matrix/#support-matrix","text":"Please find below the recommended Linux distribution and third-party software that have been tested with the different tags. Information for future tags/releases is indicative only and may change. Tag CentOS openSUSE Ubuntu MOFED v1.0.1 7.7 No No 5.0.1 v1.2 7.9 No 20.04 5.1.x v2.0 7.9/8.3 15.2 20.04 5.1.x DAOS is primarily validated on Intel x86_64 architecture. Some users have reported successful compilation and basic testing on ARM64 platforms.","title":"Support Matrix"},{"location":"user/container/","text":"Container Management \u00b6 DAOS containers are datasets managed by the users. A container is the unit of snapshot and has a type. It can be a POSIX namespace, an HDF5 file or any other new application-specific data model. The chapter explains how to manager container, while the subsequent ones detail how to access a DAOS container from applications. Container Creation/Destroy \u00b6 Containers can be created and destroyed through the daos(1) utility. provided to manage containers. To create and then query a container labeled mycont on a pool (labeled tank ): $ daos cont create tank --label mycont Container UUID : daefe12c-45d4-44f7-8e56-995d02549041 Container Label: mycont Container Type : unknown Successfully created container daefe12c-45d4-44f7-8e56-995d02549041 $ daos cont query tank mycont Container UUID : daefe12c-45d4-44f7-8e56-995d02549041 Container Label : mycont Container Type : unknown Pool UUID : 0d1fad71-5681-48d4-acdd-7bb2e786f12e Number of snapshots : 0 Latest Persistent Snapshot : 0 Highest Aggregated Epoch : 263546931609567249 Container redundancy factor: 0 Snapshot Epochs : The label can be up to 127 characters long and must only include alphanumeric characters, colon (':'), period ('.') or underscore ('_'). The container type (i.e., POSIX or HDF5) can be passed via the --type option. As shown below, the pool UUID, container UUID, and container attributes can be stored in the extended attributes of a POSIX file or directory for convenience. Then subsequent invocations of the daos tools need to reference the path to the POSIX file or directory. $ daos cont create tank --path /tmp/mycontainer --type=POSIX --oclass=SX Container UUID : 30e5d364-62c9-4ddf-9284-1021359455f2 Container Type : POSIX Successfully created container 30e5d364-62c9-4ddf-9284-1021359455f2 type POSIX $ daos cont query --path /tmp/mycontainer Container UUID : 30e5d364-62c9-4ddf-9284-1021359455f2 Container Type : POSIX Pool UUID : 0d1fad71-5681-48d4-acdd-7bb2e786f12e Number of snapshots : 0 Latest Persistent Snapshot : 0 Highest Aggregated Epoch : 263548861715283973 Container redundancy factor: 0 Snapshot Epochs : Object Class : SX Chunk Size : 1.0 MiB To list all containers available in a pool: $ daos cont list tank UUID Label ---- ----- 30e5d364-62c9-4ddf-9284-1021359455f2 container_label_not_set daefe12c-45d4-44f7-8e56-995d02549041 mycont To destroy a container: $ daos cont destroy tank mycont Successfully destroyed container mycont $ daos cont destroy --path /tmp/mycontainer Successfully destroyed container 30e5d364-62c9-4ddf-9284-1021359455f2 Container Properties \u00b6 Container properties are the main mechanism that one can use to control the behavior of container. This includes the type of middleware, whether some features like deduplication or checksum are enabled. Some properties are immutable after creation creation, while some others can be dynamically changed. Querying Properties \u00b6 The user-level administration daos utility may be used to query a container's properties. Refer to the manual page for full usage details. $ daos cont get-prop tank mycont # -OR- --path interface shown below $ daos cont get-prop --path=/tmp/mycontainer Properties for container mycont Name Value ---- ----- Highest Allocated OID 0 Checksum off Checksum Chunk Size 32 KiB Compression off Deduplication off Dedupe Threshold 4.0 KiB EC Cell Size 1.0 MiB Encryption off Group jlombard@ Label mycont Layout Type unknown (0) Layout Version 1 Max Snapshot 0 Owner jlombard@ Redundancy Factor rf0 Redundancy Level rank (1) Server Checksumming off Health HEALTHY Access Control List A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT Additionally, a container's properties may be retrieved using the libdaos API daos_cont_query() function. Refer to the file src/include/daos_cont.h Doxygen comments and the online documentation available here . Changing Properties \u00b6 By default, a container will inherit a set of default value for each property. Those can be overridden at container creation time via the --properties option. $ daos cont create tank --label mycont2 --properties cksum:sha1,dedup:hash,rf:1 Container UUID : a6286ead-1952-4faa-bf87-00fc0f3785aa Container Label: mycont2 Container Type : unknown Successfully created container a6286ead-1952-4faa-bf87-00fc0f3785aa $ daos cont query tank mycont2 Properties for container mycont2 Name Value ---- ----- Highest Allocated OID 0 Checksum sha1 Checksum Chunk Size 32 KiB Compression off Deduplication hash Dedupe Threshold 4.0 KiB EC Cell Size 1.0 MiB Encryption off Group jlombard@ Label mycont2 Layout Type unknown (0) Layout Version 1 Max Snapshot 0 Owner jlombard@ Redundancy Factor rf1 Redundancy Level rank (1) Server Checksumming off Health HEALTHY Access Control List A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT Mutable properties can be modified after container creation via the set-prop option. $ daos cont set-prop tank mycont2 --properties label:mycont3 Properties were successfully set This effectively changed the container label. $ daos cont get-prop tank mycont2 ERROR: daos: DER_NONEXIST(-1005): The specified entity does not exist $ daos cont get-prop tank mycont3 Properties for container mycont3 Name Value ---- ----- Highest Allocated OID 0 Checksum sha1 Checksum Chunk Size 32 KiB Compression off Deduplication hash Dedupe Threshold 4.0 KiB EC Cell Size 1.0 MiB Encryption off Group jlombard@ Label mycont3 Layout Type unknown (0) Layout Version 1 Max Snapshot 0 Owner jlombard@ Redundancy Factor rf1 Redundancy Level rank (1) Server Checksumming off Health HEALTHY Access Control List A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT Property Values \u00b6 The table below summarizes the different container properties available. Container Property Immutable Description label No String associate with a containers. e.g., \"Cat_Pics\" or \"ResNet50_training_data\". owner Yes User acting as the owner of the container. group Yes Group acting as the owner of the container layout_type Yes The container type (POSIX, HDF5, ...) layout_ver Yes A version of the layout that can be used by I/O middleware to handle interoperability. rf Yes The redundancy factor that drives the minimal data protection required for objects stored in the container. e.g., RF1 means no data protection, RF3 only allows 3-way replication or erasure code N+2. rf_lvl Yes The fault domain level to use for data redundancy placement. This is used to determine object placement. ec_cell Yes Erasure code cell size for erasure-coded objects. cksum Yes Checksum off, or algorithm to use (adler32, crc[16 cksum_size Yes Checksum chunk size. srv_cksum Yes Perform additional checksum verification on server (default: off). max_snapshot No Impose a upper limit on number of snapshots to retain (default: 0, no limitation). acl No Container access control list. compression Yes Whether online compression is enabled (off, lz4, deflate[1-4]) dedup Yes Inline deduplication off, hash based (hash) or using memory compare (memcmp) dedup_threshold Yes Minimum I/O size to consider for deduplication encryption Yes Inline encryption off, or algorithm to use (XTS[128 256], CBC[128 192 status No Current state of the container alloc_oid No Maximum allocated object ID by container allocator Refer to the Data Integrity and Access Control Lists sections for more details on the checksum and access-related properties. Refer to the Inline Deduplication section for details about additional properties for that preview feature that are not listed here. While those properties are currently stored persistently with container metadata, many of them are still under development. The ability to modify some of these properties on an existing container will also be provided in a future release. Data Integrity \u00b6 DAOS allows to detect and fix (when data protection is enabled) silent data corruptions. This is done by calculating checksums for both data and metadata in the DAOS library on the client side and storing those checksums persistently in SCM. The checksums will then be validated on access and on update/write as well on the server side if server verify option is enabled. Corrupted data will never be returned to the application. When a corruption is detected, DAOS will try to read from a different replica, if any. If the original data cannot be recovered, then an error will be reported to the application. To enable and configure checksums, the following container properties are used during container create. cksum ( DAOS_PROP_CO_CSUM ): the type of checksum algorithm to use. Supported values are adler32, crc[16|32|64] or sha[1|256|512]. By default, checksum is disabled for new containers. cksum_size ( DAOS_PROP_CO_CSUM_CHUNK_SIZE ): defines the chunk size used for creating checksums of array types. (default is 32K). srv_cksum ( DAOS_PROP_CO_CSUM_SERVER_VERIFY ): Because of the probable decrease to IOPS, in most cases, it is not desired to verify checksums on an object update on the server side. It is sufficient for the client to verify on a fetch because any data corruption, whether on the object update, storage, or fetch, will be caught. However, there is an advantage to knowing if corruption happens on an update. The update would fail right away, indicating to the client to retry the RPC or report an error to upper levels. For instance, to create a new container with crc64 checksum enabled and checksum verification on the server side, one can use the following command line: $ daos cont create tank --label mycont --properties cksum:crc64,srv_cksum:on Successfully created container dfa09efd-4529-482c-b7cd-748c29ef7419 $ daos cont get-prop tank mycont4 | grep cksum Checksum crc64 Checksum Chunk Size 32 KiB Server Checksumming on Note Note that currently, once a container is created, its checksum configuration cannot be changed. Inline Deduplication (Preview) \u00b6 Data deduplication (dedup) is a process that allows to eliminate duplicated data copies in order to decrease capacity requirements. DAOS has some initial support of inline dedup. When dedup is enabled, each DAOS server maintains a per-pool table indexing extents by their hash (i.e. checksum). Any new I/Os bigger than the deduplication threshold will thus be looked up in this table to find out whether an existing extent with the same signature has already been stored. If an extent is found, then two options are provided: Transferring the data from the client to the server and doing a memory compare (i.e. memcmp) of the two extents to verify that they are indeed identical. Trusting the hash function and skipping the data transfer. To minimize issue with hash collision, a cryptographic hash function (i.e. SHA256) is used in this case. The benefit of this approarch is that the data to be written does not need to be transferred to the server. Data processing is thus greatly accelerated. The inline dedup feature can be enabled on a per-container basis. To enable and configure dedup, the following container properties are used: dedup ( DAOS_PROP_CO_DEDUP ): Type of dedup mechanism to use. Supported values are off (default), memcmp (memory compare) or hash (hash-based using SHA256). dedup_threshold ( DAOS_PROP_CO_DEDUP_THRESHOLD ): defines the minimal I/O size to consider the I/O for dedup (default is 4K). Warning Dedup is a feature preview in 2.0 and has some known limitations. Aggregation of deduplicated extents isn't supported and the checksum tree isn't persistent yet. This means that aggregation is disabled for a container with dedplication enabled and duplicated extents won't be matched after a server restart. NVMe isn't supported for dedup enabled container, so please make sure not using dedup on the pool with NVMe enabled. Compression & Encryption (unsupported) \u00b6 The compression ( DAOS_PROP_CO_COMPRESS ) and encryption ( DAOS_PROP_CO_ENCRYPT ) properties are reserved for configuring respectively online compression and encryption. These features are currently not on the roadmap. Snapshot & Rollback \u00b6 The daos tool provides container {create/destroy}-snap and list-snaps commands. $ daos cont create-snap tank mycont snapshot/epoch 262508437483290624 has been created $ daos cont list-snaps tank mycont Container's snapshots : 262508437483290624 $ daos cont destroy-snap tank mycont -e 262508437483290624 The max_snapshot ( DAOS_PROP_CO_SNAPSHOT_MAX ) property is used to limit the maximum number of snapshots to retain. When a new snapshot is taken, and the threshold is reached, the oldest snapshot will be automatically deleted. Rolling back the content of a container to a snapshot is planned for future DAOS versions. User Attributes \u00b6 Similar to POSIX extended attributes, users can attach some metadata to each container through the daos cont [set|get|list|del]-attr commands or via the daos_cont_{list/get/set}_attr() functions of the libdaos API. $ daos cont set-attr tank mycont import_date \"12/01/2021\" $ daos cont list-attr tank mycont Attributes for container mycont: Name ---- import_date $ daos cont get-attr tank mycont import_date Attributes for container mycont: Name Value ---- ----- import_date 12/01/2021 $ daos cont del-attr tank mycont import_date $ daos cont list-attr tank mycont Attributes for container mycont: No attributes found. Access Control Lists \u00b6 Client user and group access for containers is controlled by Access Control Lists (ACLs) . Access-controlled container accesses include: Opening the container for access. Reading and writing data in the container. Reading and writing objects. Getting, setting, and listing user attributes. Getting, setting, and listing snapshots. Deleting the container (if the pool does not grant the user permission). Getting and setting container properties. Getting and modifying the container ACL. Modifying the container's owner. This is reflected in the set of supported container permissions . Pool vs. Container Permissions \u00b6 In general, pool permissions are separate from container permissions, and access to one does not guarantee access to the other. However, a user must have permission to connect to a container's pool before they can access the container in any way, regardless of their permissions on that container. Once the user has connected to a pool, container access decisions are based on the individual container ACL. A user need not have read/write access to a pool in order to open a container with read/write access, for example. There is one situation in which the pool can grant a container-level permission: Container deletion. If a user has Delete permission on a pool, this grants them the ability to delete any container in the pool, regardless of their permissions on that container. If the user does not have Delete permission on the pool, they will only be able to delete containers for which they have been explicitly granted Delete permission in the container's ACL. ACL at Container Creation \u00b6 To create a container labeled mycont in a pool labeled tank with a custom ACL: $ daos cont create <pool_label> --label <container_label> --acl-file=<path> The ACL file format is detailed in the security overview . Displaying ACL \u00b6 To view a container's ACL: $ daos cont get-acl <pool_label> <container_label> The output is in the same string format used in the ACL file during creation, with one ACE per line. Modifying a Container's ACL \u00b6 For all of these commands using an ACL file, the ACL file must be in the format noted above for container creation. Overwriting the ACL \u00b6 To replace a container's ACL with a new ACL: $ daos cont overwrite-acl <pool_label> <container_label> --acl-file=<path> Adding and Updating ACEs \u00b6 To add or update multiple entries in an existing container ACL: $ daos cont update-acl <pool_label> <container_label> --acl-file=<path> To add or update a single entry in an existing container ACL: $ daos cont update-acl <pool_label> <container_label> --entry <ACE> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one. Removing an ACE \u00b6 To delete an entry for a given principal in an existing container ACL: $ daos cont delete-acl <pool_label> <container_label> --principal=<principal> The principal argument refers to the principal , or identity, of the entry to be removed. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ GROUP@ EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the container will be decided based on the remaining ACL rules. Ownership \u00b6 The ownership of the container corresponds to the special principals OWNER@ and GROUP@ in the ACL. These values are a part of the container properties. They may be set on container creation and changed later. Privileges \u00b6 The owner-user ( OWNER@ ) has some implicit privileges on their container. These permissions are silently included alongside any permissions that the user was explicitly granted by entries in the ACL. The owner-user will always have the following implicit capabilities: Open container Set ACL (A) Get ACL (a) Because the owner's special permissions are implicit, they do not need to be specified in the OWNER@ entry. After determining the user's privileges from the container ACL, DAOS checks whether the user requesting access is the owner-user. If so, DAOS grants the owner's implicit permissions to that user, in addition to any permissions granted by the ACL. In contrast, the owner-group ( GROUP@ ) has no special permissions beyond those explicitly granted by the GROUP@ entry in the ACL. Setting Ownership at Creation \u00b6 The default owner user and group are the effective user and group of the user creating the container. However, a specific user and/or group may be specified at container creation time. $ daos cont create <pool_label> <container_label> --user=<owner-user> --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals . Changing Ownership \u00b6 To change the owner user: $ daos cont set-owner <pool_label> <container_label> --user=<owner-user> To change the owner group: $ daos cont set-owner <pool_label> <container_label> --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals .","title":"Container Management"},{"location":"user/container/#container-management","text":"DAOS containers are datasets managed by the users. A container is the unit of snapshot and has a type. It can be a POSIX namespace, an HDF5 file or any other new application-specific data model. The chapter explains how to manager container, while the subsequent ones detail how to access a DAOS container from applications.","title":"Container Management"},{"location":"user/container/#container-creationdestroy","text":"Containers can be created and destroyed through the daos(1) utility. provided to manage containers. To create and then query a container labeled mycont on a pool (labeled tank ): $ daos cont create tank --label mycont Container UUID : daefe12c-45d4-44f7-8e56-995d02549041 Container Label: mycont Container Type : unknown Successfully created container daefe12c-45d4-44f7-8e56-995d02549041 $ daos cont query tank mycont Container UUID : daefe12c-45d4-44f7-8e56-995d02549041 Container Label : mycont Container Type : unknown Pool UUID : 0d1fad71-5681-48d4-acdd-7bb2e786f12e Number of snapshots : 0 Latest Persistent Snapshot : 0 Highest Aggregated Epoch : 263546931609567249 Container redundancy factor: 0 Snapshot Epochs : The label can be up to 127 characters long and must only include alphanumeric characters, colon (':'), period ('.') or underscore ('_'). The container type (i.e., POSIX or HDF5) can be passed via the --type option. As shown below, the pool UUID, container UUID, and container attributes can be stored in the extended attributes of a POSIX file or directory for convenience. Then subsequent invocations of the daos tools need to reference the path to the POSIX file or directory. $ daos cont create tank --path /tmp/mycontainer --type=POSIX --oclass=SX Container UUID : 30e5d364-62c9-4ddf-9284-1021359455f2 Container Type : POSIX Successfully created container 30e5d364-62c9-4ddf-9284-1021359455f2 type POSIX $ daos cont query --path /tmp/mycontainer Container UUID : 30e5d364-62c9-4ddf-9284-1021359455f2 Container Type : POSIX Pool UUID : 0d1fad71-5681-48d4-acdd-7bb2e786f12e Number of snapshots : 0 Latest Persistent Snapshot : 0 Highest Aggregated Epoch : 263548861715283973 Container redundancy factor: 0 Snapshot Epochs : Object Class : SX Chunk Size : 1.0 MiB To list all containers available in a pool: $ daos cont list tank UUID Label ---- ----- 30e5d364-62c9-4ddf-9284-1021359455f2 container_label_not_set daefe12c-45d4-44f7-8e56-995d02549041 mycont To destroy a container: $ daos cont destroy tank mycont Successfully destroyed container mycont $ daos cont destroy --path /tmp/mycontainer Successfully destroyed container 30e5d364-62c9-4ddf-9284-1021359455f2","title":"Container Creation/Destroy"},{"location":"user/container/#container-properties","text":"Container properties are the main mechanism that one can use to control the behavior of container. This includes the type of middleware, whether some features like deduplication or checksum are enabled. Some properties are immutable after creation creation, while some others can be dynamically changed.","title":"Container Properties"},{"location":"user/container/#querying-properties","text":"The user-level administration daos utility may be used to query a container's properties. Refer to the manual page for full usage details. $ daos cont get-prop tank mycont # -OR- --path interface shown below $ daos cont get-prop --path=/tmp/mycontainer Properties for container mycont Name Value ---- ----- Highest Allocated OID 0 Checksum off Checksum Chunk Size 32 KiB Compression off Deduplication off Dedupe Threshold 4.0 KiB EC Cell Size 1.0 MiB Encryption off Group jlombard@ Label mycont Layout Type unknown (0) Layout Version 1 Max Snapshot 0 Owner jlombard@ Redundancy Factor rf0 Redundancy Level rank (1) Server Checksumming off Health HEALTHY Access Control List A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT Additionally, a container's properties may be retrieved using the libdaos API daos_cont_query() function. Refer to the file src/include/daos_cont.h Doxygen comments and the online documentation available here .","title":"Querying Properties"},{"location":"user/container/#changing-properties","text":"By default, a container will inherit a set of default value for each property. Those can be overridden at container creation time via the --properties option. $ daos cont create tank --label mycont2 --properties cksum:sha1,dedup:hash,rf:1 Container UUID : a6286ead-1952-4faa-bf87-00fc0f3785aa Container Label: mycont2 Container Type : unknown Successfully created container a6286ead-1952-4faa-bf87-00fc0f3785aa $ daos cont query tank mycont2 Properties for container mycont2 Name Value ---- ----- Highest Allocated OID 0 Checksum sha1 Checksum Chunk Size 32 KiB Compression off Deduplication hash Dedupe Threshold 4.0 KiB EC Cell Size 1.0 MiB Encryption off Group jlombard@ Label mycont2 Layout Type unknown (0) Layout Version 1 Max Snapshot 0 Owner jlombard@ Redundancy Factor rf1 Redundancy Level rank (1) Server Checksumming off Health HEALTHY Access Control List A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT Mutable properties can be modified after container creation via the set-prop option. $ daos cont set-prop tank mycont2 --properties label:mycont3 Properties were successfully set This effectively changed the container label. $ daos cont get-prop tank mycont2 ERROR: daos: DER_NONEXIST(-1005): The specified entity does not exist $ daos cont get-prop tank mycont3 Properties for container mycont3 Name Value ---- ----- Highest Allocated OID 0 Checksum sha1 Checksum Chunk Size 32 KiB Compression off Deduplication hash Dedupe Threshold 4.0 KiB EC Cell Size 1.0 MiB Encryption off Group jlombard@ Label mycont3 Layout Type unknown (0) Layout Version 1 Max Snapshot 0 Owner jlombard@ Redundancy Factor rf1 Redundancy Level rank (1) Server Checksumming off Health HEALTHY Access Control List A::OWNER@:rwdtTaAo, A:G:GROUP@:rwtT","title":"Changing Properties"},{"location":"user/container/#property-values","text":"The table below summarizes the different container properties available. Container Property Immutable Description label No String associate with a containers. e.g., \"Cat_Pics\" or \"ResNet50_training_data\". owner Yes User acting as the owner of the container. group Yes Group acting as the owner of the container layout_type Yes The container type (POSIX, HDF5, ...) layout_ver Yes A version of the layout that can be used by I/O middleware to handle interoperability. rf Yes The redundancy factor that drives the minimal data protection required for objects stored in the container. e.g., RF1 means no data protection, RF3 only allows 3-way replication or erasure code N+2. rf_lvl Yes The fault domain level to use for data redundancy placement. This is used to determine object placement. ec_cell Yes Erasure code cell size for erasure-coded objects. cksum Yes Checksum off, or algorithm to use (adler32, crc[16 cksum_size Yes Checksum chunk size. srv_cksum Yes Perform additional checksum verification on server (default: off). max_snapshot No Impose a upper limit on number of snapshots to retain (default: 0, no limitation). acl No Container access control list. compression Yes Whether online compression is enabled (off, lz4, deflate[1-4]) dedup Yes Inline deduplication off, hash based (hash) or using memory compare (memcmp) dedup_threshold Yes Minimum I/O size to consider for deduplication encryption Yes Inline encryption off, or algorithm to use (XTS[128 256], CBC[128 192 status No Current state of the container alloc_oid No Maximum allocated object ID by container allocator Refer to the Data Integrity and Access Control Lists sections for more details on the checksum and access-related properties. Refer to the Inline Deduplication section for details about additional properties for that preview feature that are not listed here. While those properties are currently stored persistently with container metadata, many of them are still under development. The ability to modify some of these properties on an existing container will also be provided in a future release.","title":"Property Values"},{"location":"user/container/#data-integrity","text":"DAOS allows to detect and fix (when data protection is enabled) silent data corruptions. This is done by calculating checksums for both data and metadata in the DAOS library on the client side and storing those checksums persistently in SCM. The checksums will then be validated on access and on update/write as well on the server side if server verify option is enabled. Corrupted data will never be returned to the application. When a corruption is detected, DAOS will try to read from a different replica, if any. If the original data cannot be recovered, then an error will be reported to the application. To enable and configure checksums, the following container properties are used during container create. cksum ( DAOS_PROP_CO_CSUM ): the type of checksum algorithm to use. Supported values are adler32, crc[16|32|64] or sha[1|256|512]. By default, checksum is disabled for new containers. cksum_size ( DAOS_PROP_CO_CSUM_CHUNK_SIZE ): defines the chunk size used for creating checksums of array types. (default is 32K). srv_cksum ( DAOS_PROP_CO_CSUM_SERVER_VERIFY ): Because of the probable decrease to IOPS, in most cases, it is not desired to verify checksums on an object update on the server side. It is sufficient for the client to verify on a fetch because any data corruption, whether on the object update, storage, or fetch, will be caught. However, there is an advantage to knowing if corruption happens on an update. The update would fail right away, indicating to the client to retry the RPC or report an error to upper levels. For instance, to create a new container with crc64 checksum enabled and checksum verification on the server side, one can use the following command line: $ daos cont create tank --label mycont --properties cksum:crc64,srv_cksum:on Successfully created container dfa09efd-4529-482c-b7cd-748c29ef7419 $ daos cont get-prop tank mycont4 | grep cksum Checksum crc64 Checksum Chunk Size 32 KiB Server Checksumming on Note Note that currently, once a container is created, its checksum configuration cannot be changed.","title":"Data Integrity"},{"location":"user/container/#inline-deduplication-preview","text":"Data deduplication (dedup) is a process that allows to eliminate duplicated data copies in order to decrease capacity requirements. DAOS has some initial support of inline dedup. When dedup is enabled, each DAOS server maintains a per-pool table indexing extents by their hash (i.e. checksum). Any new I/Os bigger than the deduplication threshold will thus be looked up in this table to find out whether an existing extent with the same signature has already been stored. If an extent is found, then two options are provided: Transferring the data from the client to the server and doing a memory compare (i.e. memcmp) of the two extents to verify that they are indeed identical. Trusting the hash function and skipping the data transfer. To minimize issue with hash collision, a cryptographic hash function (i.e. SHA256) is used in this case. The benefit of this approarch is that the data to be written does not need to be transferred to the server. Data processing is thus greatly accelerated. The inline dedup feature can be enabled on a per-container basis. To enable and configure dedup, the following container properties are used: dedup ( DAOS_PROP_CO_DEDUP ): Type of dedup mechanism to use. Supported values are off (default), memcmp (memory compare) or hash (hash-based using SHA256). dedup_threshold ( DAOS_PROP_CO_DEDUP_THRESHOLD ): defines the minimal I/O size to consider the I/O for dedup (default is 4K). Warning Dedup is a feature preview in 2.0 and has some known limitations. Aggregation of deduplicated extents isn't supported and the checksum tree isn't persistent yet. This means that aggregation is disabled for a container with dedplication enabled and duplicated extents won't be matched after a server restart. NVMe isn't supported for dedup enabled container, so please make sure not using dedup on the pool with NVMe enabled.","title":"Inline Deduplication (Preview)"},{"location":"user/container/#compression-encryption-unsupported","text":"The compression ( DAOS_PROP_CO_COMPRESS ) and encryption ( DAOS_PROP_CO_ENCRYPT ) properties are reserved for configuring respectively online compression and encryption. These features are currently not on the roadmap.","title":"Compression &amp; Encryption (unsupported)"},{"location":"user/container/#snapshot-rollback","text":"The daos tool provides container {create/destroy}-snap and list-snaps commands. $ daos cont create-snap tank mycont snapshot/epoch 262508437483290624 has been created $ daos cont list-snaps tank mycont Container's snapshots : 262508437483290624 $ daos cont destroy-snap tank mycont -e 262508437483290624 The max_snapshot ( DAOS_PROP_CO_SNAPSHOT_MAX ) property is used to limit the maximum number of snapshots to retain. When a new snapshot is taken, and the threshold is reached, the oldest snapshot will be automatically deleted. Rolling back the content of a container to a snapshot is planned for future DAOS versions.","title":"Snapshot &amp; Rollback"},{"location":"user/container/#user-attributes","text":"Similar to POSIX extended attributes, users can attach some metadata to each container through the daos cont [set|get|list|del]-attr commands or via the daos_cont_{list/get/set}_attr() functions of the libdaos API. $ daos cont set-attr tank mycont import_date \"12/01/2021\" $ daos cont list-attr tank mycont Attributes for container mycont: Name ---- import_date $ daos cont get-attr tank mycont import_date Attributes for container mycont: Name Value ---- ----- import_date 12/01/2021 $ daos cont del-attr tank mycont import_date $ daos cont list-attr tank mycont Attributes for container mycont: No attributes found.","title":"User Attributes"},{"location":"user/container/#access-control-lists","text":"Client user and group access for containers is controlled by Access Control Lists (ACLs) . Access-controlled container accesses include: Opening the container for access. Reading and writing data in the container. Reading and writing objects. Getting, setting, and listing user attributes. Getting, setting, and listing snapshots. Deleting the container (if the pool does not grant the user permission). Getting and setting container properties. Getting and modifying the container ACL. Modifying the container's owner. This is reflected in the set of supported container permissions .","title":"Access Control Lists"},{"location":"user/container/#pool-vs-container-permissions","text":"In general, pool permissions are separate from container permissions, and access to one does not guarantee access to the other. However, a user must have permission to connect to a container's pool before they can access the container in any way, regardless of their permissions on that container. Once the user has connected to a pool, container access decisions are based on the individual container ACL. A user need not have read/write access to a pool in order to open a container with read/write access, for example. There is one situation in which the pool can grant a container-level permission: Container deletion. If a user has Delete permission on a pool, this grants them the ability to delete any container in the pool, regardless of their permissions on that container. If the user does not have Delete permission on the pool, they will only be able to delete containers for which they have been explicitly granted Delete permission in the container's ACL.","title":"Pool vs. Container Permissions"},{"location":"user/container/#acl-at-container-creation","text":"To create a container labeled mycont in a pool labeled tank with a custom ACL: $ daos cont create <pool_label> --label <container_label> --acl-file=<path> The ACL file format is detailed in the security overview .","title":"ACL at Container Creation"},{"location":"user/container/#displaying-acl","text":"To view a container's ACL: $ daos cont get-acl <pool_label> <container_label> The output is in the same string format used in the ACL file during creation, with one ACE per line.","title":"Displaying ACL"},{"location":"user/container/#modifying-a-containers-acl","text":"For all of these commands using an ACL file, the ACL file must be in the format noted above for container creation.","title":"Modifying a Container's ACL"},{"location":"user/container/#overwriting-the-acl","text":"To replace a container's ACL with a new ACL: $ daos cont overwrite-acl <pool_label> <container_label> --acl-file=<path>","title":"Overwriting the ACL"},{"location":"user/container/#adding-and-updating-aces","text":"To add or update multiple entries in an existing container ACL: $ daos cont update-acl <pool_label> <container_label> --acl-file=<path> To add or update a single entry in an existing container ACL: $ daos cont update-acl <pool_label> <container_label> --entry <ACE> If there is no existing entry for the principal in the ACL, the new entry is added to the ACL. If there is already an entry for the principal, that entry is replaced with the new one.","title":"Adding and Updating ACEs"},{"location":"user/container/#removing-an-ace","text":"To delete an entry for a given principal in an existing container ACL: $ daos cont delete-acl <pool_label> <container_label> --principal=<principal> The principal argument refers to the principal , or identity, of the entry to be removed. For the delete operation, the principal argument must be formatted as follows: Named user: u:username@ Named group: g:groupname@ Special principals: OWNER@ GROUP@ EVERYONE@ The entry for that principal will be completely removed. This does not always mean that the principal will have no access. Rather, their access to the container will be decided based on the remaining ACL rules.","title":"Removing an ACE"},{"location":"user/container/#ownership","text":"The ownership of the container corresponds to the special principals OWNER@ and GROUP@ in the ACL. These values are a part of the container properties. They may be set on container creation and changed later.","title":"Ownership"},{"location":"user/container/#privileges","text":"The owner-user ( OWNER@ ) has some implicit privileges on their container. These permissions are silently included alongside any permissions that the user was explicitly granted by entries in the ACL. The owner-user will always have the following implicit capabilities: Open container Set ACL (A) Get ACL (a) Because the owner's special permissions are implicit, they do not need to be specified in the OWNER@ entry. After determining the user's privileges from the container ACL, DAOS checks whether the user requesting access is the owner-user. If so, DAOS grants the owner's implicit permissions to that user, in addition to any permissions granted by the ACL. In contrast, the owner-group ( GROUP@ ) has no special permissions beyond those explicitly granted by the GROUP@ entry in the ACL.","title":"Privileges"},{"location":"user/container/#setting-ownership-at-creation","text":"The default owner user and group are the effective user and group of the user creating the container. However, a specific user and/or group may be specified at container creation time. $ daos cont create <pool_label> <container_label> --user=<owner-user> --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals .","title":"Setting Ownership at Creation"},{"location":"user/container/#changing-ownership","text":"To change the owner user: $ daos cont set-owner <pool_label> <container_label> --user=<owner-user> To change the owner group: $ daos cont set-owner <pool_label> <container_label> --group=<owner-group> The user and group names are case sensitive and must be formatted as DAOS ACL user/group principals .","title":"Changing Ownership"},{"location":"user/datamover/","text":"Data Mover \u00b6 The Dataset Mover is a collection of multiple tools that allow users to copy and serialize data across DAOS and POSIX file systems. There is support for data movement across POSIX, DAOS, and HDF5 containers. There is also support for serializing and deserializing a DAOS container, where a representation of the container is stored on a POSIX filesystem in an HDF5 file(s) and can be restored to a new DAOS container. Overview of Tools \u00b6 These tools are implemented within the daos command. daos filesystem copy - Copy between POSIX containers and POSIX filesystems using the libdfs library. daos container clone - Copy any container to a new container using the Object API ( libdaos library). These tools have MPI support and are implemented in the external MpiFileutils repository. dcp - Copy between POSIX containers and POSIX filesystems using the libdfs library, or copy between any two DAOS containers using the Object API ( libdaos library). dsync - Similar to dcp , but attempts to only copy the difference between the source and destination. daos-serialize - Serialize any DAOS container to an HDF5 file(s). daos-deserialize - Deserialize any DAOS container that was serialized with daos-serialize . More documentation and uses cases for these tools can be found here . Build instructions for these tools can be found here . DAOS Tools Usage \u00b6 daos filesystem copy \u00b6 There are two mandatory command-line options; these are: Command-line Option Description --src=daos://<pool/cont> | <path> the source path --dst=daos://<pool/cont> | <path> the destination path Note In DAOS 1.2, only directories are supported as the source or destination. Files, directories, and symbolic links are copied from the source directory. Examples \u00b6 Copy a POSIX container to a POSIX filesystem: $ daos filesystem copy --src daos://<pool_uuid>/<cont_uuid> --dst <posix_path> Copy from a POSIX filesystem to a sub-directory in a POSIX container: $ daos filesystem copy --src <posix_path> --dst daos://<pool_uuid>/<cont_uuid>/<sub_dir> Copy from a POSIX container by specifying a UNS path: $ daos filesystem copy --src <uns_path> --dst <posix_path> daos container clone \u00b6 There are two mandatory command-line options; these are: Command-line Option Description --src=daos://<pool/cont> | <path> the source container --dst=daos://<pool>[/cont>] | <path> the destination container The destination container must not already exist. Examples \u00b6 Clone a container to a new container with a given UUID: $ daos container clone --src /<pool_uuid>/<cont_uuid> --dst /<pool_uuid>/<new_cont_uuid> Clone a container to a new container with an auto-generated UUID: $ daos container clone --src /<pool_uuid>/<cont_uuid> --dst /<pool_uuid>","title":"Data Mover"},{"location":"user/datamover/#data-mover","text":"The Dataset Mover is a collection of multiple tools that allow users to copy and serialize data across DAOS and POSIX file systems. There is support for data movement across POSIX, DAOS, and HDF5 containers. There is also support for serializing and deserializing a DAOS container, where a representation of the container is stored on a POSIX filesystem in an HDF5 file(s) and can be restored to a new DAOS container.","title":"Data Mover"},{"location":"user/datamover/#overview-of-tools","text":"These tools are implemented within the daos command. daos filesystem copy - Copy between POSIX containers and POSIX filesystems using the libdfs library. daos container clone - Copy any container to a new container using the Object API ( libdaos library). These tools have MPI support and are implemented in the external MpiFileutils repository. dcp - Copy between POSIX containers and POSIX filesystems using the libdfs library, or copy between any two DAOS containers using the Object API ( libdaos library). dsync - Similar to dcp , but attempts to only copy the difference between the source and destination. daos-serialize - Serialize any DAOS container to an HDF5 file(s). daos-deserialize - Deserialize any DAOS container that was serialized with daos-serialize . More documentation and uses cases for these tools can be found here . Build instructions for these tools can be found here .","title":"Overview of Tools"},{"location":"user/datamover/#daos-tools-usage","text":"","title":"DAOS Tools Usage"},{"location":"user/datamover/#daos-filesystem-copy","text":"There are two mandatory command-line options; these are: Command-line Option Description --src=daos://<pool/cont> | <path> the source path --dst=daos://<pool/cont> | <path> the destination path Note In DAOS 1.2, only directories are supported as the source or destination. Files, directories, and symbolic links are copied from the source directory.","title":"daos filesystem copy"},{"location":"user/datamover/#examples","text":"Copy a POSIX container to a POSIX filesystem: $ daos filesystem copy --src daos://<pool_uuid>/<cont_uuid> --dst <posix_path> Copy from a POSIX filesystem to a sub-directory in a POSIX container: $ daos filesystem copy --src <posix_path> --dst daos://<pool_uuid>/<cont_uuid>/<sub_dir> Copy from a POSIX container by specifying a UNS path: $ daos filesystem copy --src <uns_path> --dst <posix_path>","title":"Examples"},{"location":"user/datamover/#daos-container-clone","text":"There are two mandatory command-line options; these are: Command-line Option Description --src=daos://<pool/cont> | <path> the source container --dst=daos://<pool>[/cont>] | <path> the destination container The destination container must not already exist.","title":"daos container clone"},{"location":"user/datamover/#examples_1","text":"Clone a container to a new container with a given UUID: $ daos container clone --src /<pool_uuid>/<cont_uuid> --dst /<pool_uuid>/<new_cont_uuid> Clone a container to a new container with an auto-generated UUID: $ daos container clone --src /<pool_uuid>/<cont_uuid> --dst /<pool_uuid>","title":"Examples"},{"location":"user/filesystem/","text":"File System \u00b6 A container can be mounted as shared POSIX namespace on multiple compute nodes. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed directly to applications or I/O frameworks (e.g., for frameworks like Spark or TensorFlow, or benchmarks like IOR or mdtest that support different storage backend plugins). It can also be exposed transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottlenecks by delivering full OS bypass for POSIX read/write operations. The performance is going to be best generally when using the DFS API directly. Using the IO interception library with dfuse should yield the same performance for IO operations (read/write) as the DFS API with minimal overhead. Performance of metadata operations (file creation, deletion, rename, etc.) over dfuse will be much slower than the DFS API since there is no interception to bypass the fuse/kernel layer. libdfs \u00b6 The DAOS File System (DFS) is implemented in the libdfs library, and allows a DAOS container to be accessed as a hierarchical POSIX namespace. libdfs supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and are not implemented on a per-file or per-directory basis. Supported Operations \u00b6 The DFS API closely represents the POSIX API. The API includes operations to: * Mount: create/open superblock and root object * Un-mount: release open handles * Lookup: traverse a path and return an open file/dir handle * IO: read & write with an iovec * Stat: retrieve attributes of an entry * Mkdir: create a dir * Readdir: enumerate all entries under a directory * Open: create/Open a file/dir * Remove: unlink a file/dir * Move: rename * Release: close an open handle of a file/dir * Extended Attributes: set, get, list, remove POSIX Compliance \u00b6 The following features from POSIX will not be supported: * Hard links * mmap support with MAP_SHARED will be consistent from single client only. Note that this is supported through DFUSE only (i.e. not through the DFS API). * Char devices, block devices, sockets and pipes * User/group quotas * setuid(), setgid() programs, supplementary groups, ACLs are not supported within the DFS namespace. * [access/change/modify] time not updated appropriately, potentially on close only. * Flock (maybe at dfuse local node level only) * Block size in stat buf is not accurate (no account for holes, extended attributes) * Various parameters reported via statfs like number of blocks, files, free/available space * POSIX permissions inside an encapsulated namespace * Still enforced at the DAOS pool/container level * Effectively means that all files belong to the same \"project\" It is possible to use libdfs in a parallel application from multiple nodes. DFS provides two modes that offer different levels of consistency. The modes can be set on container creation time: 1) Relaxed mode for well-behaved applications that generate conflict-free operations for which a very high level of concurrency will be supported. 2) Balanced mode for applications that require stricter consistency at the cost of performance. This mode is currently not fully supported and DFS by default will use the relaxed mode. On container access, if the container is created with balanced mode, it can be accessed in balanced mode only. If the container was created with relaxed mode, it can be accessed in relaxed or balanced mode. In either mode, there is a consistency semantic issue that is not properly handled: Open-unlink semantics: This occurs when a client obtains an open handle on an object (file or directory), and accesses that object (reads/writes data or create other files), while another client removes that object that the other client has opened from under it. In DAOS, we don't track object open handles as that would be very expensive, and so in such conflicting cases, the worst case scenario is the lost/leaked space that is written to those orphan objects that have been unlinked from the namespace. Other consistency issues are handled differently between the two consistency mode: Same Operation Executed Concurrently (Supported in both Relaxed and Balanced Mode): For example, clients try to create or remove the same file concurrently, one should succeed and others will fail. Create/Unlink/Rename Conflicts (Supported in Balanced Mode only): For example, a client renames a file, but another unlinks the old file at the same time. Operation Atomicity (Supported only in Balanced mode): If a client crashes in the middle of the rename, the state of the container should be consistent as if the operation never happened. Visibility (Supported in Balanced and Relaxed mode): A write from one client should be visible to another client with a simple coordination between the clients. DFuse (DAOS FUSE) \u00b6 DFuse provides DAOS File System access through the standard libc/kernel/VFS POSIX infrastructure. This allows existing applications to use DAOS without modification, and provides a path to upgrade those applications to native DAOS support. Additionally, DFuse provides an Interception Library libioil to transparently allow POSIX clients to talk directly to DAOS servers, providing OS-Bypass for I/O without modifying or recompiling of the application. DFuse builds heavily on DFS. Data written via DFuse can be accessed by DFS and vice versa. DFuse Daemon \u00b6 The dfuse daemon runs a single instance per node to provide a user POSIX access to DAOS. It should be run with the credentials of the user, and typically will be started and stopped on each compute node as part of the prolog and epilog scripts of any resource manager or scheduler in use. Restrictions \u00b6 DFuse is limited to a single user. Access to the filesystem from other users, including root, will not be honored. As a consequence of this, the chown and chgrp calls are not supported. Hard links and special device files, except symbolic links, are not supported, nor are any ACLs. DFuse can run in the foreground, keeping the terminal window open, or it can daemonize to run like a system daemon. However, to do this and still be able to access DAOS it needs to daemonize before calling daos_init() . This in turns means it cannot report some kinds of startup errors either on stdout/stderr or via its return code. When initially starting with DFuse it is recommended to run in foreground mode ( --foreground ) to better observe any failures. Inodes are managed on the local node by DFuse. So while inode numbers will be consistent on a node for the duration of the session, they are not guaranteed to be consistent across restarts of DFuse or across nodes. It is not possible to see pool/container listings through DFuse. So if readdir , ls or others are used, DFuse will return ENOTSUP . Launching \u00b6 DFuse should be run with the credentials (user/group) of the user who will be accessing it, and who owns any pools that will be used. There are two mandatory command-line options, these are: Command-line Option Description --mountpoint=<path> path to mount dfuse The mount point specified should be an empty directory on the local node that is owned by the user. Additionally, there are several optional command-line options: Command-line Option Description --pool=<uuid> pool uuid to connect to --container=<uuid> container uuid to open --sys-name=<name> DAOS system name --foreground run in foreground --singlethreaded run single threaded When DFuse starts, it will register a single mount with the kernel, at the location specified by the --mountpoint option. This mount will be visible in /proc/mounts , and possibly in the output of df . The contents of multiple pools/containers will be accessible via this single kernel mountpoint. Operation Modes \u00b6 DFuse will only create one kernel level mount point regardless of how it is launched. How POSIX containers are represented within that mount point varies depending on the DFuse command-line options. DFuse can operate in three modes. Single Container Mode \u00b6 That's the most common use case where a pool and a POSIX container are provided on the command line. The mount point will map to the root of the container itself. Files can be accessed by simply concatenating the mount point and the name of the file, relative to the root of the container. $ daos cont create tank -l mycont -t POSIX Container UUID : 8a8f08bb-5034-41e8-b7ae-0cdce347c558 Container Label: mycont Container Type : POSIX Successfully created container 8a8f08bb-5034-41e8-b7ae-0cdce347c558 $ mkdir /tmp/dfuse $ dfuse -m /tmp/dfuse --pool tank --cont mycont $ touch /tmp/dfuse/foo $ ls -l /tmp/dfuse/ total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo $ df -h /tmp/dfuse/ Filesystem Size Used Avail Use% Mounted on dfuse 9.4G 326K 9.4G 1% /tmp/dfuse $ fusermount3 -u /tmp/dfuse/ Pool Mode \u00b6 If a pool uuid is specified but not a container uuid, then the containers can be accessed by the path <mount point>/<container uuid> . The container uuid will have to be provided from an external source. $ daos cont create tank -l mycont2 -t POSIX Container UUID : 0db21789-5372-4f2a-b7bc-14c0a5e968df Container Label: mycont2 Container Type : POSIX Successfully created container 0db21789-5372-4f2a-b7bc-14c0a5e968df $ dfuse -m /tmp/dfuse --pool tank $ ls -l /tmp/dfuse/ ls: cannot open directory '/tmp/dfuse/': Operation not supported $ ls -l /tmp/dfuse/0db21789-5372-4f2a-b7bc-14c0a5e968df total 0 $ ls -l /tmp/dfuse/8a8f08bb-5034-41e8-b7ae-0cdce347c558 total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo $ fusermount3 -u /tmp/dfuse/ System Mode \u00b6 If neither a pool or container is specified, then pools and container can be accessed by the path <mount point>/<pool uuid>/<container uuid> . However it should be noted that readdir() and therefore ls do not work on either mount points or directories representing pools here. So the pool and container uuids will have to be provided from an external source. $ dfuse -m /tmp/dfuse $ df -h /tmp/dfuse Filesystem Size Used Avail Use% Mounted on dfuse - - - - /tmp/dfuse $ daos pool query tank | grep -- -.*- Pool 004abf7c-26c8-4cba-9059-8b3be39161fc, ntarget=32, disabled=0, leader=0, version=1 $ ls -l /tmp/dfuse/004abf7c-26c8-4cba-9059-8b3be39161fc/0db21789-5372-4f2a-b7bc-14c0a5e968df total 0 $ ls -l /tmp/dfuse/004abf7c-26c8-4cba-9059-8b3be39161fc/8a8f08bb-5034-41e8-b7ae-0cdce347c558 total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo While this mode is not expected to be used directly by users, it is useful for the unified namespace integration. Links into other Containers \u00b6 It is possible to link to other containers in DFuse, where subdirectories within a container resolve not to regular directories, but rather to the root of entirely different POSIX containers. To create a new container and link it into the namespace of an existing one, use the following command. $ daos container create <pool_label> --type POSIX --path <path_to_entry_point> The pool should already exist, and the path should specify a location somewhere within a DFuse mount point that resolves to a POSIX container. Once a link is created, it can be accessed through the new path. Following the link is virtually transparent. No container uuid is required. If one is not supplied, it will be created. To destroy a container again, the following command should be used. $ daos container destroy --path <path to entry point> This will both remove the link between the containers and remove the container that was linked to. There is no support for adding links to already existing containers or removing links to containers without also removing the container itself. Information about a container, for example, the presence of an entry point between containers, or the pool and container uuids of the container linked to can be read with the following command. $ daos container info --path <path to entry point> Please find below an example. $ dfuse -m /tmp/dfuse --pool tank --cont mycont $ cd /tmp/dfuse/ $ ls foo $ daos cont create tank -l mycont3 --type POSIX --path ./link_to_externa_container Container UUID : 933944a9-ddf2-491a-bdbf-4442f0437d56 Container Label: mycont3 Container Type : POSIX Successfully created container 933944a9-ddf2-491a-bdbf-4442f0437d56 type POSIX $ ls -lrt total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo drwxr-xr-x 1 jlombard jlombard 72 Jul 10 20:56 link_to_externa_container $ daos cont destroy --path ./link_to_externa_container/ Successfully destroyed container 933944a9-ddf2-491a-bdbf-4442f0437d56 jlombard@wolf-151:/tmp/dfuse$ ls -l total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo Caching \u00b6 For performance reasons caching will be enabled by default in DFuse, including both data and metadata caching. It is possible to tune these settings both at a high level on the DFuse command line and fine grained control via container attributes. The following types of data will be cached by default. Kernel caching of dentries Kernel caching of negative dentries Kernel caching of inodes (file sizes, permissions etc) Kernel caching of file contents Readahead in dfuse and inserting data into kernel cache MMAP write optimization Note Caching is enabled by default in dfuse. This might cause some parallel applications to fail. Please disable caching if you experience this or want up to date data sharing between nodes. To selectively control caching within a container the following container attributes should be used, if any attribute is set then the rest are assumed to be set to 0 or off, except dentry-dir-time which defaults to dentry-time Attribute name Description dfuse-attr-time How long file attributes are cached dfuse-dentry-time How long directory entries are cached dfuse-dentry-dir-time How long dentries are cached, if the entry is itself a directory dfuse-ndentry-time How long negative dentries are cached dfuse-data-cache Data caching enabled for this file (\"on\"/\"off\") dfuse-direct-io-disable Force use of page cache for this container (\"on\"/\"off\") For metadata caching attributes specify the duration that the cache should be valid for, specified in seconds, and allowing 'S' or 'M' suffix. dfuse-data-cache should be set to \"on\", or \"off\" if set, any other value will log an error, and result in the cache being off. The O_DIRECT flag for open files will be honoured with this option enabled, files which do not set O_DIRECT will be cached. dfuse-direct-io-disable will enable data caching, similar to dfuse-data-cache, however if this is set to \"on\" then the O_DIRECT flag will be ignored, and all files will use the page cache. This default value for this is \"off\". With no options specified attr and dentry timeouts will be 1 second, dentry-dir and ndentry timeouts will be 5 seconds, and data caching will be enabled. These are two command line options to control the DFuse process itself. Command line option Description --disable-caching Disables all caching --disable-wb-caching Disables write-back cache These will affect all containers accessed via DFuse, regardless of any container attributes. Stopping DFuse \u00b6 When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos When this is done, the local DFuse daemon should shut down the mount point, disconnect from the DAOS servers, and exit. You can also verify that the mount point is no longer listed in /proc/mounts . Interception Library \u00b6 An interception library called libioil is available to work with DFuse. This library works in conjunction with DFuse and allows the interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any application changes. This provides kernel-bypass for I/O data, leading to improved performance. To use this, set LD_PRELOAD to point to the shared library in the DAOS install directory: LD_PRELOAD=/path/to/daos/install/lib/libioil.so LD_PRELOAD=/usr/lib64/libioil.so # when installed from RPMs","title":"File System"},{"location":"user/filesystem/#file-system","text":"A container can be mounted as shared POSIX namespace on multiple compute nodes. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed directly to applications or I/O frameworks (e.g., for frameworks like Spark or TensorFlow, or benchmarks like IOR or mdtest that support different storage backend plugins). It can also be exposed transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottlenecks by delivering full OS bypass for POSIX read/write operations. The performance is going to be best generally when using the DFS API directly. Using the IO interception library with dfuse should yield the same performance for IO operations (read/write) as the DFS API with minimal overhead. Performance of metadata operations (file creation, deletion, rename, etc.) over dfuse will be much slower than the DFS API since there is no interception to bypass the fuse/kernel layer.","title":"File System"},{"location":"user/filesystem/#libdfs","text":"The DAOS File System (DFS) is implemented in the libdfs library, and allows a DAOS container to be accessed as a hierarchical POSIX namespace. libdfs supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and are not implemented on a per-file or per-directory basis.","title":"libdfs"},{"location":"user/filesystem/#supported-operations","text":"The DFS API closely represents the POSIX API. The API includes operations to: * Mount: create/open superblock and root object * Un-mount: release open handles * Lookup: traverse a path and return an open file/dir handle * IO: read & write with an iovec * Stat: retrieve attributes of an entry * Mkdir: create a dir * Readdir: enumerate all entries under a directory * Open: create/Open a file/dir * Remove: unlink a file/dir * Move: rename * Release: close an open handle of a file/dir * Extended Attributes: set, get, list, remove","title":"Supported Operations"},{"location":"user/filesystem/#posix-compliance","text":"The following features from POSIX will not be supported: * Hard links * mmap support with MAP_SHARED will be consistent from single client only. Note that this is supported through DFUSE only (i.e. not through the DFS API). * Char devices, block devices, sockets and pipes * User/group quotas * setuid(), setgid() programs, supplementary groups, ACLs are not supported within the DFS namespace. * [access/change/modify] time not updated appropriately, potentially on close only. * Flock (maybe at dfuse local node level only) * Block size in stat buf is not accurate (no account for holes, extended attributes) * Various parameters reported via statfs like number of blocks, files, free/available space * POSIX permissions inside an encapsulated namespace * Still enforced at the DAOS pool/container level * Effectively means that all files belong to the same \"project\" It is possible to use libdfs in a parallel application from multiple nodes. DFS provides two modes that offer different levels of consistency. The modes can be set on container creation time: 1) Relaxed mode for well-behaved applications that generate conflict-free operations for which a very high level of concurrency will be supported. 2) Balanced mode for applications that require stricter consistency at the cost of performance. This mode is currently not fully supported and DFS by default will use the relaxed mode. On container access, if the container is created with balanced mode, it can be accessed in balanced mode only. If the container was created with relaxed mode, it can be accessed in relaxed or balanced mode. In either mode, there is a consistency semantic issue that is not properly handled: Open-unlink semantics: This occurs when a client obtains an open handle on an object (file or directory), and accesses that object (reads/writes data or create other files), while another client removes that object that the other client has opened from under it. In DAOS, we don't track object open handles as that would be very expensive, and so in such conflicting cases, the worst case scenario is the lost/leaked space that is written to those orphan objects that have been unlinked from the namespace. Other consistency issues are handled differently between the two consistency mode: Same Operation Executed Concurrently (Supported in both Relaxed and Balanced Mode): For example, clients try to create or remove the same file concurrently, one should succeed and others will fail. Create/Unlink/Rename Conflicts (Supported in Balanced Mode only): For example, a client renames a file, but another unlinks the old file at the same time. Operation Atomicity (Supported only in Balanced mode): If a client crashes in the middle of the rename, the state of the container should be consistent as if the operation never happened. Visibility (Supported in Balanced and Relaxed mode): A write from one client should be visible to another client with a simple coordination between the clients.","title":"POSIX Compliance"},{"location":"user/filesystem/#dfuse-daos-fuse","text":"DFuse provides DAOS File System access through the standard libc/kernel/VFS POSIX infrastructure. This allows existing applications to use DAOS without modification, and provides a path to upgrade those applications to native DAOS support. Additionally, DFuse provides an Interception Library libioil to transparently allow POSIX clients to talk directly to DAOS servers, providing OS-Bypass for I/O without modifying or recompiling of the application. DFuse builds heavily on DFS. Data written via DFuse can be accessed by DFS and vice versa.","title":"DFuse (DAOS FUSE)"},{"location":"user/filesystem/#dfuse-daemon","text":"The dfuse daemon runs a single instance per node to provide a user POSIX access to DAOS. It should be run with the credentials of the user, and typically will be started and stopped on each compute node as part of the prolog and epilog scripts of any resource manager or scheduler in use.","title":"DFuse Daemon"},{"location":"user/filesystem/#restrictions","text":"DFuse is limited to a single user. Access to the filesystem from other users, including root, will not be honored. As a consequence of this, the chown and chgrp calls are not supported. Hard links and special device files, except symbolic links, are not supported, nor are any ACLs. DFuse can run in the foreground, keeping the terminal window open, or it can daemonize to run like a system daemon. However, to do this and still be able to access DAOS it needs to daemonize before calling daos_init() . This in turns means it cannot report some kinds of startup errors either on stdout/stderr or via its return code. When initially starting with DFuse it is recommended to run in foreground mode ( --foreground ) to better observe any failures. Inodes are managed on the local node by DFuse. So while inode numbers will be consistent on a node for the duration of the session, they are not guaranteed to be consistent across restarts of DFuse or across nodes. It is not possible to see pool/container listings through DFuse. So if readdir , ls or others are used, DFuse will return ENOTSUP .","title":"Restrictions"},{"location":"user/filesystem/#launching","text":"DFuse should be run with the credentials (user/group) of the user who will be accessing it, and who owns any pools that will be used. There are two mandatory command-line options, these are: Command-line Option Description --mountpoint=<path> path to mount dfuse The mount point specified should be an empty directory on the local node that is owned by the user. Additionally, there are several optional command-line options: Command-line Option Description --pool=<uuid> pool uuid to connect to --container=<uuid> container uuid to open --sys-name=<name> DAOS system name --foreground run in foreground --singlethreaded run single threaded When DFuse starts, it will register a single mount with the kernel, at the location specified by the --mountpoint option. This mount will be visible in /proc/mounts , and possibly in the output of df . The contents of multiple pools/containers will be accessible via this single kernel mountpoint.","title":"Launching"},{"location":"user/filesystem/#operation-modes","text":"DFuse will only create one kernel level mount point regardless of how it is launched. How POSIX containers are represented within that mount point varies depending on the DFuse command-line options. DFuse can operate in three modes.","title":"Operation Modes"},{"location":"user/filesystem/#single-container-mode","text":"That's the most common use case where a pool and a POSIX container are provided on the command line. The mount point will map to the root of the container itself. Files can be accessed by simply concatenating the mount point and the name of the file, relative to the root of the container. $ daos cont create tank -l mycont -t POSIX Container UUID : 8a8f08bb-5034-41e8-b7ae-0cdce347c558 Container Label: mycont Container Type : POSIX Successfully created container 8a8f08bb-5034-41e8-b7ae-0cdce347c558 $ mkdir /tmp/dfuse $ dfuse -m /tmp/dfuse --pool tank --cont mycont $ touch /tmp/dfuse/foo $ ls -l /tmp/dfuse/ total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo $ df -h /tmp/dfuse/ Filesystem Size Used Avail Use% Mounted on dfuse 9.4G 326K 9.4G 1% /tmp/dfuse $ fusermount3 -u /tmp/dfuse/","title":"Single Container Mode"},{"location":"user/filesystem/#pool-mode","text":"If a pool uuid is specified but not a container uuid, then the containers can be accessed by the path <mount point>/<container uuid> . The container uuid will have to be provided from an external source. $ daos cont create tank -l mycont2 -t POSIX Container UUID : 0db21789-5372-4f2a-b7bc-14c0a5e968df Container Label: mycont2 Container Type : POSIX Successfully created container 0db21789-5372-4f2a-b7bc-14c0a5e968df $ dfuse -m /tmp/dfuse --pool tank $ ls -l /tmp/dfuse/ ls: cannot open directory '/tmp/dfuse/': Operation not supported $ ls -l /tmp/dfuse/0db21789-5372-4f2a-b7bc-14c0a5e968df total 0 $ ls -l /tmp/dfuse/8a8f08bb-5034-41e8-b7ae-0cdce347c558 total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo $ fusermount3 -u /tmp/dfuse/","title":"Pool Mode"},{"location":"user/filesystem/#system-mode","text":"If neither a pool or container is specified, then pools and container can be accessed by the path <mount point>/<pool uuid>/<container uuid> . However it should be noted that readdir() and therefore ls do not work on either mount points or directories representing pools here. So the pool and container uuids will have to be provided from an external source. $ dfuse -m /tmp/dfuse $ df -h /tmp/dfuse Filesystem Size Used Avail Use% Mounted on dfuse - - - - /tmp/dfuse $ daos pool query tank | grep -- -.*- Pool 004abf7c-26c8-4cba-9059-8b3be39161fc, ntarget=32, disabled=0, leader=0, version=1 $ ls -l /tmp/dfuse/004abf7c-26c8-4cba-9059-8b3be39161fc/0db21789-5372-4f2a-b7bc-14c0a5e968df total 0 $ ls -l /tmp/dfuse/004abf7c-26c8-4cba-9059-8b3be39161fc/8a8f08bb-5034-41e8-b7ae-0cdce347c558 total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo While this mode is not expected to be used directly by users, it is useful for the unified namespace integration.","title":"System Mode"},{"location":"user/filesystem/#links-into-other-containers","text":"It is possible to link to other containers in DFuse, where subdirectories within a container resolve not to regular directories, but rather to the root of entirely different POSIX containers. To create a new container and link it into the namespace of an existing one, use the following command. $ daos container create <pool_label> --type POSIX --path <path_to_entry_point> The pool should already exist, and the path should specify a location somewhere within a DFuse mount point that resolves to a POSIX container. Once a link is created, it can be accessed through the new path. Following the link is virtually transparent. No container uuid is required. If one is not supplied, it will be created. To destroy a container again, the following command should be used. $ daos container destroy --path <path to entry point> This will both remove the link between the containers and remove the container that was linked to. There is no support for adding links to already existing containers or removing links to containers without also removing the container itself. Information about a container, for example, the presence of an entry point between containers, or the pool and container uuids of the container linked to can be read with the following command. $ daos container info --path <path to entry point> Please find below an example. $ dfuse -m /tmp/dfuse --pool tank --cont mycont $ cd /tmp/dfuse/ $ ls foo $ daos cont create tank -l mycont3 --type POSIX --path ./link_to_externa_container Container UUID : 933944a9-ddf2-491a-bdbf-4442f0437d56 Container Label: mycont3 Container Type : POSIX Successfully created container 933944a9-ddf2-491a-bdbf-4442f0437d56 type POSIX $ ls -lrt total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo drwxr-xr-x 1 jlombard jlombard 72 Jul 10 20:56 link_to_externa_container $ daos cont destroy --path ./link_to_externa_container/ Successfully destroyed container 933944a9-ddf2-491a-bdbf-4442f0437d56 jlombard@wolf-151:/tmp/dfuse$ ls -l total 0 -rw-rw-r-- 1 jlombard jlombard 0 Jul 10 20:23 foo","title":"Links into other Containers"},{"location":"user/filesystem/#caching","text":"For performance reasons caching will be enabled by default in DFuse, including both data and metadata caching. It is possible to tune these settings both at a high level on the DFuse command line and fine grained control via container attributes. The following types of data will be cached by default. Kernel caching of dentries Kernel caching of negative dentries Kernel caching of inodes (file sizes, permissions etc) Kernel caching of file contents Readahead in dfuse and inserting data into kernel cache MMAP write optimization Note Caching is enabled by default in dfuse. This might cause some parallel applications to fail. Please disable caching if you experience this or want up to date data sharing between nodes. To selectively control caching within a container the following container attributes should be used, if any attribute is set then the rest are assumed to be set to 0 or off, except dentry-dir-time which defaults to dentry-time Attribute name Description dfuse-attr-time How long file attributes are cached dfuse-dentry-time How long directory entries are cached dfuse-dentry-dir-time How long dentries are cached, if the entry is itself a directory dfuse-ndentry-time How long negative dentries are cached dfuse-data-cache Data caching enabled for this file (\"on\"/\"off\") dfuse-direct-io-disable Force use of page cache for this container (\"on\"/\"off\") For metadata caching attributes specify the duration that the cache should be valid for, specified in seconds, and allowing 'S' or 'M' suffix. dfuse-data-cache should be set to \"on\", or \"off\" if set, any other value will log an error, and result in the cache being off. The O_DIRECT flag for open files will be honoured with this option enabled, files which do not set O_DIRECT will be cached. dfuse-direct-io-disable will enable data caching, similar to dfuse-data-cache, however if this is set to \"on\" then the O_DIRECT flag will be ignored, and all files will use the page cache. This default value for this is \"off\". With no options specified attr and dentry timeouts will be 1 second, dentry-dir and ndentry timeouts will be 5 seconds, and data caching will be enabled. These are two command line options to control the DFuse process itself. Command line option Description --disable-caching Disables all caching --disable-wb-caching Disables write-back cache These will affect all containers accessed via DFuse, regardless of any container attributes.","title":"Caching"},{"location":"user/filesystem/#stopping-dfuse","text":"When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos When this is done, the local DFuse daemon should shut down the mount point, disconnect from the DAOS servers, and exit. You can also verify that the mount point is no longer listed in /proc/mounts .","title":"Stopping DFuse"},{"location":"user/filesystem/#interception-library","text":"An interception library called libioil is available to work with DFuse. This library works in conjunction with DFuse and allows the interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any application changes. This provides kernel-bypass for I/O data, leading to improved performance. To use this, set LD_PRELOAD to point to the shared library in the DAOS install directory: LD_PRELOAD=/path/to/daos/install/lib/libioil.so LD_PRELOAD=/usr/lib64/libioil.so # when installed from RPMs","title":"Interception Library"},{"location":"user/hdf5/","text":"HDF5 Support \u00b6 The Hierarchical Data Format Version 5 (HDF5) specification and tools are maintained by the HDF Group (https://www.hdfgroup.org/). Applications that use HDF5 can utilize DAOS in two ways: HDF5 over MPI-IO \u00b6 Parallel HDF5 is typically layered on top of MPI-IO. By building HDF5 and the user application with an MPI stack that includes the DAOS support for MPI-IO, such HDF5 applications can be run on top of DAOS. See the MPI-IO section for instructions on how to build and run applications with MPI-IO DAOS support. HDF5 DAOS VOL Connector \u00b6 A HDF5 DAOS connector is available from the HDF Group. Please refer to the HDF5 DAOS VOL Connector Users Guide for instructions on how to build and use HDF5 with this DAOS VOL connector. The presentation Advancing HDF5's Parallel I/O for Exascale with DAOS from the HDF Users Group 2020 describes the HDF5 DAOS VOL Connector Project and its current status. The video of that presentation is also available online.","title":"HDF5 Support"},{"location":"user/hdf5/#hdf5-support","text":"The Hierarchical Data Format Version 5 (HDF5) specification and tools are maintained by the HDF Group (https://www.hdfgroup.org/). Applications that use HDF5 can utilize DAOS in two ways:","title":"HDF5 Support"},{"location":"user/hdf5/#hdf5-over-mpi-io","text":"Parallel HDF5 is typically layered on top of MPI-IO. By building HDF5 and the user application with an MPI stack that includes the DAOS support for MPI-IO, such HDF5 applications can be run on top of DAOS. See the MPI-IO section for instructions on how to build and run applications with MPI-IO DAOS support.","title":"HDF5 over MPI-IO"},{"location":"user/hdf5/#hdf5-daos-vol-connector","text":"A HDF5 DAOS connector is available from the HDF Group. Please refer to the HDF5 DAOS VOL Connector Users Guide for instructions on how to build and use HDF5 with this DAOS VOL connector. The presentation Advancing HDF5's Parallel I/O for Exascale with DAOS from the HDF Users Group 2020 describes the HDF5 DAOS VOL Connector Project and its current status. The video of that presentation is also available online.","title":"HDF5 DAOS VOL Connector"},{"location":"user/interface/","text":"Native Programming Interface \u00b6 Building against the DAOS library \u00b6 To build an application or I/O middleware against the native DAOS API, include the daos.h header file in your program and link with -Ldaos . Examples are available under src/tests . DAOS API Reference \u00b6 libdaos is written in C and uses Doxygen comments that are added to C header files. The Doxygen documentation is available here . Python Bindings \u00b6 A python module called PyDAOS provides the DAOS API to python users. pydaos \u00b6 pydaos provides a native DAOS python interface exported by a C module. It integrates the DAOS key-value store API with python dictionaries. Only strings are supported for both the key and value for now. Key-value pairs can be inserted/looked up one at a time (see put/get) or in bulk (see bput/bget), taking a python dict as an input. The bulk operations are issued in parallel (up to 16 operations in flight) to maximize the operation rate. Key-value pairs are deleted via the put/bput operations by setting the value to either None or the empty string. Once deleted, the key won't be reported during iteration. It also supports the del operation via 'del dkv.key'. The DAOS KV objects behave like a python dictionary and support: 'dkv[key]' which invokes 'dkv.get(key)' 'dkv[key] = val' which invokes 'dkv.put(key, val)' 'for key in dkv:' allows for walking through the key space via the support of python iterators 'if key is in dkv:' allows testing whether a given key is present in the DAOS KV store. 'len(dkv)' returns the number of key-value pairs. 'bool(dkv)' reports 'False' if there are no key-value pairs in the DAOS KV and 'True' otherwise. Python iterators are supported, which means that \"for key in kvobj:\" will allow you to walk through the key space. For each method, a PyDError exception is raised with a proper DAOS error code (in string format) if the operation cannot be completed. Both Python 2.7 and 3.x is supported. pydaos.raw \u00b6 The pydaos.raw submodule provides access to DAOS API functionality via Ctypes and was developed with an emphasis on test use cases. While the majority of unit tests are written in C, higher-level tests are written primarily using the Python API. Interfaces are provided for accessing DAOS management and DAOS API functionality from Python. This higher level interface allows a faster turnaround time on implementing test cases for DAOS. Layout \u00b6 The Python API is split into several files based on functionality: The Python object API: daos_api.py . The mapping of C structures to Python classes daos_cref.py High-level abstraction classes exist to manipulate DAOS storage: class DaosPool(object) class DaosContainer(object) class DaosObj(object) class IORequest(object) DaosPool is a Python class representing a DAOS pool. All pool-related functionality is exposed from this class. Operations such as creating/destroying a pool, connecting to a pool, and adding a target to a storage pool are supported. DaosContainer is a Python class representing a DAOS container. As with the DaosPool class, all container-related functionality is exposed here. This class also exposes abstracted wrapper functions for the flow of creating and committing an object to a DAOS container. DaosObj is a Python class representing a DAOS object. Functionality such as creating/deleting objects in a container, 'punching' objects (delete an object from the specified transaction only), and object query. IORequest is a Python class representing a read or write request against a DAOS object. Several classes exist for management purposes as well: class DaosContext(object) class DaosLog class DaosApiError(Exception) DaosContext is a wrapper around the DAOS libraries. It is initialized with the path where DAOS libraries can be found. DaosLog exposes functionality to write messages to the DAOS client log. DaosApiError is a custom exception class raised by the API internally in the event of a failed DAOS action. Most functions exposed in the DAOS C API support both synchronous and asynchronous execution, and the Python API exposes this same functionality. Each API takes an input event. DaosEvent is the Python representation of this event. If the input event is NULL , the call is synchronous. If an event is supplied, the function will return immediately after submitting API requests to the underlying stack, and the user can poll and query the event for completion. Ctypes \u00b6 Ctypes is a built-in Python module for interfacing Python with existing libraries written in C/C++. The Python API is built as an object-oriented wrapper around the DAOS libraries utilizing ctypes. Ctypes documentation can be found here https://docs.python.org/3/library/ctypes.html The following demonstrates a simplified example of creating a Python wrapper for the C function daos_pool_tgt_exclude_out , with each input parameter to the C function being cast via ctypes. This also demonstrates struct representation via ctypes: // daos_exclude.c #include <stdio.h> int daos_pool_tgt_exclude_out(const uuid_t uuid, const char *grp, struct d_tgt_list *tgts, daos_event_t *ev); All input parameters must be represented via ctypes. If a struct is required as an input parameter, a corresponding Python class can be created. For struct d_tgt_list : struct d_tgt_list { d_rank_t *tl_ranks; int32_t *tl_tgts; uint32_t tl_nr; }; class DTgtList(ctypes.Structure): _fields_ = [(\"tl_ranks\", ctypes.POINTER(ctypes.c_uint32)), (\"tl_tgts\", ctypes.POINTER(ctypes.c_int32)), (\"tl_nr\", ctypes.c_uint32)] The shared object containing daos_pool_tgt_exclude_out can then be imported and the function called directly: # api.py import ctypes import uuid import conversion # utility library to convert C <---> Python UUIDs # init python variables p_uuid = str(uuid.uuid4()) p_tgts = 2 p_ranks = DaosPool.__pylist_to_array([2]) # cast python variables via ctypes as necessary c_uuid = str_to_c_uuid(p_uuid) c_grp = ctypes.create_string_buffer(b\"daos_group_name\") c_tgt_list = ctypes.POINTER(DTgtList(p_ranks, p_tgts, 2))) # again, DTgtList must be passed as pointer # load the shared object my_lib = ctypes.CDLL('/full/path/to/daos_exclude.so') # now call it my_lib.daos_pool_tgt_exclude_out(c_uuid, c_grp, c_tgt_list, None) Error Handling \u00b6 The API was designed using the EAFP ( E asier to A sk F orgiveness than get P ermission) idiom. A given function will raise a custom exception on error state, DaosApiError . A user of the API is expected to catch and handle this exception as needed: # catch and log try: daos_some_action() except DaosApiError as e: self.d_log.ERROR(\"My DAOS action encountered an error!\") Logging \u00b6 The Python DAOS API exposes functionality to log messages to the DAOS client log. Messages can be logged as INFO , DEBUG , WARN , or ERR log levels. The DAOS log object must be initialized with the environmental context in which to run: from pydaos.raw import DaosLog self.d_log = DaosLog(self.context) self.d_log.INFO(\"FYI\") self.d_log.DEBUG(\"Debugging code\") self.d_log.WARNING(\"Be aware, may be issues\") self.d_log.ERROR(\"Something went very wrong\") Go Bindings \u00b6 API bindings for Go 2 are also available. https://github.com/daos-stack/daos/blob/master/src/client/pydaos/raw/README.md \u21a9 https://godoc.org/github.com/daos-stack/go-daos/pkg/daos \u21a9","title":"Native Programming Interface"},{"location":"user/interface/#native-programming-interface","text":"","title":"Native Programming Interface"},{"location":"user/interface/#building-against-the-daos-library","text":"To build an application or I/O middleware against the native DAOS API, include the daos.h header file in your program and link with -Ldaos . Examples are available under src/tests .","title":"Building against the DAOS library"},{"location":"user/interface/#daos-api-reference","text":"libdaos is written in C and uses Doxygen comments that are added to C header files. The Doxygen documentation is available here .","title":"DAOS API Reference"},{"location":"user/interface/#python-bindings","text":"A python module called PyDAOS provides the DAOS API to python users.","title":"Python Bindings"},{"location":"user/interface/#pydaos","text":"pydaos provides a native DAOS python interface exported by a C module. It integrates the DAOS key-value store API with python dictionaries. Only strings are supported for both the key and value for now. Key-value pairs can be inserted/looked up one at a time (see put/get) or in bulk (see bput/bget), taking a python dict as an input. The bulk operations are issued in parallel (up to 16 operations in flight) to maximize the operation rate. Key-value pairs are deleted via the put/bput operations by setting the value to either None or the empty string. Once deleted, the key won't be reported during iteration. It also supports the del operation via 'del dkv.key'. The DAOS KV objects behave like a python dictionary and support: 'dkv[key]' which invokes 'dkv.get(key)' 'dkv[key] = val' which invokes 'dkv.put(key, val)' 'for key in dkv:' allows for walking through the key space via the support of python iterators 'if key is in dkv:' allows testing whether a given key is present in the DAOS KV store. 'len(dkv)' returns the number of key-value pairs. 'bool(dkv)' reports 'False' if there are no key-value pairs in the DAOS KV and 'True' otherwise. Python iterators are supported, which means that \"for key in kvobj:\" will allow you to walk through the key space. For each method, a PyDError exception is raised with a proper DAOS error code (in string format) if the operation cannot be completed. Both Python 2.7 and 3.x is supported.","title":"pydaos"},{"location":"user/interface/#pydaosraw","text":"The pydaos.raw submodule provides access to DAOS API functionality via Ctypes and was developed with an emphasis on test use cases. While the majority of unit tests are written in C, higher-level tests are written primarily using the Python API. Interfaces are provided for accessing DAOS management and DAOS API functionality from Python. This higher level interface allows a faster turnaround time on implementing test cases for DAOS.","title":"pydaos.raw"},{"location":"user/interface/#layout","text":"The Python API is split into several files based on functionality: The Python object API: daos_api.py . The mapping of C structures to Python classes daos_cref.py High-level abstraction classes exist to manipulate DAOS storage: class DaosPool(object) class DaosContainer(object) class DaosObj(object) class IORequest(object) DaosPool is a Python class representing a DAOS pool. All pool-related functionality is exposed from this class. Operations such as creating/destroying a pool, connecting to a pool, and adding a target to a storage pool are supported. DaosContainer is a Python class representing a DAOS container. As with the DaosPool class, all container-related functionality is exposed here. This class also exposes abstracted wrapper functions for the flow of creating and committing an object to a DAOS container. DaosObj is a Python class representing a DAOS object. Functionality such as creating/deleting objects in a container, 'punching' objects (delete an object from the specified transaction only), and object query. IORequest is a Python class representing a read or write request against a DAOS object. Several classes exist for management purposes as well: class DaosContext(object) class DaosLog class DaosApiError(Exception) DaosContext is a wrapper around the DAOS libraries. It is initialized with the path where DAOS libraries can be found. DaosLog exposes functionality to write messages to the DAOS client log. DaosApiError is a custom exception class raised by the API internally in the event of a failed DAOS action. Most functions exposed in the DAOS C API support both synchronous and asynchronous execution, and the Python API exposes this same functionality. Each API takes an input event. DaosEvent is the Python representation of this event. If the input event is NULL , the call is synchronous. If an event is supplied, the function will return immediately after submitting API requests to the underlying stack, and the user can poll and query the event for completion.","title":"Layout"},{"location":"user/interface/#ctypes","text":"Ctypes is a built-in Python module for interfacing Python with existing libraries written in C/C++. The Python API is built as an object-oriented wrapper around the DAOS libraries utilizing ctypes. Ctypes documentation can be found here https://docs.python.org/3/library/ctypes.html The following demonstrates a simplified example of creating a Python wrapper for the C function daos_pool_tgt_exclude_out , with each input parameter to the C function being cast via ctypes. This also demonstrates struct representation via ctypes: // daos_exclude.c #include <stdio.h> int daos_pool_tgt_exclude_out(const uuid_t uuid, const char *grp, struct d_tgt_list *tgts, daos_event_t *ev); All input parameters must be represented via ctypes. If a struct is required as an input parameter, a corresponding Python class can be created. For struct d_tgt_list : struct d_tgt_list { d_rank_t *tl_ranks; int32_t *tl_tgts; uint32_t tl_nr; }; class DTgtList(ctypes.Structure): _fields_ = [(\"tl_ranks\", ctypes.POINTER(ctypes.c_uint32)), (\"tl_tgts\", ctypes.POINTER(ctypes.c_int32)), (\"tl_nr\", ctypes.c_uint32)] The shared object containing daos_pool_tgt_exclude_out can then be imported and the function called directly: # api.py import ctypes import uuid import conversion # utility library to convert C <---> Python UUIDs # init python variables p_uuid = str(uuid.uuid4()) p_tgts = 2 p_ranks = DaosPool.__pylist_to_array([2]) # cast python variables via ctypes as necessary c_uuid = str_to_c_uuid(p_uuid) c_grp = ctypes.create_string_buffer(b\"daos_group_name\") c_tgt_list = ctypes.POINTER(DTgtList(p_ranks, p_tgts, 2))) # again, DTgtList must be passed as pointer # load the shared object my_lib = ctypes.CDLL('/full/path/to/daos_exclude.so') # now call it my_lib.daos_pool_tgt_exclude_out(c_uuid, c_grp, c_tgt_list, None)","title":"Ctypes"},{"location":"user/interface/#error-handling","text":"The API was designed using the EAFP ( E asier to A sk F orgiveness than get P ermission) idiom. A given function will raise a custom exception on error state, DaosApiError . A user of the API is expected to catch and handle this exception as needed: # catch and log try: daos_some_action() except DaosApiError as e: self.d_log.ERROR(\"My DAOS action encountered an error!\")","title":"Error Handling"},{"location":"user/interface/#logging","text":"The Python DAOS API exposes functionality to log messages to the DAOS client log. Messages can be logged as INFO , DEBUG , WARN , or ERR log levels. The DAOS log object must be initialized with the environmental context in which to run: from pydaos.raw import DaosLog self.d_log = DaosLog(self.context) self.d_log.INFO(\"FYI\") self.d_log.DEBUG(\"Debugging code\") self.d_log.WARNING(\"Be aware, may be issues\") self.d_log.ERROR(\"Something went very wrong\")","title":"Logging"},{"location":"user/interface/#go-bindings","text":"API bindings for Go 2 are also available. https://github.com/daos-stack/daos/blob/master/src/client/pydaos/raw/README.md \u21a9 https://godoc.org/github.com/daos-stack/go-daos/pkg/daos \u21a9","title":"Go Bindings"},{"location":"user/mpi-io/","text":"MPI-IO Support \u00b6 The Message Passing Interface (MPI) Standard, maintained by the MPI Forum , includes a chapter on MPI-IO. ROMIO is a well-known implementation of MPI-IO and is included in many MPI implementations. DAOS provides its own MPI-IO ROMIO ADIO driver. This driver has been merged in the upstream MPICH repository, see https://github.com/pmodels/mpich/tree/main/src/mpi/romio/adio/ad_daos for details. Note Starting with DAOS 1.2, the --svc parameter (number of service replicas) is no longer needed, and the DAOS API has been changed accordingly Patches have been contributed to MPICH that detect the DAOS API version to gracefully handle this change, but those patches have not yet been picked up in the MPI releases below. For details check the latest commits here . Supported MPI Version \u00b6 MPICH \u00b6 The DAOS ROMIO ADIO driver has been accepted into MPICH . It is included in mpich-3.4.1 (released Jan 2021) . To build MPICH, including ROMIO with the DAOS ADIO driver: export MPI_LIB=\"\" git clone https://github.com/pmodels/mpich cd mpich ./autogen.sh ./configure --prefix=dir --enable-fortran=all --enable-romio \\ --enable-cxx --enable-g=all --enable-debuginfo --with-device=ch3:nemesis \\ --with-file-system=ufs+daos --with-daos=/usr make -j8; make install This assumes that DAOS is installed into the /usr tree, which is the case for the DAOS RPM installation. Other configure options can be added, modified, or removed as needed, like the network communicatio device, fortran support, etc. For those, please consule the mpich user guide. Set the PATH and LD_LIBRARY_PATH to where you want to build your client apps or libs that use MPI to the path of the installed MPICH. Intel MPI \u00b6 The Intel MPI Library includes DAOS support since the 2019.8 release . Note that Intel MPI uses libfabric (both 2019.8 and 2019.9 use libfabric-1.10.1-impi ). Care must be taken to ensure that the version of libfabric that is used is at a level that includes the patches that are critical for DAOS. DAOS 1.0.1 includes libfabric-1.9.0 , and the DAOS 1.2 and 2.0 releases includes libfabric-1.12 . To use DAOS 1.1 with Intel MPI 2019.8 or 2019.9, the libfabric that is supplied by DAOS (and that is installed into /usr/lib64 by default) needs to be used by listing it first in the library search path: export LD_LIBRARY_PATH=\"/usr/lib64/:$LD_LIBRARY_PATH\" There are other environment variables that need to be set on the client side to ensure proper functionality with the DAOS MPIIO driver and those include: export I_MPI_OFI_LIBRARY_INTERNAL=0 export FI_OFI_RXM_USE_SRX=1 Open MPI \u00b6 Open MPI 4.0.5 does not yet provide DAOS support. Since one of its MPI-IO implementations is based on ROMIO, it will likely pick up DAOS support in an upcoming release. MVAPICH2 \u00b6 MVAPICH2 2.3.4 does not yet provide DAOS support. Since its MPI-IO implementation is based on ROMIO, it will likely pick up DAOS support in an upcoming release. Testing MPI-IO with DAOS \u00b6 Build any client (HDF5, ior, mpi test suites) normally with the mpicc command and mpich library installed above (see child pages). To run an example with MPI-IO: Create a DAOS pool on the DAOS server(s). This will return a pool uuid \"puuid\". Create a POSIX type container: daos cont create <pool_label> --type=POSIX This will return a container uuid \"cuuid\". At the client side, the following environment variables need to be set: export DAOS_POOL=puuid; export DAOS_CONT=cuuid; export DAOS_BYPASS_DUNS=1 . The pool and container UUID can be retrieved via daos pool/cont query Alternatively, the unified namespace mode can be used instead. Run the client application or test. MPI-IO applications should work seamlessly by just prepending daos: to the filename/path to use the DAOS ADIO driver. Known limitations \u00b6 Limitations of the current implementation include: No support for MPI file atomicity, preallocate, or shared file pointers.","title":"MPI-IO Support"},{"location":"user/mpi-io/#mpi-io-support","text":"The Message Passing Interface (MPI) Standard, maintained by the MPI Forum , includes a chapter on MPI-IO. ROMIO is a well-known implementation of MPI-IO and is included in many MPI implementations. DAOS provides its own MPI-IO ROMIO ADIO driver. This driver has been merged in the upstream MPICH repository, see https://github.com/pmodels/mpich/tree/main/src/mpi/romio/adio/ad_daos for details. Note Starting with DAOS 1.2, the --svc parameter (number of service replicas) is no longer needed, and the DAOS API has been changed accordingly Patches have been contributed to MPICH that detect the DAOS API version to gracefully handle this change, but those patches have not yet been picked up in the MPI releases below. For details check the latest commits here .","title":"MPI-IO Support"},{"location":"user/mpi-io/#supported-mpi-version","text":"","title":"Supported MPI Version"},{"location":"user/mpi-io/#mpich","text":"The DAOS ROMIO ADIO driver has been accepted into MPICH . It is included in mpich-3.4.1 (released Jan 2021) . To build MPICH, including ROMIO with the DAOS ADIO driver: export MPI_LIB=\"\" git clone https://github.com/pmodels/mpich cd mpich ./autogen.sh ./configure --prefix=dir --enable-fortran=all --enable-romio \\ --enable-cxx --enable-g=all --enable-debuginfo --with-device=ch3:nemesis \\ --with-file-system=ufs+daos --with-daos=/usr make -j8; make install This assumes that DAOS is installed into the /usr tree, which is the case for the DAOS RPM installation. Other configure options can be added, modified, or removed as needed, like the network communicatio device, fortran support, etc. For those, please consule the mpich user guide. Set the PATH and LD_LIBRARY_PATH to where you want to build your client apps or libs that use MPI to the path of the installed MPICH.","title":"MPICH"},{"location":"user/mpi-io/#intel-mpi","text":"The Intel MPI Library includes DAOS support since the 2019.8 release . Note that Intel MPI uses libfabric (both 2019.8 and 2019.9 use libfabric-1.10.1-impi ). Care must be taken to ensure that the version of libfabric that is used is at a level that includes the patches that are critical for DAOS. DAOS 1.0.1 includes libfabric-1.9.0 , and the DAOS 1.2 and 2.0 releases includes libfabric-1.12 . To use DAOS 1.1 with Intel MPI 2019.8 or 2019.9, the libfabric that is supplied by DAOS (and that is installed into /usr/lib64 by default) needs to be used by listing it first in the library search path: export LD_LIBRARY_PATH=\"/usr/lib64/:$LD_LIBRARY_PATH\" There are other environment variables that need to be set on the client side to ensure proper functionality with the DAOS MPIIO driver and those include: export I_MPI_OFI_LIBRARY_INTERNAL=0 export FI_OFI_RXM_USE_SRX=1","title":"Intel MPI"},{"location":"user/mpi-io/#open-mpi","text":"Open MPI 4.0.5 does not yet provide DAOS support. Since one of its MPI-IO implementations is based on ROMIO, it will likely pick up DAOS support in an upcoming release.","title":"Open MPI"},{"location":"user/mpi-io/#mvapich2","text":"MVAPICH2 2.3.4 does not yet provide DAOS support. Since its MPI-IO implementation is based on ROMIO, it will likely pick up DAOS support in an upcoming release.","title":"MVAPICH2"},{"location":"user/mpi-io/#testing-mpi-io-with-daos","text":"Build any client (HDF5, ior, mpi test suites) normally with the mpicc command and mpich library installed above (see child pages). To run an example with MPI-IO: Create a DAOS pool on the DAOS server(s). This will return a pool uuid \"puuid\". Create a POSIX type container: daos cont create <pool_label> --type=POSIX This will return a container uuid \"cuuid\". At the client side, the following environment variables need to be set: export DAOS_POOL=puuid; export DAOS_CONT=cuuid; export DAOS_BYPASS_DUNS=1 . The pool and container UUID can be retrieved via daos pool/cont query Alternatively, the unified namespace mode can be used instead. Run the client application or test. MPI-IO applications should work seamlessly by just prepending daos: to the filename/path to use the DAOS ADIO driver.","title":"Testing MPI-IO with DAOS"},{"location":"user/mpi-io/#known-limitations","text":"Limitations of the current implementation include: No support for MPI file atomicity, preallocate, or shared file pointers.","title":"Known limitations"},{"location":"user/posix/","text":"POSIX Namespace \u00b6 A regular POSIX namespace can be encapsulated into a DAOS container. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed directly to applications or I/O frameworks (e.g., for frameworks like Spark or TensorFlow, or benchmarks like IOR or mdtest that support different storage backend plugins). It can also be exposed transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottlenecks by delivering full OS bypass for POSIX read/write operations. The performance is going to be best generally when using the DFS API directly. Using the IO interception library with dfuse should yield the same performance for IO operations (read/write) as the DFS API with minimal overhead. Performance of metadata operations (file creation, deletion, rename, etc.) over dfuse will be much slower than the DFS API since there is no interception to bypass the fuse/kernel layer. libdfs \u00b6 The DAOS File System (DFS) is implemented in the libdfs library, and allows a DAOS container to be accessed as a hierarchical POSIX namespace. libdfs supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and are not implemented on a per-file or per-directory basis. The DFS API closely represents the POSIX API. The API includes operations to: * Mount: create/open superblock and root object * Un-mount: release open handles * Lookup: traverse a path and return an open file/dir handle * IO: read & write with an iovec * Stat: retrieve attributes of an entry * Mkdir: create a dir * Readdir: enumerate all entries under a directory * Open: create/Open a file/dir * Remove: unlink a file/dir * Move: rename * Release: close an open handle of a file/dir * Extended Attributes: set, get, list, remove The following features from POSIX will not be supported: * Hard links * mmap support with MAP_SHARED will be consistent from single client only. Note that this is supported through DFUSE only (i.e. not through the DFS API). * Char devices, block devices, sockets and pipes * User/group quotas * setuid(), setgid() programs, supplementary groups, ACLs are not supported within the DFS namespace. * [access/change/modify] time not updated appropriately, potentially on close only. * Flock (maybe at dfuse local node level only) * Block size in stat buf is not accurate (no account for holes, extended attributes) * Various parameters reported via statfs like number of blocks, files, free/available space * POSIX permissions inside an encapsulated namespace * Still enforced at the DAOS pool/container level * Effectively means that all files belong to the same \u201cproject\u201d It is possible to use libdfs in a parallel application from multiple nodes. DFS provides two modes that offer different levels of consistency. The modes can be set on container creation time: 1) Relaxed mode for well-behaved applications that generate conflict-free operations for which a very high level of concurrency will be supported. 2) Balanced mode for applications that require stricter consistency at the cost of performance. This mode is currently not fully supported and DFS by default will use the relaxed mode. On container access, if the container is created with balanced mode, it can be accessed in balanced mode only. If the container was created with relaxed mode, it can be accessed in relaxed or balanced mode. In either mode, there is a consistency semantic issue that is not properly handled: Open-unlink semantics: This occurs when a client obtains an open handle on an object (file or directory), and accesses that object (reads/writes data or create other files), while another client removes that object that the other client has opened from under it. In DAOS, we don\u2019t track object open handles as that would be very expensive, and so in such conflicting cases, the worst case scenario is the lost/leaked space that is written to those orphan objects that have been unlinked from the namespace. Other consistency issues are handled differently between the two consistency mode: Same Operation Executed Concurrently (Supported in both Relaxed and Balanced Mode): For example, clients try to create or remove the same file concurrently, one should succeed and others will fail. Create/Unlink/Rename Conflicts (Supported in Balanced Mode only): For example, a client renames a file, but another unlinks the old file at the same time. Operation Atomicity (Supported only in Balanced mode): If a client crashes in the middle of the rename, the state of the container should be consistent as if the operation never happened. Visibility (Supported in Balanced and Relaxed mode): A write from one client should be visible to another client with a simple coordination between the clients. DFuse \u00b6 DFuse provides DAOS File System access through the standard libc/kernel/VFS POSIX infrastructure. This allows existing applications to use DAOS without modification, and provides a path to upgrade those applications to native DAOS support. Additionally, DFuse provides an Interception Library libioil to transparently allow POSIX clients to talk directly to DAOS servers, providing OS-Bypass for I/O without modifying or recompiling of the application. DFuse builds heavily on DFS. Data written via DFuse can be accessed by DFS and vice versa. DFuse Daemon \u00b6 The dfuse daemon runs a single instance per node to provide a user POSIX access to DAOS. It should be run with the credentials of the user, and typically will be started and stopped on each compute node as part of the prolog and epilog scripts of any resource manager or scheduler in use. One DFuse daemon per node can process requests for multiple clients. A single DFuse instance can provide access to multiple pools and containers concurrently, or can be limited to a single pool, or a single container. Restrictions \u00b6 DFuse is limited to a single user. Access to the filesystem from other users, including root, will not be honored. As a consequence of this, the chown and chgrp calls are not supported. Hard links and special device files, except symbolic links, are not supported, nor are any ACLs. DFuse can run in the foreground, keeping the terminal window open, or it can daemonize to run like a system daemon. However, to do this and still be able to access DAOS it needs to daemonize before calling daos_init() . This in turns means it cannot report some kinds of startup errors either on stdout/stderr or via its return code. When initially starting with DFuse it is recommended to run in foreground mode ( --foreground ) to better observe any failures. Inodes are managed on the local node by DFuse. So while inode numbers will be consistent on a node for the duration of the session, they are not guaranteed to be consistent across restarts of DFuse or across nodes. It is not possible to see pool/container listings through DFuse. So if readdir , ls or others are used, DFuse will return ENOTSUP . Launching \u00b6 DFuse should be run with the credentials (user/group) of the user who will be accessing it, and who owns any pools that will be used. There are two mandatory command-line options, these are: Command-line Option Description --mountpoint=<path> path to mount dfuse The mount point specified should be an empty directory on the local node that is owned by the user. Additionally, there are several optional command-line options: Command-line Option Description --pool=<uuid> pool uuid to connect to --container=<uuid> container uuid to open --sys-name=<name> DAOS system name --foreground run in foreground --singlethreaded run single threaded When DFuse starts, it will register a single mount with the kernel, at the location specified by the --mountpoint option. This mount will be visible in /proc/mounts , and possibly in the output of df . The contents of multiple pools/containers will be accessible via this single kernel mountpoint. Pool/Container Paths \u00b6 DFuse will only create one kernel level mount point regardless of how it is launched. How POSIX containers are represented within that mount point varies depending on the DFuse command-line options: If both a pool uuid and a container uuid are specified on the command line, then the mount point will map to the root of the container itself. Files can be accessed by simply concatenating the mount point and the name of the file, relative to the root of the container. If neither a pool or container is specified, then pools and container can be accessed by the path <mount point>/<pool uuid>/<container uuid> . However it should be noted that readdir() and therefore ls do not work on either mount points or directories representing pools here. So the pool and container uuids will have to be provided from an external source. If a pool uuid is specified but not a container uuid, then the containers can be accessed by the path <mount point>/<container uuid> . The container uuid will have to be provided from an external source. It is anticipated that in most cases, both pool uuid and container uuid will be used, so the mount point itself will map directly onto a POSIX container. Links into other Containers \u00b6 It is possible to link to other containers in DFuse, where subdirectories within a container resolve not to regular directories, but rather to the root of entirely different POSIX containers. To create a new container and link it into the namespace of an existing one, use the following command. $ daos container create --type POSIX --pool <pool uuid> --path <path to entry point> The pool uuid should already exist, and the path should specify a location somewhere within a DFuse mount point that resolves to a POSIX container. Once a link is created, it can be accessed through the new path. Following the link is virtually transparent. No container uuid is required. If one is not supplied, it will be created. To destroy a container again, the following command should be used. $ daos container destroy --path <path to entry point> This will both remove the link between the containers and remove the container that was linked to. There is no support for adding links to already existing containers or removing links to containers without also removing the container itself. Information about a container, for example, the presence of an entry point between containers, or the pool and container uuids of the container linked to can be read with the following command. $ daos container info --path <path to entry point> Caching \u00b6 For performance reasons caching will be enabled by default in DFuse, including both data and metadata caching. It is possible to tune these settings both at a high level on the DFuse command line and fine grained control via container attributes. The following types of data will be cached by default. Kernel caching of dentries Kernel caching of negative dentries Kernel caching of inodes (file sizes, permissions etc) Kernel caching of file contents Readahead in dfuse and inserting data into kernel cache MMAP write optimization !note Caching is enabled by default in dfuse. This might cause some parallel applications to fail. Please disable caching if you experience this or want up to date data sharing between nodes. To selectively control caching within a container the following container attributes should be used, if any attribute is set then the rest are assumed to be set to 0 or off, except dentry-dir-time which defaults to dentry-time Attribute name Description dfuse-attr-time How long file attributes are cached dfuse-dentry-time How long directory entries are cached dfuse-dentry-dir-time How long dentries are cached, if the entry is itself a directory dfuse-ndentry-time How long negative dentries are cached dfuse-data-cache Data caching enabled for this file (\"on\"/\"off\") dfuse-direct-io-disable Force use of page cache for this container (\"on\"/\"off\") For metadata caching attributes specify the duration that the cache should be valid for, specified in seconds, and allowing 'S' or 'M' suffix. dfuse-data-cache should be set to \"on\", or \"off\" if set, any other value will log an error, and result in the cache being off. The O_DIRECT flag for open files will be honoured with this option enabled, files which do not set O_DIRECT will be cached. dfuse-direct-io-disable will enable data caching, similar to dfuse-data-cache, however if this is set to \"on\" then the O_DIRECT flag will be ignored, and all files will use the page cache. This default value for this is \"off\". With no options specified attr and dentry timeouts will be 1 second, dentry-dir and ndentry timeouts will be 5 seconds, and data caching will be enabled. These are two command line options to control the DFuse process itself. **Command line option Description --disable-caching Disables all caching --disable-wb-caching Disables write-back cache These will affect all containers accessed via DFuse, regardless of any container attributes. Stopping DFuse \u00b6 When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos When this is done, the local DFuse daemon should shut down the mount point, disconnect from the DAOS servers, and exit. You can also verify that the mount point is no longer listed in /proc/mounts . Interception Library \u00b6 An interception library called libioil is available to work with DFuse. This library works in conjunction with DFuse and allows the interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any application changes. This provides kernel-bypass for I/O data, leading to improved performance. To use this, set LD_PRELOAD to point to the shared library in the DAOS install directory: LD_PRELOAD=/path/to/daos/install/lib/libioil.so LD_PRELOAD=/usr/lib64/libioil.so # when installed from RPMs","title":"POSIX Namespace"},{"location":"user/posix/#posix-namespace","text":"A regular POSIX namespace can be encapsulated into a DAOS container. This capability is provided by the libdfs library that implements the file and directory abstractions over the native libdaos library. The POSIX emulation can be exposed directly to applications or I/O frameworks (e.g., for frameworks like Spark or TensorFlow, or benchmarks like IOR or mdtest that support different storage backend plugins). It can also be exposed transparently via a FUSE daemon, combined optionally with an interception library to address some of the FUSE performance bottlenecks by delivering full OS bypass for POSIX read/write operations. The performance is going to be best generally when using the DFS API directly. Using the IO interception library with dfuse should yield the same performance for IO operations (read/write) as the DFS API with minimal overhead. Performance of metadata operations (file creation, deletion, rename, etc.) over dfuse will be much slower than the DFS API since there is no interception to bypass the fuse/kernel layer.","title":"POSIX Namespace"},{"location":"user/posix/#libdfs","text":"The DAOS File System (DFS) is implemented in the libdfs library, and allows a DAOS container to be accessed as a hierarchical POSIX namespace. libdfs supports files, directories, and symbolic links, but not hard links. Access permissions are inherited from the parent pool and are not implemented on a per-file or per-directory basis. The DFS API closely represents the POSIX API. The API includes operations to: * Mount: create/open superblock and root object * Un-mount: release open handles * Lookup: traverse a path and return an open file/dir handle * IO: read & write with an iovec * Stat: retrieve attributes of an entry * Mkdir: create a dir * Readdir: enumerate all entries under a directory * Open: create/Open a file/dir * Remove: unlink a file/dir * Move: rename * Release: close an open handle of a file/dir * Extended Attributes: set, get, list, remove The following features from POSIX will not be supported: * Hard links * mmap support with MAP_SHARED will be consistent from single client only. Note that this is supported through DFUSE only (i.e. not through the DFS API). * Char devices, block devices, sockets and pipes * User/group quotas * setuid(), setgid() programs, supplementary groups, ACLs are not supported within the DFS namespace. * [access/change/modify] time not updated appropriately, potentially on close only. * Flock (maybe at dfuse local node level only) * Block size in stat buf is not accurate (no account for holes, extended attributes) * Various parameters reported via statfs like number of blocks, files, free/available space * POSIX permissions inside an encapsulated namespace * Still enforced at the DAOS pool/container level * Effectively means that all files belong to the same \u201cproject\u201d It is possible to use libdfs in a parallel application from multiple nodes. DFS provides two modes that offer different levels of consistency. The modes can be set on container creation time: 1) Relaxed mode for well-behaved applications that generate conflict-free operations for which a very high level of concurrency will be supported. 2) Balanced mode for applications that require stricter consistency at the cost of performance. This mode is currently not fully supported and DFS by default will use the relaxed mode. On container access, if the container is created with balanced mode, it can be accessed in balanced mode only. If the container was created with relaxed mode, it can be accessed in relaxed or balanced mode. In either mode, there is a consistency semantic issue that is not properly handled: Open-unlink semantics: This occurs when a client obtains an open handle on an object (file or directory), and accesses that object (reads/writes data or create other files), while another client removes that object that the other client has opened from under it. In DAOS, we don\u2019t track object open handles as that would be very expensive, and so in such conflicting cases, the worst case scenario is the lost/leaked space that is written to those orphan objects that have been unlinked from the namespace. Other consistency issues are handled differently between the two consistency mode: Same Operation Executed Concurrently (Supported in both Relaxed and Balanced Mode): For example, clients try to create or remove the same file concurrently, one should succeed and others will fail. Create/Unlink/Rename Conflicts (Supported in Balanced Mode only): For example, a client renames a file, but another unlinks the old file at the same time. Operation Atomicity (Supported only in Balanced mode): If a client crashes in the middle of the rename, the state of the container should be consistent as if the operation never happened. Visibility (Supported in Balanced and Relaxed mode): A write from one client should be visible to another client with a simple coordination between the clients.","title":"libdfs"},{"location":"user/posix/#dfuse","text":"DFuse provides DAOS File System access through the standard libc/kernel/VFS POSIX infrastructure. This allows existing applications to use DAOS without modification, and provides a path to upgrade those applications to native DAOS support. Additionally, DFuse provides an Interception Library libioil to transparently allow POSIX clients to talk directly to DAOS servers, providing OS-Bypass for I/O without modifying or recompiling of the application. DFuse builds heavily on DFS. Data written via DFuse can be accessed by DFS and vice versa.","title":"DFuse"},{"location":"user/posix/#dfuse-daemon","text":"The dfuse daemon runs a single instance per node to provide a user POSIX access to DAOS. It should be run with the credentials of the user, and typically will be started and stopped on each compute node as part of the prolog and epilog scripts of any resource manager or scheduler in use. One DFuse daemon per node can process requests for multiple clients. A single DFuse instance can provide access to multiple pools and containers concurrently, or can be limited to a single pool, or a single container.","title":"DFuse Daemon"},{"location":"user/posix/#restrictions","text":"DFuse is limited to a single user. Access to the filesystem from other users, including root, will not be honored. As a consequence of this, the chown and chgrp calls are not supported. Hard links and special device files, except symbolic links, are not supported, nor are any ACLs. DFuse can run in the foreground, keeping the terminal window open, or it can daemonize to run like a system daemon. However, to do this and still be able to access DAOS it needs to daemonize before calling daos_init() . This in turns means it cannot report some kinds of startup errors either on stdout/stderr or via its return code. When initially starting with DFuse it is recommended to run in foreground mode ( --foreground ) to better observe any failures. Inodes are managed on the local node by DFuse. So while inode numbers will be consistent on a node for the duration of the session, they are not guaranteed to be consistent across restarts of DFuse or across nodes. It is not possible to see pool/container listings through DFuse. So if readdir , ls or others are used, DFuse will return ENOTSUP .","title":"Restrictions"},{"location":"user/posix/#launching","text":"DFuse should be run with the credentials (user/group) of the user who will be accessing it, and who owns any pools that will be used. There are two mandatory command-line options, these are: Command-line Option Description --mountpoint=<path> path to mount dfuse The mount point specified should be an empty directory on the local node that is owned by the user. Additionally, there are several optional command-line options: Command-line Option Description --pool=<uuid> pool uuid to connect to --container=<uuid> container uuid to open --sys-name=<name> DAOS system name --foreground run in foreground --singlethreaded run single threaded When DFuse starts, it will register a single mount with the kernel, at the location specified by the --mountpoint option. This mount will be visible in /proc/mounts , and possibly in the output of df . The contents of multiple pools/containers will be accessible via this single kernel mountpoint.","title":"Launching"},{"location":"user/posix/#poolcontainer-paths","text":"DFuse will only create one kernel level mount point regardless of how it is launched. How POSIX containers are represented within that mount point varies depending on the DFuse command-line options: If both a pool uuid and a container uuid are specified on the command line, then the mount point will map to the root of the container itself. Files can be accessed by simply concatenating the mount point and the name of the file, relative to the root of the container. If neither a pool or container is specified, then pools and container can be accessed by the path <mount point>/<pool uuid>/<container uuid> . However it should be noted that readdir() and therefore ls do not work on either mount points or directories representing pools here. So the pool and container uuids will have to be provided from an external source. If a pool uuid is specified but not a container uuid, then the containers can be accessed by the path <mount point>/<container uuid> . The container uuid will have to be provided from an external source. It is anticipated that in most cases, both pool uuid and container uuid will be used, so the mount point itself will map directly onto a POSIX container.","title":"Pool/Container Paths"},{"location":"user/posix/#links-into-other-containers","text":"It is possible to link to other containers in DFuse, where subdirectories within a container resolve not to regular directories, but rather to the root of entirely different POSIX containers. To create a new container and link it into the namespace of an existing one, use the following command. $ daos container create --type POSIX --pool <pool uuid> --path <path to entry point> The pool uuid should already exist, and the path should specify a location somewhere within a DFuse mount point that resolves to a POSIX container. Once a link is created, it can be accessed through the new path. Following the link is virtually transparent. No container uuid is required. If one is not supplied, it will be created. To destroy a container again, the following command should be used. $ daos container destroy --path <path to entry point> This will both remove the link between the containers and remove the container that was linked to. There is no support for adding links to already existing containers or removing links to containers without also removing the container itself. Information about a container, for example, the presence of an entry point between containers, or the pool and container uuids of the container linked to can be read with the following command. $ daos container info --path <path to entry point>","title":"Links into other Containers"},{"location":"user/posix/#caching","text":"For performance reasons caching will be enabled by default in DFuse, including both data and metadata caching. It is possible to tune these settings both at a high level on the DFuse command line and fine grained control via container attributes. The following types of data will be cached by default. Kernel caching of dentries Kernel caching of negative dentries Kernel caching of inodes (file sizes, permissions etc) Kernel caching of file contents Readahead in dfuse and inserting data into kernel cache MMAP write optimization !note Caching is enabled by default in dfuse. This might cause some parallel applications to fail. Please disable caching if you experience this or want up to date data sharing between nodes. To selectively control caching within a container the following container attributes should be used, if any attribute is set then the rest are assumed to be set to 0 or off, except dentry-dir-time which defaults to dentry-time Attribute name Description dfuse-attr-time How long file attributes are cached dfuse-dentry-time How long directory entries are cached dfuse-dentry-dir-time How long dentries are cached, if the entry is itself a directory dfuse-ndentry-time How long negative dentries are cached dfuse-data-cache Data caching enabled for this file (\"on\"/\"off\") dfuse-direct-io-disable Force use of page cache for this container (\"on\"/\"off\") For metadata caching attributes specify the duration that the cache should be valid for, specified in seconds, and allowing 'S' or 'M' suffix. dfuse-data-cache should be set to \"on\", or \"off\" if set, any other value will log an error, and result in the cache being off. The O_DIRECT flag for open files will be honoured with this option enabled, files which do not set O_DIRECT will be cached. dfuse-direct-io-disable will enable data caching, similar to dfuse-data-cache, however if this is set to \"on\" then the O_DIRECT flag will be ignored, and all files will use the page cache. This default value for this is \"off\". With no options specified attr and dentry timeouts will be 1 second, dentry-dir and ndentry timeouts will be 5 seconds, and data caching will be enabled. These are two command line options to control the DFuse process itself. **Command line option Description --disable-caching Disables all caching --disable-wb-caching Disables write-back cache These will affect all containers accessed via DFuse, regardless of any container attributes.","title":"Caching"},{"location":"user/posix/#stopping-dfuse","text":"When done, the file system can be unmounted via fusermount: $ fusermount3 -u /tmp/daos When this is done, the local DFuse daemon should shut down the mount point, disconnect from the DAOS servers, and exit. You can also verify that the mount point is no longer listed in /proc/mounts .","title":"Stopping DFuse"},{"location":"user/posix/#interception-library","text":"An interception library called libioil is available to work with DFuse. This library works in conjunction with DFuse and allows the interception of POSIX I/O calls and issue the I/O operations directly from the application context through libdaos without any application changes. This provides kernel-bypass for I/O data, leading to improved performance. To use this, set LD_PRELOAD to point to the shared library in the DAOS install directory: LD_PRELOAD=/path/to/daos/install/lib/libioil.so LD_PRELOAD=/usr/lib64/libioil.so # when installed from RPMs","title":"Interception Library"},{"location":"user/spark/","text":"DAOS Hadoop Filesystem \u00b6 Here, we describe the quick steps required to use the DAOS Hadoop filesystem to access DAOS from Hadoop and Spark. Prerequisites \u00b6 Linux OS Java 8 Hadoop 2.7 or later Spark 3.1 or later DAOS Readiness We assume that the DAOS servers and agents have already been deployed in the environment. Otherwise, they can be deployed by following the DAOS Installation Guide . Maven Download \u00b6 There are two artifacts to download, daos-java and hadoop-daos, from maven. Here are maven dependencies. You can download them with below commands if you have maven installed. mvn dependency:get -Dartifact=io.daos:daos-java:<version> -Ddest=./ mvn dependency:get -Dartifact=io.daos:hadoop-daos:<version> -Ddest=./ Or search these artifacts from maven central(https://search.maven.org) and download them manually. You can also build artifacts by yourself. see Build DAOS Hadoop Filesystem for details. Deployment \u00b6 JAR Files \u00b6 daos-java-<version>.jar and hadoop-daos-<version>.jar need to be deployed on every compute node that runs Spark or Hadoop. Place them in a directory, e.g., $SPARK_HOME/jars for Spark and $HADOOP_HOME/share/hadoop/common/lib for Hadoop, which is accessible to all the nodes or copy them to every node. daos-site-example.xml \u00b6 Extract from hadoop-daos-<version>.jar and rename to daos-site.xml . Then copy it to your application config directory, e.g., $SPARK_HOME/conf for Spark and $HADOOP_HOME/etc/hadoop for Hadoop. Configuring Hadoop \u00b6 Environment Variable \u00b6 Export all DAOS-specific env variables in your application, e.g., spark-env.sh for Spark and hadoop-env.sh for Hadoop. Or you can simply put env variables in your .bashrc . Besides, you should have LD_LIBRARY_PATH include DAOS library path so that Java can link to DAOS libs, like below. $ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<DAOS_INSTALL>/lib64:<DAOS_INSTALL>/lib DAOS URI \u00b6 In daos-site.xml , we default the DAOS URI as simplest form, \"daos:///\". For other form of URIs, please check DAOS More URIs . If the DAOS pool and container have not been created, we can use the following command to create them and get the pool UUID and container UUID. $ dmg pool create --scm-size=<scm size> --nvme-size=<nvme size> $ daos cont create --pool <pool UUID> --type POSIX After that, configure daos-site.xml with the pool and container created. <configuration> ... <property> <name>fs.daos.pool.uuid</name> <value>your pool UUID</value> <description>UUID of DAOS pool</description> </property> <property> <name>fs.daos.container.uuid</name> <value>your container UUID</value> <description>UUID of DAOS container created with \"--type posix\"</description> </property> ... </configuration> Please put daos-site.xml in right place, e.g., Java classpath, and loadable by Hadoop DAOS FileSystem. Validating Hadoop Access \u00b6 If everything goes well, you should see /user directory being listed after issuing below command. $ hadoop fs -ls / You can also play around with other Hadoop commands, like -copyFromLocal and -copyToLocal. You can also start Yarn and run some mapreduce jobs on Yarn. See Run Map-Reduce in Hadoop Configuring Spark \u00b6 To access DAOS Hadoop filesystem in Spark, add the jar files to the classpath of the Spark executor and driver. This can be configured in Spark's configuration file spark-defaults.conf . spark.executor.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar spark.driver.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar Validatng Spark Access \u00b6 All Spark APIs that work with the Hadoop filesystem will work with DAOS. We can use the daos:/// URI to access files stored in DAOS. For example, to read the people.json file from the root directory of DAOS filesystem, we can use the following pySpark code: df = spark.read.json(\"daos:///people.json\") Appendix \u00b6 Building DAOS Hadoop Filesystem \u00b6 Below are the steps to build the Java jar files for the DAOS Java and DAOS Hadoop filesystem. Spark and Hadoop require these jars in their classpath. You can ignore this section if you already have the pre-built jars. $ git clone https://github.com/daos-stack/daos.git $ cd daos $ git checkout <desired branch or commit> ## assume DAOS is built and installed to <daos_install> directory $ cd src/client/java ## with-proto3-netty4-deps profile builds jars with protobuf 3 and netty-buffer 4 shaded ## It spares you potential third-party jar compatibility issue. $ mvn -Pdistribute,with-proto3-netty4-deps clean package -DskipTests -Dgpg.skip -Ddaos.install.path=<daos_install> After build, the package daos-java-<version>-assemble.tgz will be available under distribution/target . DAOS More URIs \u00b6 DAOS FileSystem binds to schema \"daos\". DAOS URIs are in the format of \"daos://[authority]//[path]\". Both authority and path are optional. There are three types of DAOS URIs, DAOS UNS path, DAOS Non-UNS path and Special UUID path depending on where you want the DAOS Filesystem to get initialized and configured. DAOS UNS Path \u00b6 The simple form of URI is \"daos:///\\<your uns path>[/sub path]\". \"\\<your path>\" is your OS file path created with the daos command or Java DAOS UNS method, DaosUns.create() . The \"[sub path]\" is optional. You can create the UNS path with below command. $ daos cont create --pool <pool UUID> -path <your path> --type=POSIX Or $ java -Dpath=\"your path\" -Dpool_id=\"your pool uuid\" -cp ./daos-java-<version>-shaded.jar io.daos.dfs.DaosUns create After creation, you can use below command to see what DAOS properties set to the path. $ getfattr -d -m - <your path> DAOS Non-UNS Path \u00b6 Check Set DAOS URI and Pool/Container . Special UUID Path \u00b6 DAOS supports a specialized URI with pool/container UUIDs embedded. The format is \"daos://pool UUID/container UUID\". As you can see, we don't need to find the UUIDs from neither UNS path nor configuration like above two types of URIs. You may want to connect to two DAOS servers or two DFS instances mounted to different containers in one DAOS server from same JVM. Then, you need to add authority to your URI to make it unique since Hadoop caches filesystem instance keyed by \"schema + authority\" in global (JVM). It applies to the both types of URIs described above. Run Map-Reduce in Hadoop \u00b6 Edit $HADOOP_HOME/etc/hadoop/core-site.xml to change fs.defaultFS to daos:/// . It is not recommended to set fs.defaultFS to a DAOS UNS path. You may get an error complaining pool/container UUIDs cannot be found. It's because Hadoop considers the default filesystem is DAOS since you configured DAOS UNS URI. YARN has some working directories defaulting to local path without schema, like \"/tmp/yarn\", which is then constructed as \"daos:///tmp/yarn\". With this URI, Hadoop cannot connect to DAOS since no pool/container UUIDs can be found if daos-site.xml is not provided too. . Then append below configuration to this file and $HADOOP_HOME/etc/hadoop/yarn-site.xml . <property> <name>fs.AbstractFileSystem.daos.impl</name> <value>io.daos.fs.hadoop.DaosAbsFsImpl</value> </property> DAOS has no data locality since it is remote storage. You need to add below configuration to the scheduler configuration file, like capacity-scheduler.xml in yarn. <property> <name>yarn.scheduler.capacity.node-locality-delay</name> <value>-1</value> </property> Then replicate daos-site.xml , core-site.xml , yarn-site.xml and capacity-scheduler.xml to other nodes. Known Issues \u00b6 If you use Omni-path PSM2 provider in DAOS, you'll get connection issue in Yarn container due to PSM2 resource not being released properly in time. Tune More Configurations \u00b6 If your DAOS URI is the non-UNS, you can follow descriptions of each config item to set your own values in loadable daos-site.xml . If your DAOS URI is the UNS path, your configurations, except those set by DAOS UNS creation, in daos-site.xml can still be effective. To make configuration source consistent, an alternative to the configuration file daos-site.xml is to set all configurations to the UNS path. You put the configs to the same UNS path with below command. # install attr package if get \"command not found\" error $ setfattr -n user.daos.hadoop -v \"fs.daos.server.group=daos_server\" <your path> Or $ java -Dpath=\"your path\" -Dattr=user.daos.hadoop -Dvalue=\"fs.daos.server.group=daos_server\" -cp ./daos-java-<version>-shaded.jar io.daos.dfs.DaosUns setappinfo For the \"value\" property, you need to follow pattern, key1=value1:key2=value2.. .. And key should be from daos-site-example.xml . If value contains characters of '=' or ':', you need to escape the value with below command. $ java -Dop=escape-app-value -Dinput=\"daos_server:1=2\" -cp ./daos-java-<version>-shaded.jar io.daos.dfs.DaosUns util You'll get escaped value, \"daos_server\\u003a1\\u003d2\", for \"daos_server:1=2\". If you configure the same property in both daos-site.xml and UNS path, the value in daos-site.xml takes priority. If user sets Hadoop configuration before initializing Hadoop DAOS FileSystem, the user's configuration takes priority. PSM2 Issue \u00b6 For some libfabric providers, like PSM2, signal chaining should be enabled to better interoperate with DAOS and its dependencies which may install its own signal handlers. It ensures that signal calls are intercepted so that they do not actually replace the JVM's signal handlers if the handlers conflict with those already installed by the JVM. Instead, these calls save the new signal handlers, or \"chain\" them behind the JVM-installed handlers. Later, when any of these signals are raised and found not to be targeted at the JVM, the DAOS's handlers are invoked. $ export LD_PRELOAD=<YOUR JDK HOME>/jre/lib/amd64/libjsig.so","title":"Spark and Hadoop"},{"location":"user/spark/#daos-hadoop-filesystem","text":"Here, we describe the quick steps required to use the DAOS Hadoop filesystem to access DAOS from Hadoop and Spark.","title":"DAOS Hadoop Filesystem"},{"location":"user/spark/#prerequisites","text":"Linux OS Java 8 Hadoop 2.7 or later Spark 3.1 or later DAOS Readiness We assume that the DAOS servers and agents have already been deployed in the environment. Otherwise, they can be deployed by following the DAOS Installation Guide .","title":"Prerequisites"},{"location":"user/spark/#maven-download","text":"There are two artifacts to download, daos-java and hadoop-daos, from maven. Here are maven dependencies. You can download them with below commands if you have maven installed. mvn dependency:get -Dartifact=io.daos:daos-java:<version> -Ddest=./ mvn dependency:get -Dartifact=io.daos:hadoop-daos:<version> -Ddest=./ Or search these artifacts from maven central(https://search.maven.org) and download them manually. You can also build artifacts by yourself. see Build DAOS Hadoop Filesystem for details.","title":"Maven Download"},{"location":"user/spark/#deployment","text":"","title":"Deployment"},{"location":"user/spark/#jar-files","text":"daos-java-<version>.jar and hadoop-daos-<version>.jar need to be deployed on every compute node that runs Spark or Hadoop. Place them in a directory, e.g., $SPARK_HOME/jars for Spark and $HADOOP_HOME/share/hadoop/common/lib for Hadoop, which is accessible to all the nodes or copy them to every node.","title":"JAR Files"},{"location":"user/spark/#daos-site-examplexml","text":"Extract from hadoop-daos-<version>.jar and rename to daos-site.xml . Then copy it to your application config directory, e.g., $SPARK_HOME/conf for Spark and $HADOOP_HOME/etc/hadoop for Hadoop.","title":"daos-site-example.xml"},{"location":"user/spark/#configuring-hadoop","text":"","title":"Configuring Hadoop"},{"location":"user/spark/#environment-variable","text":"Export all DAOS-specific env variables in your application, e.g., spark-env.sh for Spark and hadoop-env.sh for Hadoop. Or you can simply put env variables in your .bashrc . Besides, you should have LD_LIBRARY_PATH include DAOS library path so that Java can link to DAOS libs, like below. $ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<DAOS_INSTALL>/lib64:<DAOS_INSTALL>/lib","title":"Environment Variable"},{"location":"user/spark/#daos-uri","text":"In daos-site.xml , we default the DAOS URI as simplest form, \"daos:///\". For other form of URIs, please check DAOS More URIs . If the DAOS pool and container have not been created, we can use the following command to create them and get the pool UUID and container UUID. $ dmg pool create --scm-size=<scm size> --nvme-size=<nvme size> $ daos cont create --pool <pool UUID> --type POSIX After that, configure daos-site.xml with the pool and container created. <configuration> ... <property> <name>fs.daos.pool.uuid</name> <value>your pool UUID</value> <description>UUID of DAOS pool</description> </property> <property> <name>fs.daos.container.uuid</name> <value>your container UUID</value> <description>UUID of DAOS container created with \"--type posix\"</description> </property> ... </configuration> Please put daos-site.xml in right place, e.g., Java classpath, and loadable by Hadoop DAOS FileSystem.","title":"DAOS URI"},{"location":"user/spark/#validating-hadoop-access","text":"If everything goes well, you should see /user directory being listed after issuing below command. $ hadoop fs -ls / You can also play around with other Hadoop commands, like -copyFromLocal and -copyToLocal. You can also start Yarn and run some mapreduce jobs on Yarn. See Run Map-Reduce in Hadoop","title":"Validating Hadoop Access"},{"location":"user/spark/#configuring-spark","text":"To access DAOS Hadoop filesystem in Spark, add the jar files to the classpath of the Spark executor and driver. This can be configured in Spark's configuration file spark-defaults.conf . spark.executor.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar spark.driver.extraClassPath /path/to/daos-java-<version>.jar:/path/to/hadoop-daos-<version>.jar","title":"Configuring Spark"},{"location":"user/spark/#validatng-spark-access","text":"All Spark APIs that work with the Hadoop filesystem will work with DAOS. We can use the daos:/// URI to access files stored in DAOS. For example, to read the people.json file from the root directory of DAOS filesystem, we can use the following pySpark code: df = spark.read.json(\"daos:///people.json\")","title":"Validatng Spark Access"},{"location":"user/spark/#appendix","text":"","title":"Appendix"},{"location":"user/spark/#building-daos-hadoop-filesystem","text":"Below are the steps to build the Java jar files for the DAOS Java and DAOS Hadoop filesystem. Spark and Hadoop require these jars in their classpath. You can ignore this section if you already have the pre-built jars. $ git clone https://github.com/daos-stack/daos.git $ cd daos $ git checkout <desired branch or commit> ## assume DAOS is built and installed to <daos_install> directory $ cd src/client/java ## with-proto3-netty4-deps profile builds jars with protobuf 3 and netty-buffer 4 shaded ## It spares you potential third-party jar compatibility issue. $ mvn -Pdistribute,with-proto3-netty4-deps clean package -DskipTests -Dgpg.skip -Ddaos.install.path=<daos_install> After build, the package daos-java-<version>-assemble.tgz will be available under distribution/target .","title":"Building DAOS Hadoop Filesystem"},{"location":"user/spark/#daos-more-uris","text":"DAOS FileSystem binds to schema \"daos\". DAOS URIs are in the format of \"daos://[authority]//[path]\". Both authority and path are optional. There are three types of DAOS URIs, DAOS UNS path, DAOS Non-UNS path and Special UUID path depending on where you want the DAOS Filesystem to get initialized and configured.","title":"DAOS More URIs"},{"location":"user/spark/#daos-uns-path","text":"The simple form of URI is \"daos:///\\<your uns path>[/sub path]\". \"\\<your path>\" is your OS file path created with the daos command or Java DAOS UNS method, DaosUns.create() . The \"[sub path]\" is optional. You can create the UNS path with below command. $ daos cont create --pool <pool UUID> -path <your path> --type=POSIX Or $ java -Dpath=\"your path\" -Dpool_id=\"your pool uuid\" -cp ./daos-java-<version>-shaded.jar io.daos.dfs.DaosUns create After creation, you can use below command to see what DAOS properties set to the path. $ getfattr -d -m - <your path>","title":"DAOS UNS Path"},{"location":"user/spark/#daos-non-uns-path","text":"Check Set DAOS URI and Pool/Container .","title":"DAOS Non-UNS Path"},{"location":"user/spark/#special-uuid-path","text":"DAOS supports a specialized URI with pool/container UUIDs embedded. The format is \"daos://pool UUID/container UUID\". As you can see, we don't need to find the UUIDs from neither UNS path nor configuration like above two types of URIs. You may want to connect to two DAOS servers or two DFS instances mounted to different containers in one DAOS server from same JVM. Then, you need to add authority to your URI to make it unique since Hadoop caches filesystem instance keyed by \"schema + authority\" in global (JVM). It applies to the both types of URIs described above.","title":"Special UUID Path"},{"location":"user/spark/#run-map-reduce-in-hadoop","text":"Edit $HADOOP_HOME/etc/hadoop/core-site.xml to change fs.defaultFS to daos:/// . It is not recommended to set fs.defaultFS to a DAOS UNS path. You may get an error complaining pool/container UUIDs cannot be found. It's because Hadoop considers the default filesystem is DAOS since you configured DAOS UNS URI. YARN has some working directories defaulting to local path without schema, like \"/tmp/yarn\", which is then constructed as \"daos:///tmp/yarn\". With this URI, Hadoop cannot connect to DAOS since no pool/container UUIDs can be found if daos-site.xml is not provided too. . Then append below configuration to this file and $HADOOP_HOME/etc/hadoop/yarn-site.xml . <property> <name>fs.AbstractFileSystem.daos.impl</name> <value>io.daos.fs.hadoop.DaosAbsFsImpl</value> </property> DAOS has no data locality since it is remote storage. You need to add below configuration to the scheduler configuration file, like capacity-scheduler.xml in yarn. <property> <name>yarn.scheduler.capacity.node-locality-delay</name> <value>-1</value> </property> Then replicate daos-site.xml , core-site.xml , yarn-site.xml and capacity-scheduler.xml to other nodes.","title":"Run Map-Reduce in Hadoop"},{"location":"user/spark/#known-issues","text":"If you use Omni-path PSM2 provider in DAOS, you'll get connection issue in Yarn container due to PSM2 resource not being released properly in time.","title":"Known Issues"},{"location":"user/spark/#tune-more-configurations","text":"If your DAOS URI is the non-UNS, you can follow descriptions of each config item to set your own values in loadable daos-site.xml . If your DAOS URI is the UNS path, your configurations, except those set by DAOS UNS creation, in daos-site.xml can still be effective. To make configuration source consistent, an alternative to the configuration file daos-site.xml is to set all configurations to the UNS path. You put the configs to the same UNS path with below command. # install attr package if get \"command not found\" error $ setfattr -n user.daos.hadoop -v \"fs.daos.server.group=daos_server\" <your path> Or $ java -Dpath=\"your path\" -Dattr=user.daos.hadoop -Dvalue=\"fs.daos.server.group=daos_server\" -cp ./daos-java-<version>-shaded.jar io.daos.dfs.DaosUns setappinfo For the \"value\" property, you need to follow pattern, key1=value1:key2=value2.. .. And key should be from daos-site-example.xml . If value contains characters of '=' or ':', you need to escape the value with below command. $ java -Dop=escape-app-value -Dinput=\"daos_server:1=2\" -cp ./daos-java-<version>-shaded.jar io.daos.dfs.DaosUns util You'll get escaped value, \"daos_server\\u003a1\\u003d2\", for \"daos_server:1=2\". If you configure the same property in both daos-site.xml and UNS path, the value in daos-site.xml takes priority. If user sets Hadoop configuration before initializing Hadoop DAOS FileSystem, the user's configuration takes priority.","title":"Tune More Configurations"},{"location":"user/spark/#psm2-issue","text":"For some libfabric providers, like PSM2, signal chaining should be enabled to better interoperate with DAOS and its dependencies which may install its own signal handlers. It ensures that signal calls are intercepted so that they do not actually replace the JVM's signal handlers if the handlers conflict with those already installed by the JVM. Instead, these calls save the new signal handlers, or \"chain\" them behind the JVM-installed handlers. Later, when any of these signals are raised and found not to be targeted at the JVM, the DAOS's handlers are invoked. $ export LD_PRELOAD=<YOUR JDK HOME>/jre/lib/amd64/libjsig.so","title":"PSM2 Issue"},{"location":"user/workflow/","text":"Workflow \u00b6 Use Cases \u00b6 A DAOS pool is a persistent storage reservation that is allocated to a project or specific job. Pools are allocated, shrunk, grown and destroyed by the administrators. The typical workflow consists of: * New project members meet and define storage requirements including space, bandwidth, IOPS & data protection needs. * Administrators collect those requirements, create a pool for the new project and set relevant ACL to grant access to project members. * Administrators notify the project members that the pool has been created and provide the pool label to the users. Users can then create datasets (i.e. called containers) in the pool. Containers will share the pool space and have their own ACL to be managed by the container owner. Since pool creation is relatively fast, it is also possible to integrate it with the resource manager to create and ingest data into an ephemeral pool for each job. Another alternative use case is to emulate a parallel file system by creating one big pool with a single POSIX container to be accessed by all users. daos Utility \u00b6 The daos(1) utility is built over the libdaos library and is the primary command-line interface for users to interact with their pool and containers. It supports a -j option to generate a parseable json output. The daos utility follows the same syntax as dmg (reserved for administrator) and takes a resource (e.g. pool, container, filesystem) and a command (e.g. query, create, destroy) plus a set of command-specific options. $ daos --help Usage: daos RESOURCE COMMAND [OPTIONS] <command> daos is a tool that can be used to manage/query pool content, create/query/manage/destroy a container inside a pool, copy data between a POSIX container and a POSIX filesystem, clone a DAOS container, or query/manage an object inside a container. Application Options: --debug enable debug output --verbose enable verbose output (when applicable) -j, --json enable JSON output Help Options: -h, --help Show this help message Available commands: container perform tasks related to DAOS containers (aliases: cont) filesystem POSIX filesystem operations (aliases: fs) object DAOS object operations (aliases: obj) pool perform tasks related to DAOS pools version print daos version Pool Access Validation \u00b6 To validate the pool can be successfully accessed prior to running applications, the daos pool autotest suite can be executed. To run it against a pool labeled tank , run the following command: $ daos pool autotest tank Step Operation Status Time(sec) Comment 0 Initializing DAOS OK 0.000 1 Connecting to pool OK 0.070 2 Creating container OK 0.000 uuid = ba5c6a78-6ddc-4c7e-a73b-b7574c8d85b8 3 Opening container OK 0.060 10 Generating 1M S1 layouts OK 2.960 11 Generating 10K SX layouts OK 0.130 20 Inserting 1M 128B values OK 27.350 21 Reading 128B values back OK 26.020 24 Inserting 1M 4KB values OK 54.410 25 Reading 4KB values back OK 54.380 28 Inserting 100K 1MB values OK 605.870 29 Reading 1MB values back OK 680.360 96 Closing container OK 0.000 97 Destroying container OK 0.030 98 Disconnecting from pool OK 0.010 99 Tearing down DAOS OK 0.000 All steps passed. Note The command is executed in a development environment, performance differences will vary, based on your system. Warning Smaller pools may show DER_NOSPACE(-1007): 'No space on storage target' Pool Query \u00b6 Once a pool has been assigned to your project (labeled tank in the example below), you can verify how much space was allocated to your project via the daos pool query <pool_label> command as follows: $ daos pool query tank Pool ada29109-0589-4fb8-9726-1252faea5d01, ntarget=32, disabled=0, leader=0, version=1 Pool space info: - Target(VOS) count:32 - SCM: Total size: 50 GB Free: 50 GB, min:1.6 GB, max:1.6 GB, mean:1.6 GB - NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild idle, 0 objs, 0 recs In addition to the space information, details on the pool rebuild status and number of targets is also provided. This information can also be retrieved programmatically via the daos_pool_query() function of the libdaos library and python equivalent. Pool Attributes \u00b6 Project-wise information can be stored in pool user attributes (not to be confused with pool properties). Pool attributes can be manipulated via the daos pool [set|get|list|del]-attr commands. $ daos pool set-attr tank project_deadline \"September 30, 2025\" $ daos pool list-attr tank Attributes for pool 004abf7c-26c8-4cba-9059-8b3be39161fc: Name ---- project_deadline $ daos pool get-attr tank project_deadline Attributes for pool 004abf7c-26c8-4cba-9059-8b3be39161fc: Name Value ---- ----- project_deadline September 30, 2025 $ daos pool del-attr tank project_deadline $ daos pool list-attr tank Attributes for pool 004abf7c-26c8-4cba-9059-8b3be39161fc: No attributes found. Pool attributes can be manipulaged programmatically via the daos_pool_[get|get|list|del]_attr() functions exported by the libdaos library and python equivalent (see PyDAOS).","title":"Workflow"},{"location":"user/workflow/#workflow","text":"","title":"Workflow"},{"location":"user/workflow/#use-cases","text":"A DAOS pool is a persistent storage reservation that is allocated to a project or specific job. Pools are allocated, shrunk, grown and destroyed by the administrators. The typical workflow consists of: * New project members meet and define storage requirements including space, bandwidth, IOPS & data protection needs. * Administrators collect those requirements, create a pool for the new project and set relevant ACL to grant access to project members. * Administrators notify the project members that the pool has been created and provide the pool label to the users. Users can then create datasets (i.e. called containers) in the pool. Containers will share the pool space and have their own ACL to be managed by the container owner. Since pool creation is relatively fast, it is also possible to integrate it with the resource manager to create and ingest data into an ephemeral pool for each job. Another alternative use case is to emulate a parallel file system by creating one big pool with a single POSIX container to be accessed by all users.","title":"Use Cases"},{"location":"user/workflow/#daos-utility","text":"The daos(1) utility is built over the libdaos library and is the primary command-line interface for users to interact with their pool and containers. It supports a -j option to generate a parseable json output. The daos utility follows the same syntax as dmg (reserved for administrator) and takes a resource (e.g. pool, container, filesystem) and a command (e.g. query, create, destroy) plus a set of command-specific options. $ daos --help Usage: daos RESOURCE COMMAND [OPTIONS] <command> daos is a tool that can be used to manage/query pool content, create/query/manage/destroy a container inside a pool, copy data between a POSIX container and a POSIX filesystem, clone a DAOS container, or query/manage an object inside a container. Application Options: --debug enable debug output --verbose enable verbose output (when applicable) -j, --json enable JSON output Help Options: -h, --help Show this help message Available commands: container perform tasks related to DAOS containers (aliases: cont) filesystem POSIX filesystem operations (aliases: fs) object DAOS object operations (aliases: obj) pool perform tasks related to DAOS pools version print daos version","title":"daos Utility"},{"location":"user/workflow/#pool-access-validation","text":"To validate the pool can be successfully accessed prior to running applications, the daos pool autotest suite can be executed. To run it against a pool labeled tank , run the following command: $ daos pool autotest tank Step Operation Status Time(sec) Comment 0 Initializing DAOS OK 0.000 1 Connecting to pool OK 0.070 2 Creating container OK 0.000 uuid = ba5c6a78-6ddc-4c7e-a73b-b7574c8d85b8 3 Opening container OK 0.060 10 Generating 1M S1 layouts OK 2.960 11 Generating 10K SX layouts OK 0.130 20 Inserting 1M 128B values OK 27.350 21 Reading 128B values back OK 26.020 24 Inserting 1M 4KB values OK 54.410 25 Reading 4KB values back OK 54.380 28 Inserting 100K 1MB values OK 605.870 29 Reading 1MB values back OK 680.360 96 Closing container OK 0.000 97 Destroying container OK 0.030 98 Disconnecting from pool OK 0.010 99 Tearing down DAOS OK 0.000 All steps passed. Note The command is executed in a development environment, performance differences will vary, based on your system. Warning Smaller pools may show DER_NOSPACE(-1007): 'No space on storage target'","title":"Pool Access Validation"},{"location":"user/workflow/#pool-query","text":"Once a pool has been assigned to your project (labeled tank in the example below), you can verify how much space was allocated to your project via the daos pool query <pool_label> command as follows: $ daos pool query tank Pool ada29109-0589-4fb8-9726-1252faea5d01, ntarget=32, disabled=0, leader=0, version=1 Pool space info: - Target(VOS) count:32 - SCM: Total size: 50 GB Free: 50 GB, min:1.6 GB, max:1.6 GB, mean:1.6 GB - NVMe: Total size: 0 B Free: 0 B, min:0 B, max:0 B, mean:0 B Rebuild idle, 0 objs, 0 recs In addition to the space information, details on the pool rebuild status and number of targets is also provided. This information can also be retrieved programmatically via the daos_pool_query() function of the libdaos library and python equivalent.","title":"Pool Query"},{"location":"user/workflow/#pool-attributes","text":"Project-wise information can be stored in pool user attributes (not to be confused with pool properties). Pool attributes can be manipulated via the daos pool [set|get|list|del]-attr commands. $ daos pool set-attr tank project_deadline \"September 30, 2025\" $ daos pool list-attr tank Attributes for pool 004abf7c-26c8-4cba-9059-8b3be39161fc: Name ---- project_deadline $ daos pool get-attr tank project_deadline Attributes for pool 004abf7c-26c8-4cba-9059-8b3be39161fc: Name Value ---- ----- project_deadline September 30, 2025 $ daos pool del-attr tank project_deadline $ daos pool list-attr tank Attributes for pool 004abf7c-26c8-4cba-9059-8b3be39161fc: No attributes found. Pool attributes can be manipulaged programmatically via the daos_pool_[get|get|list|del]_attr() functions exported by the libdaos library and python equivalent (see PyDAOS).","title":"Pool Attributes"}]}